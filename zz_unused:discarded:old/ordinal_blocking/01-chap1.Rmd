# PAPER I: IMPROVING BALANCE IN SURVEY EXPERIMENTS BY BLOCKING ON ORDINAL VARIABLES {#ordblock}

## Introduction {#ordblock-intro}

Survey experiments collect background information and contain questions that represent differing types of treatment, depending on the specific nature of the experiment. Such experiments are created to uncover any effect of a certain treatment on public opinion and behavior. In order to uncover such potential effects, treatment groups need to be comparable. All treatment groups need to look the same in every measure, i.e. they must be balanced. This can be achieved through random assignment of participants to treatment groups. One such option for random assignment is complete randomization. For each participant, the computer flips a coin to decide which treatment group to assign her to  [@urdan_statistics_2010]. Using complete randomization for large samples probabilistically results in balance based on the Law of Large Numbers. Using it for small samples, however, can result in serious imbalance. It can easily be that the treatment groups will not look the same. This can leave experimental results in statistically murky waters [@imai_quantitative_2018;@king_designing_1994;@fox_applied_2015]. In survey experiments, the overall sample size is often split across several treatment groups. @chong_framing_2007, for instance, split 869 participants in a framing experiment on urban growth over 17 treatment groups, which leads to an average of just over 50 participants per group. Complete randomization is very unlikely to lead to balanced treatment groups of this size. Researchers need to employ statistical methods to obtain balanced groups here.

One statistical method to ensure balance for small samples is blocking. Blocking involves the arrangement of participants in groups that are similar to one another in terms of the participants' covariates, i.e. their background information. Within these groups, participants are then randomly assigned. This is different from complete randomization, where participants are immediately randomly assigned without taking their covariate information into account.


```{r Law of Large Numbers Simulations, eval=FALSE, include=FALSE}

### THIS IS THE CODE THAT CREATES THE SIMULATIONS. I RAN THIS ON JEFF'S MACHINE. IT'S INCLUDED HERE FOR THE SAKE OF COMPLETION ###

## Save .csv files for blocked and randomized ##
## Sample is between 6 and 1000, with variations and sequences for different group sizes ##
## Following Ryan's suggestion, I set the seq() to bigger iterations, to keep the calulcation time, resulting file size and density down ##
## Repeats: 100 ##
## Levels: 5 ##
## Groups: 2 ##

sampled_numbers <- seq(from = 6, to = 1000, by = 8)
length(sampled_numbers)
repeats <- 100
lev <- 5
groups <- as.factor(c("control", "treatment"))

listoflists_1 <- rep(list(list()), length(lev)) # lists with with length(lev) # of lists (to store sampled numbers)

df_rand <- data.frame(matrix(NA, repeats, (length(groups)+2))) # needed for listofdf_rand (+2 to add variance and levels columns after the treatment group columns)
colnames(df_rand) <- c(levels(groups), "variance", "levels")
listofdf_rand <- rep(list(df_rand), length(sampled_numbers)) # list with length(sampled_numbers) # of df_rand (needed for listoflists_2)

listoflists_2 <- rep(list(listofdf_rand), length(lev)) # list with length(lev) # of listofdf_rand (to store means of randomized treatment groups)
listoflists_3 <- rep(list(list()), length(sampled_numbers)) # list with length(sampled_numers) # of lists (to store blocking assignments)

df_blocked <- data.frame(matrix(NA, 1, (length(groups)+3))) # needed for listofdf_blocked (+3 to add columns later)
colnames(df_blocked) <- c(levels(groups), "sampled_numbers", "repeat", "diff")
listofdf_blocked <- rep(list(df_blocked), repeats) # list with repeats # of df_blocked (needed for listoflists_4)

listoflists_4 <- rep(list(listofdf_blocked), length(sampled_numbers)) #list with length(sampled_numbers) # of listofdf_blocked (to store means of blocked treatment groups)

pb <- txtProgressBar(min = 1, max = length(sampled_numbers), style = 3) # creates the percentage progress bar across the sampled numbers

for(q in 1:length(lev)){
  for(x in 1:length(sampled_numbers)){
    setTxtProgressBar(pb, x) # loads the percentage progress bar into the loop
    set.seed(123)
    listoflists_1[[q]][[x]] <- data.frame(replicate(repeats, sample(lev[q], sampled_numbers[x], replace = TRUE))) 
    # randomly sample numbers
    listoflists_1[[q]][[x]]$levels <- rep(lev[q], sampled_numbers[x]) 
    # add column of levels
    listoflists_1[[q]][[x]]$groups <- simple_ra(N = sampled_numbers[x], conditions=groups)  
    # assign treatment for each amount of randomly sampled numbers and add as column
    listoflists_1[[q]][[x]]$sampled_numbers <- rep(sampled_numbers[x], sampled_numbers[x]) 
    # add column of of sampled numbers
    listoflists_1[[q]][[x]]$repeats <- rep(repeats, sampled_numbers[x]) 
    # add column of repeats
    listoflists_1[[q]][[x]]$id <- 1:sampled_numbers[x]
    # add column of continuous id (for blocking)
      for(i in 1:repeats){
        listoflists_3[[x]][[i]] <- assignment(block(listoflists_1[[q]][[x]], id.vars = "id", block.vars = names(listoflists_1[[q]][[x]])[i], n.tr = length(groups)), seed = 123)
        # block and assign samples to treatment groups
        for(z in 1:length(groups)){
          listoflists_2[[q]][[x]][i,z] <- mean(listoflists_1[[q]][[x]][,i][listoflists_1[[q]][[x]]$groups == levels(groups)[z]])
          # take means of each treatment group and add as column
          listoflists_4[[x]][[i]][1,z] <- mean(listoflists_1[[q]][[x]][listoflists_1[[q]][[x]]$id %in% unname(listoflists_3[[x]][[i]]$assg[[1]][,z]), i]) # take means of each treatment group and add as column
          # %in% is needed because $assg lists the assigned units, not their actual sampled values
        }
        listoflists_4[[x]][[i]][1,(length(groups)+1)] <- sampled_numbers[x]
        # add column of sampled numbers
        listoflists_4[[x]][[i]][1,(length(groups)+2)] <- repeats
        # add column of repeats
        listoflists_4[[x]][[i]][1,(length(groups)+3)] <- apply(listoflists_4[[x]][[i]][,1:length(groups)], 1, function(w) diff(range(w)))
        # # calculate the max diff between treatment groups and add as column
        listoflists_2[[q]][[x]][i,(length(groups)+1)] <- var(listoflists_1[[q]][[x]][,i]) 
        # take variance of each repeat and add as column
        listoflists_2[[q]][[x]][i,(length(groups)+2)] <- lev[q] 
        # add column of levels
      }  
    listoflists_2[[q]][[x]]$groups <- rep(length(groups), repeats) 
    # add groups to dataframes
    listoflists_2[[q]][[x]]$sampled_numbers <- rep(sampled_numbers[x], repeats)
    # add sampled numbers to dataframes
    listoflists_2[[q]][[x]]$repeats <- rep(repeats, repeats)
    # add repeats to dataframes
    }
}

list_all_samples <- unlist(listoflists_1, recursive=FALSE) 
# turn listoflists_1 (which is a list of lists of dataframes) into a list of dataframes
list_all_means_variances <- unlist(listoflists_2, recursive = FALSE) 
# same for listoflists_2
all_samples <- ldply(list_all_samples, data.frame) 
# turn the list of dataframes of all samples into one giant dataframe of all samples
all_means_variances <- ldply(list_all_means_variances, data.frame) 
# same for list of dataframes of all means and variances
all_means_variances$diff <- apply(all_means_variances[,1:length(groups)], 1, function(x) diff(range(x)))
# calculate the max diff between treatment groups and add them as column "diff"
all_means_variances$label <- c(rep("rand", nrow(all_means_variances)))
# add the label "rand" to all rows (for plotting)

list_all_blocked <- unlist(listoflists_4, recursive = FALSE)
# turn listoflists_4 (which is a list of lists of dataframes) into a list of dataframes
all_blocked <- ldply(list_all_blocked, data.frame)
# turn the list of dataframes with all blocked diffs into one giant dataframe of all diffs
all_blocked$label <- c(rep("blocked", nrow(all_blocked)))
# add the label "blocked" to all rows (for plotting)

write.csv(all_samples, file = "all_samples_2.csv")
write.csv(all_means_variances, file = "all_means_variances_2.csv")
write.csv(all_blocked, file = "all_blocked_2.csv")




## Save .csv files for blocked and randomized ##
## Same as above, just for 3 groups ##

sampled_numbers <- seq(from = 9, to = 1002, by = 9)
length(sampled_numbers)
repeats <- 100
lev <- 5
groups <- as.factor(c("control", "treatment1", "treatment2"))

listoflists_1 <- rep(list(list()), length(lev)) # lists with with length(lev) # of lists (to store sampled numbers)

df_rand <- data.frame(matrix(NA, repeats, (length(groups)+2))) # needed for listofdf_rand (+2 to add variance and levels columns after the treatment group columns)
colnames(df_rand) <- c(levels(groups), "variance", "levels")
listofdf_rand <- rep(list(df_rand), length(sampled_numbers)) # list with length(sampled_numbers) # of df_rand (needed for listoflists_2)

listoflists_2 <- rep(list(listofdf_rand), length(lev)) # list with length(lev) # of listofdf_rand (to store means of randomized treatment groups)
listoflists_3 <- rep(list(list()), length(sampled_numbers)) # list with length(sampled_numers) # of lists (to store blocking assignments)

df_blocked <- data.frame(matrix(NA, 1, (length(groups)+3))) # needed for listofdf_blocked (+3 to add columns later)
colnames(df_blocked) <- c(levels(groups), "sampled_numbers", "repeat", "diff")
listofdf_blocked <- rep(list(df_blocked), repeats) # list with repeats # of df_blocked (needed for listoflists_4)

listoflists_4 <- rep(list(listofdf_blocked), length(sampled_numbers)) #list with length(sampled_numbers) # of listofdf_blocked (to store means of blocked treatment groups)

pb <- txtProgressBar(min = 1, max = length(sampled_numbers), style = 3) # creates the percentage progress bar across the sampled numbers

for(q in 1:length(lev)){
  for(x in 1:length(sampled_numbers)){
    setTxtProgressBar(pb, x) # loads the percentage progress bar into the loop
    set.seed(123)
    listoflists_1[[q]][[x]] <- data.frame(replicate(repeats, sample(lev[q], sampled_numbers[x], replace = TRUE))) 
    # randomly sample numbers
    listoflists_1[[q]][[x]]$levels <- rep(lev[q], sampled_numbers[x]) 
    # add column of levels
    listoflists_1[[q]][[x]]$groups <- simple_ra(N = sampled_numbers[x], conditions=groups)  
    # assign treatment for each amount of randomly sampled numbers and add as column
    listoflists_1[[q]][[x]]$sampled_numbers <- rep(sampled_numbers[x], sampled_numbers[x]) 
    # add column of of sampled numbers
    listoflists_1[[q]][[x]]$repeats <- rep(repeats, sampled_numbers[x]) 
    # add column of repeats
    listoflists_1[[q]][[x]]$id <- 1:sampled_numbers[x]
    # add column of continuous id (for blocking)
      for(i in 1:repeats){
        listoflists_3[[x]][[i]] <- assignment(block(listoflists_1[[q]][[x]], id.vars = "id", block.vars = names(listoflists_1[[q]][[x]])[i], n.tr = length(groups)), seed = 123)
        # block and assign samples to treatment groups
        for(z in 1:length(groups)){
          listoflists_2[[q]][[x]][i,z] <- mean(listoflists_1[[q]][[x]][,i][listoflists_1[[q]][[x]]$groups == levels(groups)[z]])
          # take means of each treatment group and add as column
          listoflists_4[[x]][[i]][1,z] <- mean(listoflists_1[[q]][[x]][listoflists_1[[q]][[x]]$id %in% unname(listoflists_3[[x]][[i]]$assg[[1]][,z]), i]) # take means of each treatment group and add as column
          # %in% is needed because $assg lists the assigned units, not their actual sampled values
        }
        listoflists_4[[x]][[i]][1,(length(groups)+1)] <- sampled_numbers[x]
        # add column of sampled numbers
        listoflists_4[[x]][[i]][1,(length(groups)+2)] <- repeats
        # add column of repeats
        listoflists_4[[x]][[i]][1,(length(groups)+3)] <- apply(listoflists_4[[x]][[i]][,1:length(groups)], 1, function(w) diff(range(w)))
        # # calculate the max diff between treatment groups and add as column
        listoflists_2[[q]][[x]][i,(length(groups)+1)] <- var(listoflists_1[[q]][[x]][,i]) 
        # take variance of each repeat and add as column
        listoflists_2[[q]][[x]][i,(length(groups)+2)] <- lev[q] 
        # add column of levels
      }  
    listoflists_2[[q]][[x]]$groups <- rep(length(groups), repeats) 
    # add groups to dataframes
    listoflists_2[[q]][[x]]$sampled_numbers <- rep(sampled_numbers[x], repeats)
    # add sampled numbers to dataframes
    listoflists_2[[q]][[x]]$repeats <- rep(repeats, repeats)
    # add repeats to dataframes
    }
}

list_all_samples <- unlist(listoflists_1, recursive=FALSE) 
# turn listoflists_1 (which is a list of lists of dataframes) into a list of dataframes
list_all_means_variances <- unlist(listoflists_2, recursive = FALSE) 
# same for listoflists_2
all_samples <- ldply(list_all_samples, data.frame) 
# turn the list of dataframes of all samples into one giant dataframe of all samples
all_means_variances <- ldply(list_all_means_variances, data.frame) 
# same for list of dataframes of all means and variances
all_means_variances$diff <- apply(all_means_variances[,1:length(groups)], 1, function(x) diff(range(x)))
# calculate the max diff between treatment groups and add them as column "diff"
all_means_variances$label <- c(rep("rand", nrow(all_means_variances)))
# add the label "rand" to all rows (for plotting)

list_all_blocked <- unlist(listoflists_4, recursive = FALSE)
# turn listoflists_4 (which is a list of lists of dataframes) into a list of dataframes
all_blocked <- ldply(list_all_blocked, data.frame)
# turn the list of dataframes with all blocked diffs into one giant dataframe of all diffs
all_blocked$label <- c(rep("blocked", nrow(all_blocked)))
# add the label "blocked" to all rows (for plotting)

write.csv(all_samples, file = "all_samples_3.csv")
write.csv(all_means_variances, file = "all_means_variances_3.csv")
write.csv(all_blocked, file = "all_blocked_3.csv")




## Save .csv files for blocked and randomized ##
## Same as above, just for 5 groups

sampled_numbers <- seq(from = 15, to = 1000, by = 10)
length(sampled_numbers)
repeats <- 100
lev <- 5
groups <- as.factor(c("control", "treatment1", "treatment2", "treatment3", "treatment4"))

listoflists_1 <- rep(list(list()), length(lev)) # lists with with length(lev) # of lists (to store sampled numbers)

df_rand <- data.frame(matrix(NA, repeats, (length(groups)+2))) # needed for listofdf_rand (+2 to add variance and levels columns after the treatment group columns)
colnames(df_rand) <- c(levels(groups), "variance", "levels")
listofdf_rand <- rep(list(df_rand), length(sampled_numbers)) # list with length(sampled_numbers) # of df_rand (needed for listoflists_2)

listoflists_2 <- rep(list(listofdf_rand), length(lev)) # list with length(lev) # of listofdf_rand (to store means of randomized treatment groups)
listoflists_3 <- rep(list(list()), length(sampled_numbers)) # list with length(sampled_numers) # of lists (to store blocking assignments)

df_blocked <- data.frame(matrix(NA, 1, (length(groups)+3))) # needed for listofdf_blocked (+3 to add columns later)
colnames(df_blocked) <- c(levels(groups), "sampled_numbers", "repeat", "diff")
listofdf_blocked <- rep(list(df_blocked), repeats) # list with repeats # of df_blocked (needed for listoflists_4)

listoflists_4 <- rep(list(listofdf_blocked), length(sampled_numbers)) #list with length(sampled_numbers) # of listofdf_blocked (to store means of blocked treatment groups)

pb <- txtProgressBar(min = 1, max = length(sampled_numbers), style = 3) # creates the percentage progress bar across the sampled numbers

for(q in 1:length(lev)){
  for(x in 1:length(sampled_numbers)){
    setTxtProgressBar(pb, x) # loads the percentage progress bar into the loop
    set.seed(123)
    listoflists_1[[q]][[x]] <- data.frame(replicate(repeats, sample(lev[q], sampled_numbers[x], replace = TRUE))) 
    # randomly sample numbers
    listoflists_1[[q]][[x]]$levels <- rep(lev[q], sampled_numbers[x]) 
    # add column of levels
    listoflists_1[[q]][[x]]$groups <- simple_ra(N = sampled_numbers[x], conditions=groups)  
    # assign treatment for each amount of randomly sampled numbers and add as column
    listoflists_1[[q]][[x]]$sampled_numbers <- rep(sampled_numbers[x], sampled_numbers[x]) 
    # add column of of sampled numbers
    listoflists_1[[q]][[x]]$repeats <- rep(repeats, sampled_numbers[x]) 
    # add column of repeats
    listoflists_1[[q]][[x]]$id <- 1:sampled_numbers[x]
    # add column of continuous id (for blocking)
      for(i in 1:repeats){
        listoflists_3[[x]][[i]] <- assignment(block(listoflists_1[[q]][[x]], id.vars = "id", block.vars = names(listoflists_1[[q]][[x]])[i], n.tr = length(groups)), seed = 123)
        # block and assign samples to treatment groups
        for(z in 1:length(groups)){
          listoflists_2[[q]][[x]][i,z] <- mean(listoflists_1[[q]][[x]][,i][listoflists_1[[q]][[x]]$groups == levels(groups)[z]])
          # take means of each treatment group and add as column
          listoflists_4[[x]][[i]][1,z] <- mean(listoflists_1[[q]][[x]][listoflists_1[[q]][[x]]$id %in% unname(listoflists_3[[x]][[i]]$assg[[1]][,z]), i]) # take means of each treatment group and add as column
          # %in% is needed because $assg lists the assigned units, not their actual sampled values
        }
        listoflists_4[[x]][[i]][1,(length(groups)+1)] <- sampled_numbers[x]
        # add column of sampled numbers
        listoflists_4[[x]][[i]][1,(length(groups)+2)] <- repeats
        # add column of repeats
        listoflists_4[[x]][[i]][1,(length(groups)+3)] <- apply(listoflists_4[[x]][[i]][,1:length(groups)], 1, function(w) diff(range(w)))
        # # calculate the max diff between treatment groups and add as column
        listoflists_2[[q]][[x]][i,(length(groups)+1)] <- var(listoflists_1[[q]][[x]][,i]) 
        # take variance of each repeat and add as column
        listoflists_2[[q]][[x]][i,(length(groups)+2)] <- lev[q] 
        # add column of levels
      }  
    listoflists_2[[q]][[x]]$groups <- rep(length(groups), repeats) 
    # add groups to dataframes
    listoflists_2[[q]][[x]]$sampled_numbers <- rep(sampled_numbers[x], repeats)
    # add sampled numbers to dataframes
    listoflists_2[[q]][[x]]$repeats <- rep(repeats, repeats)
    # add repeats to dataframes
    }
}

list_all_samples <- unlist(listoflists_1, recursive=FALSE) 
# turn listoflists_1 (which is a list of lists of dataframes) into a list of dataframes
list_all_means_variances <- unlist(listoflists_2, recursive = FALSE) 
# same for listoflists_2
all_samples <- ldply(list_all_samples, data.frame) 
# turn the list of dataframes of all samples into one giant dataframe of all samples
all_means_variances <- ldply(list_all_means_variances, data.frame) 
# same for list of dataframes of all means and variances
all_means_variances$diff <- apply(all_means_variances[,1:length(groups)], 1, function(x) diff(range(x)))
# calculate the max diff between treatment groups and add them as column "diff"
all_means_variances$label <- c(rep("rand", nrow(all_means_variances)))
# add the label "rand" to all rows (for plotting)

list_all_blocked <- unlist(listoflists_4, recursive = FALSE)
# turn listoflists_4 (which is a list of lists of dataframes) into a list of dataframes
all_blocked <- ldply(list_all_blocked, data.frame)
# turn the list of dataframes with all blocked diffs into one giant dataframe of all diffs
all_blocked$label <- c(rep("blocked", nrow(all_blocked)))
# add the label "blocked" to all rows (for plotting)

write.csv(all_samples, file = "all_samples_5.csv")
write.csv(all_means_variances, file = "all_means_variances_5.csv")
write.csv(all_blocked, file = "all_blocked_5.csv")




## Save .csv files for blocked and randomized ##
## Same as above, just for 10 groups

sampled_numbers <- seq(from = 30, to = 1000, by = 10)
length(sampled_numbers)
repeats <- 100
lev <- 5
groups <- as.factor(c("control", "treatment1", "treatment2", "treatment3", "treatment4", "treatment5", "treatment6", "treatment7", "treatment8", "treatment9"))

listoflists_1 <- rep(list(list()), length(lev)) # lists with with length(lev) # of lists (to store sampled numbers)

df_rand <- data.frame(matrix(NA, repeats, (length(groups)+2))) # needed for listofdf_rand (+2 to add variance and levels columns after the treatment group columns)
colnames(df_rand) <- c(levels(groups), "variance", "levels")
listofdf_rand <- rep(list(df_rand), length(sampled_numbers)) # list with length(sampled_numbers) # of df_rand (needed for listoflists_2)

listoflists_2 <- rep(list(listofdf_rand), length(lev)) # list with length(lev) # of listofdf_rand (to store means of randomized treatment groups)
listoflists_3 <- rep(list(list()), length(sampled_numbers)) # list with length(sampled_numers) # of lists (to store blocking assignments)

df_blocked <- data.frame(matrix(NA, 1, (length(groups)+3))) # needed for listofdf_blocked (+3 to add columns later)
colnames(df_blocked) <- c(levels(groups), "sampled_numbers", "repeat", "diff")
listofdf_blocked <- rep(list(df_blocked), repeats) # list with repeats # of df_blocked (needed for listoflists_4)

listoflists_4 <- rep(list(listofdf_blocked), length(sampled_numbers)) #list with length(sampled_numbers) # of listofdf_blocked (to store means of blocked treatment groups)

pb <- txtProgressBar(min = 1, max = length(sampled_numbers), style = 3) # creates the percentage progress bar across the sampled numbers

for(q in 1:length(lev)){
  for(x in 1:length(sampled_numbers)){
    setTxtProgressBar(pb, x) # loads the percentage progress bar into the loop
    set.seed(123)
    listoflists_1[[q]][[x]] <- data.frame(replicate(repeats, sample(lev[q], sampled_numbers[x], replace = TRUE))) 
    # randomly sample numbers
    listoflists_1[[q]][[x]]$levels <- rep(lev[q], sampled_numbers[x]) 
    # add column of levels
    listoflists_1[[q]][[x]]$groups <- simple_ra(N = sampled_numbers[x], conditions=groups)  
    # assign treatment for each amount of randomly sampled numbers and add as column
    listoflists_1[[q]][[x]]$sampled_numbers <- rep(sampled_numbers[x], sampled_numbers[x]) 
    # add column of of sampled numbers
    listoflists_1[[q]][[x]]$repeats <- rep(repeats, sampled_numbers[x]) 
    # add column of repeats
    listoflists_1[[q]][[x]]$id <- 1:sampled_numbers[x]
    # add column of continuous id (for blocking)
      for(i in 1:repeats){
        listoflists_3[[x]][[i]] <- assignment(block(listoflists_1[[q]][[x]], id.vars = "id", block.vars = names(listoflists_1[[q]][[x]])[i], n.tr = length(groups)), seed = 123)
        # block and assign samples to treatment groups
        for(z in 1:length(groups)){
          listoflists_2[[q]][[x]][i,z] <- mean(listoflists_1[[q]][[x]][,i][listoflists_1[[q]][[x]]$groups == levels(groups)[z]])
          # take means of each treatment group and add as column
          listoflists_4[[x]][[i]][1,z] <- mean(listoflists_1[[q]][[x]][listoflists_1[[q]][[x]]$id %in% unname(listoflists_3[[x]][[i]]$assg[[1]][,z]), i]) # take means of each treatment group and add as column
          # %in% is needed because $assg lists the assigned units, not their actual sampled values
        }
        listoflists_4[[x]][[i]][1,(length(groups)+1)] <- sampled_numbers[x]
        # add column of sampled numbers
        listoflists_4[[x]][[i]][1,(length(groups)+2)] <- repeats
        # add column of repeats
        listoflists_4[[x]][[i]][1,(length(groups)+3)] <- apply(listoflists_4[[x]][[i]][,1:length(groups)], 1, function(w) diff(range(w)))
        # # calculate the max diff between treatment groups and add as column
        listoflists_2[[q]][[x]][i,(length(groups)+1)] <- var(listoflists_1[[q]][[x]][,i]) 
        # take variance of each repeat and add as column
        listoflists_2[[q]][[x]][i,(length(groups)+2)] <- lev[q] 
        # add column of levels
      }  
    listoflists_2[[q]][[x]]$groups <- rep(length(groups), repeats) 
    # add groups to dataframes
    listoflists_2[[q]][[x]]$sampled_numbers <- rep(sampled_numbers[x], repeats)
    # add sampled numbers to dataframes
    listoflists_2[[q]][[x]]$repeats <- rep(repeats, repeats)
    # add repeats to dataframes
    }
}

list_all_samples <- unlist(listoflists_1, recursive=FALSE) 
# turn listoflists_1 (which is a list of lists of dataframes) into a list of dataframes
list_all_means_variances <- unlist(listoflists_2, recursive = FALSE) 
# same for listoflists_2
all_samples <- ldply(list_all_samples, data.frame) 
# turn the list of dataframes of all samples into one giant dataframe of all samples
all_means_variances <- ldply(list_all_means_variances, data.frame) 
# same for list of dataframes of all means and variances
all_means_variances$diff <- apply(all_means_variances[,1:length(groups)], 1, function(x) diff(range(x)))
# calculate the max diff between treatment groups and add them as column "diff"
all_means_variances$label <- c(rep("rand", nrow(all_means_variances)))
# add the label "rand" to all rows (for plotting)

list_all_blocked <- unlist(listoflists_4, recursive = FALSE)
# turn listoflists_4 (which is a list of lists of dataframes) into a list of dataframes
all_blocked <- ldply(list_all_blocked, data.frame)
# turn the list of dataframes with all blocked diffs into one giant dataframe of all diffs
all_blocked$label <- c(rep("blocked", nrow(all_blocked)))
# add the label "blocked" to all rows (for plotting)

write.csv(all_samples, file = "all_samples_10.csv")
write.csv(all_means_variances, file = "all_means_variances_10.csv")
write.csv(all_blocked, file = "all_blocked_10.csv")

```


```{r Law of Large Numbers Plotting Code Boxplots, include=FALSE}

### I LOAD THE CREATED SIMULATIONS HERE TO CREATE THE PLOTS ###

all_blocked_2 <- read.csv("data/all_blocked_2.csv")
all_blocked_3 <- read.csv("data/all_blocked_3.csv")
all_blocked_5 <- read.csv("data/all_blocked_5.csv")
all_blocked_10 <- read.csv("data/all_blocked_10.csv")
all_means_variances_2 <- read.csv("data/all_means_variances_2.csv")
all_means_variances_3 <- read.csv("data/all_means_variances_3.csv")
all_means_variances_5 <- read.csv("data/all_means_variances_5.csv")
all_means_variances_10 <- read.csv("data/all_means_variances_10.csv")

together <- list(all_blocked_2, all_means_variances_2, all_blocked_3, all_means_variances_3, all_blocked_5, all_means_variances_5, all_blocked_10, all_means_variances_10) # collect all dfs in a list to loop over
couple <- list() # empty list

for(i in 1:(length(together))){
   couple[[i]] <- subset(together[[i]], select = c(sampled_numbers, diff, label))
   couple[[i]]$sampled_numbers <- as.factor(couple[[i]]$sampled_numbers)
} # subset for 3 columns and turn sampled_numbers into factor

sims_2 <- rbind(couple[[1]],couple[[2]]) # combine blocked and rand for each # of treatment groups
sims_3 <- rbind(couple[[3]],couple[[4]])
sims_5 <- rbind(couple[[5]],couple[[6]])
sims_10 <- rbind(couple[[7]],couple[[8]])

selection <- c(0, 0.1, 0.2, 0.3, 0.5, 1) # the quantiles I want

quantile(as.numeric(levels(sims_2$sampled_numbers)), selection) # quantiles for 2 groups
levels(sims_2$sampled_numbers) # levels for 2 groups
sims_2_range <- subset(sims_2, subset = sampled_numbers %in% c(14, 102, 206, 302, 502, 998)) # hand-select samples for range
sims_2_one <- subset(sims_2, subset = sampled_numbers == as.numeric(levels(sims_2$sampled_numbers)[2])) # select second level for 'intro' plot

quantile(as.numeric(levels(sims_3$sampled_numbers)), selection)
levels(sims_3$sampled_numbers)
sims_3_range <- subset(sims_3, subset = sampled_numbers %in% c(18, 108, 207, 306, 504, 999))
sims_3_one <- subset(sims_3, subset = sampled_numbers == as.numeric(levels(sims_3$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_5$sampled_numbers)), selection)
levels(sims_5$sampled_numbers)
sims_5_range <- subset(sims_5, subset = sampled_numbers %in% c(25, 115, 215, 305, 505, 995))
sims_5_one <- subset(sims_5, subset = sampled_numbers == as.numeric(levels(sims_5$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_10$sampled_numbers)), selection)
levels(sims_10$sampled_numbers)
sims_10_range <- subset(sims_10, subset = sampled_numbers %in% c(40, 130, 220, 300, 500, 1000))
sims_10_one <- subset(sims_10, subset = sampled_numbers == as.numeric(levels(sims_10$sampled_numbers)[2]))

xlab <- "Sample Sizes"
ylab <- "Max. Distances Between Treatment Groups"
title <- "Distances Between Treatment Group Means in Randomized and Blocked Data"

plot_first <- ggplot(sims_2_one, aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.85, 0.75)) # first plot outside of the loop because of the legend

sims_plots <- list(sims_2_range, sims_3_one, sims_3_range, sims_5_one, sims_5_range, sims_10_one, sims_10_range) # list of all subsets for plotting
plots <- list()
for(i in 1:(length(sims_plots))){
   plots[[i]]  <- ggplot(sims_plots[[i]], aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + guides(fill=FALSE)
  } # create plot for each data subset

```

```{r Boxplot-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Imbalances between treatment groups for blocked and randomized data with increasing sample size for 2 (top row), 3 (second row), 5 (third row), and 10 treatment groups (bottom row). Leftmost pair on each right panel is exactly the pair in the left panel.\\label{BoxLawLarNum}"}

grid.arrange(plot_first, plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]], plots[[7]], ncol = 2, nrow = 4, bottom=xlab,  top = title, left=ylab)

```


```{r Law of Large Numbers Plotting Code Histograms, include=FALSE}

ylab <- "Density"
xlab <- "Max. Distances Between Treatment Groups"
title <- "Distribution of Treatment Group Differences in Randomized and Blocked Data"

plot_first_more <- ggplot(sims_2, aes(x=diff, fill=label)) + geom_histogram(alpha=0.2, aes(y=..density..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.6, 0.75)) # first plot outside of the loop because of the legend

sims_plots_more <- list(sims_3, sims_5, sims_10) # list of all data sets (minus for 2 groups for plotting)

plots_more <- list()
for(i in 1:(length(sims_plots_more))){
   plots_more[[i]] <- ggplot(sims_plots_more[[i]], aes(x=diff, fill=label)) + geom_histogram(alpha=0.2, aes(y=..density..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + guides(fill=FALSE) # create plot for each data set
}
```


```{r Hist-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=4, fig.cap="Distribution of imbalances between treatment groups for the same blocked and randomized data for 2 (top left), 3 (top right), 5 (bottom left), and 10 (bottom right) treatment groups\\label{HistLawLarNum}"}

grid.arrange(plot_first_more, plots_more[[1]], plots_more[[2]], plots_more[[3]], ncol = 2, nrow = 2, bottom = xlab, top = title, left = ylab)

```


To apply blocking, a researcher needs to know all the covariate information for all participants before assigning treatment. Oftentimes, however, this is not possible as participants arrive for assignment at differing times. The researcher knows the covariate information of the arriving participant and (if she is not the first one) of the previous already assigned participants, but she does not have any information about future incoming participants. She thus needs more advanced statistical methods to block these participants. One such method is sequential blocking. Sequential blocking assigns each participant based on information from previously assigned participants and the incoming participant herself, while participants are still arriving to be assigned. Sequential blocking uses statistical methods to determine which group to assign a participant to. These measurements differ depending on the form that the covariate information comes in. Some covariate information is binary (e.g. gender) while other is categorical (e.g. party ID, race), to name only two. Research has shown that sequential blocking greatly improves balance in small samples [@moore_blocking_2013]. 


While sequential blocking is possible for a variety of variable forms, there is currently no method to sequentially block on ordinal variables, such as education, without reducing available information or making assumptions. Substantively, this means that a researcher currently cannot block incoming participants based on their provided level of education. This is problematic for political science, as education represents one of the strongest predictors of political behavior. I develop a new method of sequential blocking for ordinal variables that fills this gap. This method is freely available to other researchers as a package using the open-source software R. The R package provides functions that allow researchers to apply sequential blocking for ordinal variables in both in-person and online survey experiments. Substantively, I employ this method in simulated data, external data from political science experiments published in peer-reviewed journal articles, and original data in the form of an online and face-to-face survey experiment on the importance of moral arguments in political framing. The following sections provide background on sequential blocking and complete randomization, showcase the statistical development of the new ordinal sequential blocking method, and give more detail about the data to be used with this method.


## Theory {#ordblock-theory}


### Preliminary Notations on Randomization {#ordblock-theory-notations}

<!--
The current ATE is not an average -- change that.
SATE and TATE (don't name it that) currently show the same thing -- one has got to be wrong/off.
In the (currently wrong) ATE, I define y_1i and y_01 as beta_i, but then I use y_1i and y_01 again for SATE and TATE -- be consistent.
-->

The simplest of experiments has two potential outcomes for participants $i$: $y_{1i}$ and $y_{0i}$, with 1 denoting the treatment and 0 referring to the control. If a researcher, for example, intends to analyze the effect of a 2-week mathematic training camp on high school students' performance in exams, she devises an experiment where one half of her student sample participates in the camp (treatment) and the other half abstains (control). At the end of the camp, both groups take the same exam. If both groups of students look the same regarding their covariates (age, mathematic skill, intelligence etc.), a comparison of the groups' average test results reveals the Average Treatment Effect (ATE): 

\begin{equation}
\beta_i = y_{1i} - y_{0i}
\end{equation}

More specifically, for a sample of $n$ students, this comparison reveals the Sample Average Treatment Effect (SATE):

\begin{equation}
\frac{1}{n} \sum\limits_{i=1}^n (y_{1i} - y_{0i})
\end{equation}

A central characteristic of such a comparison is the fundamental problem of causal inference [@holland_1986_statistics;@rubin_1974_estimating]: We are unable to observe both potential outcomes for the same participant at once. In our case, we cannot observe how well student A performs in the exam after participating in the camp whilst also observing how well the same student A would have performed without taking part in the camp. If we could, it would be simple to calculate the true average treatment effect [@moore_2012_multivariate]:

\begin{equation}
\overline{TE} = \frac{1}{2n} \sum\limits_{i=1}^{2n} (y_{1i} - y_{0i})
\end{equation}

Since the TATE is unobservable, we need to use statistical means to assess the counterfactuals. This can be done by balancing the treatment and control groups. If both groups of students look the same in every measure, we can use the students who participated in the camp (treatment) to estimate what would have happened to the students who did not participate (control) if the abstaining students had in fact attended the camp. The crucial aspect is of course whether the two groups do indeed look the same in terms of the students' covariates. There are two main mechanisms by which this can be achieved: Complete randomization and blocking.
<!--
This is only a means, not the end. The requirement is that the potential outcome of the treatment reflects what would have happened for the control units. Rephrase the last sentence(s) above to reflect that.
-->


### Complete Randomization and Blocking {#ordblock-theory-complete}

Complete randomization is equivalent to flipping a coin for each participant to be assigned to treatment or control. This chance procedure gives each participant an equal chance of being assigned to either group (or groups, in case of multiple treatment groups) [@lachin_1988_properties]. Complete randomization increases covariate balance as $n$ increases [@imai_2009_essential]. The larger a researcher's sample, the better the resulting balance from complete randomization in expectation. Probabilistically, complete randomization enables the comparison of the SATE to be unbiased, which allows the researcher to attribute any treatment effects to the treatment [@king_a-politically_2007]. 

While complete randomization thus guarantees balance as the sample size reaches infinity, it often does not do so in the naturally finite sample sizes researchers actually work with. With huge samples, the Law of Large Numbers predicts that treatment groups 'selected' through complete randomization will be balanced. With small samples, however, it is possible to get 'unlucky' and end up with unbalanced groups [@imai_2008_misunderstandings]. Blocking can help achieve balance in such scenarios [@epstein_2002_rules].

Blocking involves the arrangement of participants in groups that are similar to one another in terms of the participants' covariates. As mentioned above, the key aspect in experimental studies is whether the treatment and control groups look the same. In complete randomization, this is achieved by random chance. In blocking, this is achieved by combining covariate information about the participants with randomization. Researchers use observed covariates to create similar treatment and control groups before treatment is assigned. Blocking is thus better suited to achieving balance in finite samples, as it "directly controls the estimation error due to differing levels of observed covariates in the treatment and control groups" [@moore_2012_multivariate, p. 463].

Efficient blocking focuses on covariates that affect the outcome. In U.S. politics, for instance, it is widely established that party ID represents one of the major driving forces behind public opinion [@abramowitz_disappearing_2010; @druckman_how_2013; @fiorina_culture_2011; @king_polarization_1997]. Making sure that participants in treatment and control groups have identical levels of party ID is thus crucial in experiments that test the effect of interventions on public opinion. 


### Blocking v. Post-Experiment OLS Controls {#ordblock-theory-ols}

Researchers often employ complete randomization in survey experiments and control for covariates post-hoc in OLS regression models. This is done to increase balance and model precision. Proponents of this approach might question the need for blocking approaches if post-hoc OLS controls can achieve similar balance and are much less computationally and mathematically complex. While controlling for covariates undoubtedly improves balance over 'basic' complete randomization, it does not achieve the level of balance that blocking can provide. 

Most crucially, this is because any OLS model requires assumptions about the choice of controls to include. More often than not, it is very difficult if not impossible to determine all needed controls. Such assumptions are not required in blocked experiments. 

<!--
The above two sentences are misleading. They imply we don't need to make assumptions about what controls to include in our model -- but we always need to choose covariates to measure. Otherwise how would we get the blocking variables then?
--> 

Additionally, blocking balances variation pre-experiment, which post-hoc controls cannot provide. Finally, it stands to reason that ex-ante balance prior to treatment is preferable to post-hoc adjustments as it removes the possibility for errors to be introduced.


### Sequential Blocking Approaches {#ordblock-theory-approaches}

Sequential blocking is a special form of blocking. Generally, there are two main starting positions for the researcher who wants to conduct an experiment: (1) The researcher knows all covariate information of all participants at the time of randomization, and (2) covariate information of participants 'trickles in' as the experiment progresses and participants arrive for assignment. Complete randomization, i.e. flipping a coin for each participant to be assigned to treatment or control, is possible in both positions. 'Normal', or for our purposes 'nonsequential', blocking can only be used in scenario 1: The researcher possesses all information about all participants when the experiment begins. Sequential blocking, on the other hand, is applied when the researcher does not know all covariate information of all participants when the experiment begins. Instead, she only possesses the covariate information of the current incoming participant and previously assigned participants, but not future incoming participants. This is scenario 2. Sequential blocking is thus blocking 'on the go'.

When all participant characteristics are known when the experiment begins (scenario 1), sequential blocking is not necessary, as no participants arrive individually for treatment. In many political experiments, researchers have an already-collected data set in front of them at the start of the experiment and then randomize a set of households, precincts, or individuals to treatments all at once. All covariate information on all participants is known, i.e. the characteristics of all participants are known at the time of randomization. Examples are any studies that use pre-existing databases. A prominent example are the American National Election Studies (ANES) that are often used to analyze voter turnout [see for instance @jackman_2018_does; @leighley_who_2014].

In these experiments, 'nonsequential' blocking suffices to create homogeneous groups within which treatments can be assigned. In an experiment of scenario 2, however, the researcher has much less information about the eventual full sample to use in assigning treatment than in an experiment with pre-existing data sets. More advanced statistical methods are thus required to exploit available background information to block 'on the go', i.e. to block sequentially. @chow_2007_adaptive identify four types of sequential experiments, which are shown in Table \ref{seq-rand-designs}.

\begin{table}
\caption{Different Randomization Designs For Sequential Experiments}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{>{\itshape}l l}
\bottomrule 
\midrule
Non-Adaptive & Assignment probabilities (AP) fixed throughout the trial\\
Treatment-Adaptive & AP change based on numbers of participants already in treatment groups\\
Response-Adaptive & AP change as a function of previous participants' outcomes\\
Covariate-Adaptive & AP change based on previous participants and the current participant\\
\bottomrule
\multicolumn{2}{l}{\footnotesize{Adapted from Chow and Chang (2007)}} \\
\end{tabular}}
\label{seq-rand-designs}
\end{table} 

Non-adaptive randomization and treatment-adaptive randomization can (and often do) ignore the researcher's detailed data on participants. They thus leave important information lying on the table. Response-adaptive randomization varies probabilities based on knowledge about previous participants' outcomes. This is likely very useful for clinical trials, where one outcome is often the clear goal: Consider a test where the treatment group receives an experimental medication to fight cancer and the control group receives a placebo. If it becomes clear as the trial progresses that the medication is effective in fighting cancer, it makes sense to assign more people to treatment, as this could potentially save their lives. In political science, this is not the case. Assessing whether a certain survey treatment affects participants' support for a political issue is not a matter of life and death. This leaves covariate-adaptive randomization (CAR), which varies probabilities based on knowledge about previous participants and the current participant, which appears ideally suited.


#### Basic Covariate-Adaptive Randomization (CAR) Approaches {#ordblock-theory-approaches-basic}

There are two traditional approaches to covariate-adaptive randomization (CAR). The first approach is the biased coin design developed by @efron_1971_forcing, which sets the current participant's treatment assignment probability using its entire covariate profile at once. I follow Chow and Chang's brief description of Efron's design [@chow_2007_adaptive] here:

\begin{equation}
  P(\delta_j \lvert \Delta_{j-1}) = 
  \begin{cases}
    0.5 & \text{if $N_A(j) = N_B(j)$},\\
    \text{$p$} & \text{if $N_A(j) < N_B(j)$}, \\
    1 - \text{$p$} & \text{if $N_A(j) > N_B(j)$}.
  \end{cases}
\end{equation}

<!--
I don't actually explain what the majority of the symbols in that equation mean. I need to give the reader guidance what means and stands for what.
-->

$\delta_j$ is a binary indicator for treatment assignment of the $j$th participant ($\delta_j=1$ for treatment A; $\delta_j=0$ for treatment B). $\Delta_{j-1} = \left\{ \delta_1, ..., \delta_{j-1} \right\}$ is the the cumulative set of treatment assignments up to participant $j-1$. Imbalance is measured by

\begin{equation}
\lvert D_n \rvert = \lvert N_A(n) - n \rvert.
\end{equation}

<!--
Same here: Tell the reader what the D and N are supposed to mean/be.
-->

The second approach is minimization, developed by @pocock_1975_sequential, which considers covariates one at a time and can limit marginal imbalance across an arbitrarily large set of covariates. I follow Rosenberger and Lachin's overview of the method here [@rosenberger_2002_randomization]: Let $N_{ijk}(n), i = 1, ..., I, j = 1, ..., n_i, k = 1,2$ (1 = treatment A, 2 = treatment B), be the number of participants in category $j$ of covariate $i$ on treatment $k$ after $n$ participants have been randomized. Suppose the ($n + 1$)th participant to be randomized is a member of categories $r_1, ..., r_I$ of covariates $1, ..., I$. Let $D_i(n) = N_{ir_{i}1}(n) - N_{ir_{i}2}(n)$, with the weighted difference measure being $D(n) = \sum\limits_{i=1}^I w_iD_i(n)$. $w_i$ are weights the researcher chooses based in the perceived importance of covariates. If $D(n)$ is lower than 0, then the weighted difference measure indicates that treatment B was chosen more often so far for that set, $r_1, ..., r_I$, of strata, and that participant $n + 1$ should be assigned with higher probability to treatment A. This argument holds vice versa if $D(n)$ is greater than 0. According to Pocock and Simon, the coin should be biased with

\begin{equation}
p = \frac{c^* + 1}{3}
\end{equation}

and the following rules implemented: If $D(n) < 0$, assign the next incoming participant to treatment A with probability $p$. If $D(n) > 0$, assign the next incoming participant to treatment A with probability $1-p$. Finally, if $D(n) = 0$, assign the next incoming participant to treatment A with probability 1/2, where $c^* \in [1/2,1]$, thus $p \in [.5, \frac{2}{3}]$.

The biased coin and minimization approaches use discrete covariates with a very small number of levels [@moore_blocking_2013]. Discrete covariates allow for simpler pairing procedures, as the number of possible covariate levels is finite. This is not the case for continuous covariates, where the number of possible covariate levels rises exponentially. Blocking on continuous covariates is thus not possible with these traditional approaches [@markaryan_2010_exact; @rosenberger_2002_randomization;@eisele_1995_biased].



#### Extended CAR for Continuous Covariates {#ordblock-theory-approaches-continuous}

In the standard approaches, the incoming participant is assigned to the treatment group with the fewest participants with identical covariate information. With continuous covariates, no two participants will look the same, and thus identical participants do not exist. @moore_blocking_2013 develop a method to include continuous variables in sequential blocking with CAR by exploiting relationships between the current participant's covariate profile and those of all previously assigned participants.

@moore_blocking_2013 define the similarity between participants with the Mahalanobis distance (MD) between participants $q$ and $r$ with covariate vectors $\bm{x}_q$ and $\bm{x}_r$: \newline \noindent $MD_{qr} = \sqrt{(\bm{x}_q - \bm{x}_r)' \reallywidehat{\sum}^{-1} (\bm{x}_q - \bm{x}_r)}$. To aggregate pairwise similarity, they implement the mean, median, and trimmed mean of the pairwise MDs between the current participant and the participants in each treatment condition: Participants are indexed with treatment condition $t$ using $r \in \{1,...,R\}$. For each condition $t$, an average MD between the current participant, $q$, and the participants previously assigned, $t$. If the distance in terms of MD for the incoming participant is 2 in the control and 5 for the treatment condition, the incoming participant looks more similar to the control condition. To set the probability of assignment, @moore_blocking_2013 test several methods, most notably a set of strategies that calculates the mean Mahalanobis distances for each incoming participant, $q$, for all treatment conditions, $t$, and sorts the treatment conditions by these averages. Randomization is biased towards conditions with high scores. For each value of $k$, with $k \in \{2,3,...,6\}$, the condition with the highest average MD is then assigned a probability $k$ times larger than all other assignment probabilities. 

@moore_blocking_2013 extend basic blocking approaches to provide a method to apply sequential blocking to continuous covariates. They show that their approach  significantly improves balance for some experiments. I extend blocking approaches further by developing a method to sequentially block on ordinal covariates. 


### Ordinal literature stuff

Agresti (2010): looks like a great book. The most useful for me is chapter 2, which talks about using cumulative probabilities, ridits (average cumulative proportion scores), midranks (similar thing), and cumulative odds ratios. He also has a good chapter on Bayesian estimation

Casacci \& Pareto (2015): present a univariate approach that allows to estimate the category values of an ordinal variable from the observed frequencies on the basis of a distributional assumption -- but they offer next to no justification, so it all looks a bit fishy

Winship \& Mare (1984): say natural extensions of probit and logit models can be used here

Lucadamoa \& Amentaa (2014): their approach consists of quantifying each non-quantitative variable according to the empirical distributions of the variables involved in the analysis assuming the presence of a continuous underlying variable for each ordinal indicator

Gertheiss \& Tutz (2008): talk about penalized regression techniques and include a Bayesian perspective

Agresti (1990, p. 294): mentions two different approaches to assigning scores to ordinal variables: One where the scores are preassigned before analysis (but his recommendations here are for the scale ``to be chosen by a consensus of `experts'", which is useless for me), and one where the scores are parameters to be estimated from the data

Most literature thus deals with how to analyze ordinal variables once the data is in, to analyze the results. These approaches seem to incorporate the distribution of each the categorical variable's values in some way, e.g. when all the data is in front of you

Jeff suggested an ordered probit model, which fits with what a lot of the literature is saying. In particular, he suggested to run \texttt{polr()} with \texttt{education} as the DV and several common EVs on a good data set, like the 2016 ANES. That trains the model, gives me thresholds for each education category, bins the observations as cases according to the new thresholds, and results in the data-fitting education categories we should be using. I then block on that as before



### Sequential Blocking on Ordinal Covariates {#ordblock-theory-ordinal}

Ordinal variables represent a special form of categorical variable. Categorical variables represent types of data which are commonly divided into three groups: Nominal, interval, and ordinal variables. Nominal variables are categorical variables have two or more categories that are not intrinsically ordered. Examples include gender (female, male, transgender etc.) and race (black, white, Hispanic etc.) where the categories cannot be ordered sensibly into highest or lowest. Interval variables are ordered categorical variables with evenly spaced values. Examples include temperature where the difference between 50 degrees and 60 degrees is the identical to the difference between 80 and 90 degrees. Ordinal variables are ordered categorical variables where the spacing between values is not the same. Examples include education (elementary school, high school, some college etc.) where the distance between "elementary school" and "high school" is likely much bigger than the distance between "high school" and "some college". Each subsequent level has quantitatively more education than the previous, but the exact measure of the difference between the levels is unclear.  

In data analysis, nominal variables cannot be quantified as it does not make sense to compute statistical quantities here, e.g. a mean gender (with "male" being coded as 0 and "female" as 1) of 0.7 would be substantively meaningless. They can, however, be transformed into dummy variables, e.g. gender could be transformed into the two dummy variables "male" and "female". This manipulation does not impose any unnatural ordering onto the variable and thus does not require any theoretical assumptions.  Interval variables can be quantified as they are ordered and evenly spaced. Computing statistical quantities of interval variable is thus statistically sound, e.g. a mean temperature of 64 degrees makes substantive sense. Ordinal variables cannot sensibly be turned into dummy variables nor quantified without making unfounded assumptions. Were "elementary school", "high school", and "some college" turned into three separate dummy variables, the researcher would wrongly assume that there is no ordering to these values. Were these values quantified into the numeric labels 1, 2, and 3, the researcher would wrongly assume that these values are evenly spaced. In both cases, crucial information would be lost. This matters for political science because education, an ordinal variable, is the strongest predictor of political behavior such as turnout or donations [@dawood_campaign_2015; @fiorina_disconnect_2009; @leighley_who_2014]. Measuring this important variable based on unfounded assumptions is highly problematic. Ordinal variables thus represent a special case that demand our attention.


Specialized models dealing with ordinal variables use their inherent ordering information efficiently. This makes a simpler, more underlying description of the data possible [@agresti_2010_analysis].

Ordinal variables are abundant in the social sciences to measure attitudes and opinions, e.g. Likert scale.

Many well-known statistical measures ignore ordinality, e.g. the $p$-value. 

"Examples show that the type of ordinal method used is not that crucial , in the sense that we obtain similar substantive results with ordinal logistic regression models, loglinear models, models with other types of response functions, or measures of association and nonparametric procedures. These results may be quite different, however, from those obtained using methods that treat all the variables as nominal" [@agresti_2010_analysis, p. 3].

Advantages of using ordinal variables' ordinality:
(1) A greater variety of models can be used
(2) Greater power and possibility to detect trends instead of the null hypothesis
(3) Standard nominal models are often too trivial

Common (problematic) approach in practice: Ignore ordinal nature of response variable, assign evenly spaced numeric scores to the categories, use OLS regression. Problems with that: There is usually not a clear-cut choice for the scores. 1, 2, 3? 2, 4, 6? 1, 4, 5?

They key point is that, to truly use the ordinal nature of a variable, we need to use both its quantitative and its ordered inherent aspects. 

"The most common methods of statistical analysis, such as correlation and regression analysis, structural equation analysis, exploratory and confirmatory factor analysis, assume that the variables of interest possess continuous interval level measurement. In other words, it is assumed that the intervals between adjacent variable values are nonarbitrary" [@casacci_2015_methods].

Integer scoring method: Assigning numbers with equal intervals, i.e. 1, 2, 3, 4, 5. The assumption behind this method: It does not matter that an interval variable background is implemented because it does not affect the underlying variable structure. This assumption is problematic because it can lead to a large degree of distortion [@obrien_1981_using].



#### Extended CAR for Ordinal Covariates {#ordblock-theory-ordinal-car}

Consider the following simple example for a common ordinal variable in political science: Education. Education is a strong predictor for political behavior such as turnout or donations [@dawood_campaign_2015; @fiorina_disconnect_2009; @leighley_who_2014]. A researcher conducts a basic experiment with one treatment and one control group. The researcher also collects information about education on three levels: Low, medium, and high. For all already-assigned participants, let the education distributions across these three levels look like figure \ref{fig:Plot-Ordinal-Education-Example} for both groups.


```{r Ordinal Education Example, include=FALSE}

treatment_groups <- c("control", "treatment")
education_levels <- c("Low", "Medium", "High")
ord.inc.ex <- matrix(NA, 100, length(treatment_groups))  # empty container with 100 rows, 2 columns)
ord.inc.ex[,1] <- c(rep(education_levels[1], 40), rep(education_levels[2], 40), rep(education_levels[3], 20)) 
    # fill with percentages of low, medium, high education
ord.inc.ex[,2] <- c(rep(education_levels[1], 30), rep(education_levels[2], 10), rep(education_levels[3], 60))
    # fill with percentages of low, medium, high education
colnames(ord.inc.ex) <- treatment_groups
ord.inc.ex <- data.frame(ord.inc.ex)

# change levels so that graphs are shown in ascending order of education and so incoming participant's education is level 1
ord.inc.ex[,1] <- factor(ord.inc.ex[,1], levels = c(education_levels[1], education_levels[2], education_levels[3]))
ord.inc.ex[,2] <- factor(ord.inc.ex[,2], levels = c(education_levels[1], education_levels[2], education_levels[3]))

# store the number of education levels as I
I <- length(education_levels)

# store the number of treatment groups as J
J <- length(treatment_groups)

# empty container to store calculations of weighted level percentages
s_j <- matrix(NA, I, J)

# turn into dataframe
s_j <- data.frame(s_j)

# loop to calculate weighting factors for all levels I for all columns J
for (dy in 1:I){
  for (dx in 1:J){
    s_j[dy,dx] <- dy * (1 / (J + 1))
  }
}

# reverse row ordering, since otherwise "Low" would be last (I want it first)
s_j <- s_j %>% arrange(-row_number())
 
# loop to multiply the weighting factors by the level proportions for all levels I for all columns J
for (db in 1:I){
  for (dq in 1:J){
    s_j[db,dq] <- s_j[db,dq] * (length(ord.inc.ex[,dq][ord.inc.ex[,dq] == levels(ord.inc.ex[,dq])[db]]) / nrow(ord.inc.ex))
  }
}

# name rows and columns of resulting dataframe (just for aesthetics)
colnames(s_j) <- treatment_groups
row.names(s_j) <- education_levels

# the idea behind these elaborate loop constructions is to make the code generalizable
# that way I only have to create "treatment_groups", "education_levels", and "ord.inc.ex"
# everything else is handled automatically by the code, no matter how many levels and/or treatments

plot.control <- ggplot(ord.inc.ex, aes(x = control)) + geom_bar(aes(y = ..count..), stat = "count", fill="navyblue") + theme(axis.title.x = element_blank()) + ggtitle("Control") + theme(plot.title = element_text(hjust = 0.5)) + scale_y_continuous(name = "Percent", labels = c(0, 10,20,30,40,50,60), breaks = c(0, 10,20,30,40,50,60), limits = c(0,60))

plot.treatment <- ggplot(ord.inc.ex, aes(x = treatment)) + geom_bar(aes(y = ..count..), stat = "count", fill="darkred") + theme(axis.title.x = element_blank(), axis.title.y = element_blank()) + ggtitle("Treatment")+ theme(plot.title = element_text(hjust = 0.5)) + scale_y_continuous(labels = c(0, 10,20,30,40,50,60), breaks = c(0, 10,20,30,40,50,60))

```


```{r Plot-Ordinal-Education-Example, echo=FALSE, fig.width=10, fig.height=4, fig.cap="Example distributions of 3-level education in treatment and control groups\\label{OrdIntEx}"}

grid.arrange(plot.control, plot.treatment, ncol=2)

```


A new participant arrives with low education. We need a suitable measure, or measures, to define the dissimilarity between this incoming participant and the already assigned participants. It would be easy to transform these education levels into the numeric values of 1, 2, and 3, and simply apply the Mahalanobis distance used for discrete variables. In doing so, however, we would assume that the distance between 1 and 2 is the same as the distance between 2 and 3. We do not have to make such a strong assumption if we keep the ordinal levels low, medium, and high. Similarly, we could transform low, medium, and high education into three dummy variables. Education would then look like non-ordinal variables such as race or gender. Crucial information would be lost. We cannot employ the mean because there is no mean for ordinal variables. Exact matching, and with that Coarsened Exact Matching [@iacus_2011_causal], might suggest itself, since education is not a continuous variable. Unlike matching, however, we are not looking to create identical pairs of participants. We are comparing one participant with one specific level of education with the whole distribution of education across many other, previously-assigned participants. The distribution rarely has one specific level, therefore there will not be exact matches for the majority of incoming participants. In order to exploit the unique information contained in ordinal variables, we need different measures.

I propose a weighted approach that incorporates all levels of the respective ordinal variable in question. Using only the incoming participant's education level of the distribution for comparison would ignore all the available information on the two other education levels, medium and high, in all treatment groups, and treat education as a one-level variable, thereby removing its very nature of an ordinal variable. I propose to develop the following algorithm: Let an ordinal variable have levels $m_i = m_1, m_2, ..., m_I$. Let the level of an incoming participant be $m_1$. Let the number of treatment groups be $t_j = t_1, t_2, ..., t_J$, where $J$ can take on any number to account for binary and multi-category treatments. For each treatment group, we calculate the proportion of participants with level $m_1$, $p(m_1)$, and weight this proportion by $\frac{I}{J+1}$. For each treatment group, we then calculate the proportions of participants in the next level $m_2$, $p(m_2)$, and weight this proportion by $\frac{I-1}{J+1}$. This is continued until the proportion of participants for the last level $m_I$, $p(m_I)$, which is weighted by $\frac{1}{J+1}$. This gives us $S_j$, the weighted average of the education distribution for each treatment group:

\begin{equation}
S_j = \frac{I}{J+1} \times p(m_1) + \frac{I-1}{J+1} \times p(m_2) + \frac{I-2}{J+1} \times p(m_3) + ... + \frac{1}{J+1} \times p(m_I)
\end{equation}

which can be transformed into

\begin{equation}
S_j = \sum_{i=0}^{I-1} (\frac{I-i}{J+1} p(m_i))
\end{equation}

Note that the proportions over all the treatment groups sum up to one, $\sum\limits_{i=1}^I p(m_I) = 1$. Note also that the sum of all weights equals $\frac{I}{2} \times (I+1)$. The numerators for the respective level proportions are set in descending order because $m_1$, as the ordinal variable level of the incoming participant, is the most important in the distribution, followed by $m_2$, then $m_3$, and so on. Since we are adding up weighted proportions, a higher weighted average $S_j$ represents more similarity to the incoming participant. She will thus be assigned to the treatment group with the lowest $S_j$.

In the simple education example given above, the education variable has three levels, thus $I=3$ (Low, Medium, and High Education), and there are two treatment groups, thus $J = 2$. The incoming participant has low education, thus $m_1 = \text{Low}$. In the control group, $p(\text{Low}) =$ `r length(ord.inc.ex[,1][ord.inc.ex[,1] == levels(ord.inc.ex[,1])[1]]) / nrow(ord.inc.ex)`, $p(\text{Medium}) =$ `r length(ord.inc.ex[,1][ord.inc.ex[,1] == levels(ord.inc.ex[,1])[2]]) / nrow(ord.inc.ex)`, and $p(\text{High}) =$ `r length(ord.inc.ex[,1][ord.inc.ex[,1] == levels(ord.inc.ex[,1])[3]]) / nrow(ord.inc.ex)`. $S_1$ for the control group thus is:

\begin{equation}
S_{\text{Control}} = \frac{3}{2+1} \times `r length(ord.inc.ex[,1][ord.inc.ex[,1] == levels(ord.inc.ex[,1])[1]]) / nrow(ord.inc.ex)`+ \frac{2}{2+1} \times `r length(ord.inc.ex[,1][ord.inc.ex[,1] == levels(ord.inc.ex[,1])[2]]) / nrow(ord.inc.ex)` + \frac{1}{2+1} \times `r length(ord.inc.ex[,1][ord.inc.ex[,1] == levels(ord.inc.ex[,1])[3]]) / nrow(ord.inc.ex)` = `r round(sum(s_j[,1]), digits=3)`
\end{equation}


In the treatment group, $p(\text{Low}) =$ `r length(ord.inc.ex[,2][ord.inc.ex[,2] == levels(ord.inc.ex[,2])[1]]) / nrow(ord.inc.ex)`, $p(\text{Medium}) =$ `r length(ord.inc.ex[,2][ord.inc.ex[,2] == levels(ord.inc.ex[,2])[2]]) / nrow(ord.inc.ex)`, and $p(\text{High}) =$ `r length(ord.inc.ex[,2][ord.inc.ex[,2] == levels(ord.inc.ex[,2])[3]]) / nrow(ord.inc.ex)`. $S_2$ for the treatment group thus is:

\begin{equation}
S_{\text{Treatment}} = \frac{3}{2+1} \times `r length(ord.inc.ex[,2][ord.inc.ex[,2] == levels(ord.inc.ex[,2])[1]]) / nrow(ord.inc.ex)` + \frac{2}{2+1} \times `r length(ord.inc.ex[,2][ord.inc.ex[,2] == levels(ord.inc.ex[,2])[2]]) / nrow(ord.inc.ex)` + \frac{1}{2+1} \times `r length(ord.inc.ex[,2][ord.inc.ex[,2] == levels(ord.inc.ex[,2])[3]]) / nrow(ord.inc.ex)` = `r round(sum(s_j[,2]), digits=3)`
\end{equation}

We thus assign the incoming participant with low education to the treatment group.

<!--
Ryan: What if m_1 = medium? Then what?
-->



### Computational Development {#ordblock-theory-computational}


#### R Package: BlockExperiments {#ordblock-theory-computational-r}

The new sequential blocking method I develop is built in R and made freely available to other researchers as the R package `BlockExperiments`. `BlockExperiments` includes the functions provided by `blockTools` [@moore_2016_package] and further improves on the state of the art. By incorporating `blockTools`, `BlockExperiments` thus allows the user to apply sequential blocking to binary, continuous, and ordinal variables. It also includes web-components that allow the user to create a finished web-based survey questionnaire that can be directly fielded online. `BlockExperiments` can thus be employed for in-person sequential survey experiments as well as online sequential survey experiments.

Figure \ref{in-person-workflow} shows a sample workflow for the use of `BlockExperiments` in in-person experiments. Processes performed by the software in R are shown as blue boxes. Actions that need to be completed by the user outside of R are shown as red clouds.

\begin{figure}[ht]
\centering
\begin{tikzpicture}
    \node [block3]  (sequential) {\scriptsize{Sequential Blocking}};
    \node [cloud, left= 0.6cm of sequential] (demographics) {\scriptsize{Demographics}};     
    \node [block2, right= 0.6cm of sequential] (assignment) {\scriptsize{Assignment}};
    \node [cloud, right= 0.6cm of assignment] (treatment) {\scriptsize{Treatment}};
    \node [cloud, right= 0.6cm of treatment] (results) {\scriptsize{Results}};
	\path [line] (demographics) -- (sequential);
	\path [line] (sequential) -- (assignment);
	\path [line] (assignment) -- (treatment);
	\path [line] (treatment) -- (results);
\end{tikzpicture}
\caption{In-Person Survey Experiment Workflow} \label{in-person-workflow}
\end{figure}

The user creates the survey questionnaire that collects demographic information from each incoming participant. 
<!--
Ryan said they did this (creating the survey questionnaire) in his 2013 paper.
-->

This information is uploaded into R. `BlockExperiments` uses this information and information from all previous participants to sequentially block the incoming participant and then assign her to a treatment group. The participant then receives the treatment designed by the researcher, and her responses are saved. This process is repeated for all incoming participants.

There is currently no way to use sequential blocking when surveys are fielded online. `BlockExperiments` provides a simple interface to do so, as shown in Figure \ref{online-workflow}. As before, processes performed by the software in R are shown as blue boxes, while user actions that need to be completed outside of R are shown as red clouds.

\begin{figure}[ht]
\centering
\begin{tikzpicture}
    \node [cloud]  (questionnaire) {\scriptsize{Questionnaire}};
    \node [block2, below right=0.8cm and -0.3cm of questionnaire] (local) {\scriptsize{Local Server}};
    \node [cloud, right= 0.9cm of questionnaire] (cloud) {\scriptsize{Cloud Server}};
    \node [block3, below right=0.8cm and -0.3cm of cloud] (sequential) {\scriptsize{Sequential Blocking}};
    \node [block2, right= 2.1cm of cloud] (assignment) {\scriptsize{Assignment}};
    \node [cloud, below right=0.8cm and -0.3cm of assignment] (treatment) {\scriptsize{Treatment}};
    \node [cloud, right= 1.2cm of assignment] (results) {\scriptsize{Results}};
	\path [line] (questionnaire) -- (local);
	\path [line] (local) -- (cloud);
	\path [line] (cloud) -- (sequential);
	\path [line] (sequential) -- (assignment);
	\path [line] (assignment) -- (treatment);
	\path [line] (treatment) -- (results);
\end{tikzpicture}
\caption{Online Survey Experiment Workflow} \label{online-workflow}
\end{figure}

The user uploads the complete survey questionnaire, i.e. questions that collect demographic information and questions that apply treatment,  as a `.csv` file into R. The `shiny` function within the R package creates a web survey structure by employing the user's R environment as the local 'host server'. This means that the survey is not accessible on the web yet -- it is located on the user's local computer environment. To make the survey public, the user hosts this website on a cloud server (e.g. Amazon Web Services, Blue Ocean). The created cloud-based public website is used directly to recruit participants. The website sequentially blocks each incoming participant based on her covariate information and covariate information from all previous participants. This is done through constant interaction with the R code provided by `BlockExperiments`, which is integral to the website build. Based on the sequential blocking, the incoming participant is assigned to a treatment group and then receives the appropriate treatment. Her responses are then saved on the server. This process is repeated for all incoming participants.  

To recruit participants, the cloud-based website can easily be linked to online market platforms, such as MTurk. MTurk is a service where researchers can host tasks to be completed by anonymous participants. Participants receive financial compensation for their work and Amazon collects a commission. MTurk samples have been shown to be internally valid in survey experiments [@berinsky_evaluating_2012]. The use of MTurk in political science experiments has increased dramatically over the past decade and is now common practice [@hauser_attentive_2016]. 

For online survey experiments, `BlockExperiments` thus creates its own web-based survey. A theoretic alternative to this approach is to use online survey design platforms, such as Qualtrics. Since Qualtrics is very popular, it would appear fruitful to have `BlockExperiments` load questions directly into Qualtrics, instead of hosting the questions on a remote website via `shiny`. However, this idea appears not computationally feasible. There have been attempts to combine R code work with Qualtrics: 

@hainmueller_2014_causal show how conjoint analysis enables researchers to estimate the causal effects of multiple treatment components and assess several causal hypotheses simultaneously. To do so, they develop a computer program that creates conjoint question design templates. The program exports the templates as a `.php` file, which is then uploaded to a web server, which can then in turn be loaded into Qualtrics via the Qualtrics functions Web Service and Embedded Data. This pipes the file into the Qualtrics question design environment. While this facilitates conjoint survey questionnaire design, it does not affect Qualtrics's randomization engine but 'only' loads question categories. @barari_2017_package develop the R package `cjoint`, which is designed to calculate Average Marginal Component Effects of conjoint survey data. It has a function called `makeDesign` which creates the conjoint survey design from output created by @hainmueller_2014_causal's Conjoint Survey Design Tool and also contains two functions that pull the final `.csv` file from Qualtrics directly into R. This tool then addresses the data analysis after the survey has run, but not Qualtrics's randomization engine. Similarly, the R package `QualtricsTools` [@testa_2017_qualtricstools] provides functions to analyze Qualtrics survey data by automatically processing the data into reports breaking down the responses to each question. A similar function is offered by the R package `qualtRics` [@ginn_2018_package], which pulls survey data from Qualtrics to analyze directly in R through the Qualtrics APIs. 

None of these tools, then, concern the 'injection' of R code into the Qualtrics randomization engine. @boas_fielding_2013 recommend using the R web tool `shiny` to achieve this as it is purposefully built to easily harnesses all the statistical power from R. It thus seems a much more feasible vehicle for my project.


<!-- 
Motivation for why anyone should use this method online (from Ryan in an email; I can decide whether I want to use it):

One of the implications of better balance is more precision (smaller SEs) and power in effect estimates. The interaction logic runs as:

* Balance in a "medium-sized" sample can be improved by blocking.  This creates more precision in the estimate of the treatment effect, akin to having a larger sample.  
* Balance in a "large" sample will be guaranteed by blocking, but is also likely under complete randomization (CR) or random allocation, when the sample is "large enough".  The balance from CR in a large sample *should* be sufficient to estimate the treatment effect with precision (depending on the variation in Y, of course).
* However, the balance that is likely to obtain under CR in a "large" sample may still lead to estimates of the interaction effect that are not as precise as they could be.
* Blocking the large sample could improve the precision of the interaction effect estimate, just as it could improve the precision of the treatment effect estimate, when compared to the average or an unlucky CR.
-->


## Data {#ordblock-data}

I demonstrate the improvements produced by my sequential blocking method by comparing balances in data with ordinal covariates from several data sources: Simulations, others' external data, and my own original survey data. In each source of data, I focus on the ordinal variable that is most common and most important in political science: Education.


### Simulated Data {#ordblock-data-simulated}

I simulate an experiment with common demographic covariates, such as age, race, gender, income, education, and party ID. The sample size is 1,000 participants, which is a common sample size for political science survey experiments. The experiment features 5 treatment groups, which results in around 200 participants per treatment group. Compared to prominent survey experiments such as @chong_framing_2007, who have fewer than 900 participants for 17 treatment groups, this is a conservative setup. The outcome variable is irrelevant, since we are only interested in balance here. I employ two separate randomization methods: Complete randomization, and my sequential blocking method. Each method is simulated 1,000 times. This means I simulate 1,000 runs of this experiment with complete randomization, and another 1,000 runs of this experiment with my sequential blocking method. The comparison of the distribution of the blocked $d^2$ $p$-values and the complete randomization balance demonstrates balance improvements produced by my method.


### External Data {#ordblock-data-external}

I also use data from external experimental survey studies and sequentially block the participants in these data for ordinal covariates. To create a sequential nature, I assume that the order of the observations in the replication data file represents their entry order into a sequential experiment. One example for such data is @tomz_2013_public, who explore whether American participants are more likely to support pre-emptive military strikes on non-democracies versus democracies. The authors present participants with different country profiles and ask participants whether they would support pre-emptive American military strikes against the hypothetical country. They randomly assign various characteristics of these profiles, including (1) whether the country is a democracy, (2) whether the country has a military alliance with the United States, and (3) whether the country has a high level of trade with the United States. As before, however, the outcome variable is irrelevant, since we are only interested in balance. I plan to use up to four suitable similar experimental survey data here, including @hainmueller_2014_causal's conjoint experiment data. If possible, I also plan to obtain the data from @chong_framing_2007 (which is currently not publicly available).


### Original Data {#ordblock-data-original}

For the final application, I employ original survey data. I design a questionnaire to assess the effect of moral frames on public opinion in a survey experiment. This questionnaire is fielded online on MTurk for a random sample of U.S. adults and face-to-face for undergraduate students at American University. In each experiment, I use my sequential blocking method on ordinal covariates to assign participants to treatment groups. While I analyze the substantive results in depth in [paper III][PAPER III: MORAL ARGUMENTS AS A SOURCE OF FRAME STRENGTH], they are irrelevant here, as we are only interested in balance. I then use the collected covariate information and re-assign participants to treatment groups with complete randomization. As before, the comparison of the distributions of the blocked $d^2$ $p$-values and the complete randomization balance demonstrates balance improvements produced by my method.

