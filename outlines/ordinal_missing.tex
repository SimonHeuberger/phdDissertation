\input{\string~"/Insync/templates/preamble"}

\title{Ordinal Variables and Missing Data}

\date{\today}

\begin{document}

\maketitle

\section*{General}
	\begin{coi}
		\item This paper should be an intersection of ordinal variables and missing data. Jeff thinks there is room there for a contribution
		\item The general idea is that general treatments of missing data (listwise deletion, multiple imputation etc.) lead to different results -- depending on the type of variable. How you treat \texttt{Don't Know} and \texttt{Refused} and how this treatment affects the results depends greatly on the type of variable that you use to impute the missing data. The idea is that you would need to approach things differently depending on whether you use a nominal, interval, or ordinal variable to fill in values for \texttt{Don't Know}/\texttt{Refused}. So I am developing a method to specifically treat \texttt{Don't Know}/\texttt{Refused} with ordinal variables that improves over current uses that are generic for all types of variables
		\item This method should be a customized form of multiple imputation that is closer to the data than general multiple imputation. Jeff and Skyler developed affinity scores and hot decking in their BJPS paper. They used the number of exact matches (in the form of other participants) to calculate the affinity score. Instead, I should use a weighted distance solution between ordinal variable categories. I would use the OP model from the blocking paper to weight the distances between the categories in matches (in the form of other participants). In other words, I would use the underlying ordered probit numbers to create the weights. So this would be a specific ordinal variable adjustment of the affinity score building
		\item Set up chapter structure
	\end{coi}


\section*{Code}
	\begin{coi}
		\item My code functions for one variable with NAs, for 10,000 iterations
			\begin{coi}
				\item I have saved results for \texttt{inc}, \texttt{age}, \texttt{Dem}, \texttt{Rep} for \texttt{hd.ord} and \texttt{hd.norm}
				\item For all 4, \texttt{na.omit} performs the best. This isn't surprising, since deleting observations with missing values shouldn't be a problem with MCAR
			\end{coi}
		\item My code does not function When run for several variables with NAs
			\begin{coi}
				\item It throws up a replacement error somewhere down the line when run for 10,000 iterations. It's never in the same place, so it must be something random
				\item Always the same: \# of NAs overall. Not always the same between some runs: \# of NAs per column, \# of rows with NAs
				\item Find out why my code doesn't work for NAs in several variables
					\begin{coi}
						\item The number of NAs inserted wasn't actually the issue. It was in the \texttt{OPMord} code
						\item I painfully ran 1,000 iterations for two variables, gradually adding one line at a time to the function. I discovered the error was where I assign values to \texttt{df.cases}, specifically where the columns are all combined and the resulting vector added to the data as \texttt{educ.new}
						\item I saved the vector to an empty vector, ran everything for 1,000 iterations, then looked at the vectors and the corresponding versions of \texttt{df.cases}
						\item There were rows with all NAs in \texttt{df.cases}, which makes the vector shorter than the data, which means it can't be assigned to it as a column. I reran the \texttt{df.cases} creation code to find the culprit
						\item I had overlooked that \texttt{int.df\$Values} has one value fewer than there are variable levels, since it lists cutpoints between the levels. For 6 levels, I had assigned \texttt{int.df\$length(levels(...))}, which picks the 6th value of \texttt{int.df\$Values} -- but it only goes to 5. I had to add \texttt{-1}. Now it's working
					\end{coi}
			\end{coi}
		\item Method to insert NAs
			\begin{coi}
				\item I tried \texttt{prodNA}, which works for \texttt{MCAR} but gives me nothing for \texttt{MAR}. And \texttt{MCAR} works well for \texttt{na.omit}, since by definition it's a random sample of missing data, which doesn't matter 
				\item Use \texttt{mice} \texttt{ampute()}
					\begin{coi}
						\item It has \texttt{MCAR} and \texttt{MAR} options. \texttt{MAR} is what I need
						\item It doesn't work on just one variable; it needs at least two
						\item \texttt{ampute()} works very well
					\end{coi}
			\end{coi}
		\item With the code adjusted (\texttt{-1}), \texttt{OPMord} now works. However, some of the \texttt{int.df}s don't have all the education levels: \texttt{int.df} with all levels has 6 rows (one fewer than the levels, since each row shows a cutoff), but some have 5. This means \texttt{OPMcut} now doesn't work
			\begin{coi}
				\item I could adjust \texttt{OPMcut} to work with fewer levels, but then I would be, in the end, taking means of most data with all levels and some data with fewer levels. That's problematic
				\item It's better to discard the iterations where \texttt{int.df} has 5 rows after \texttt{OPMord} has run and then continue with the other functions. I set the code up to do that. However, after about 40 percent of running 12,500 iterations, there was an error: I hadn't factored in that the \texttt{int.df}s could also have fewer rows than that. A few of them had 4 rows, which caused the error. I adjusted the code to now discard the iterations where \texttt{int.df} has anything other than 6 rows
			\end{coi}
		\item The data ran for 12,500 iterations, 80 percent NAs, \texttt{Rep} and \texttt{inc}, \texttt{hot.deck.ord}, \texttt{hot.deck.norm}, \texttt{na.omit}. The results looked promising: \texttt{hd.ord} performed better than \texttt{hd.norm} and \texttt{na.omit}
		\item More methods
			\begin{coi}
				\item Include \texttt{mice}
				\item Include \texttt{Amelia}
				\item Use the defaults for both since that's what over 90\% of people do (even if they shouldn't)
				\item Include \texttt{hot.deck.norm} on the original education values (I ran it on the cutpoints)
			\end{coi}
		\item \texttt{hot.deck impContinuous}
			\begin{coi}
				\item No need to use it, since no variable is continuous (age has more values than \texttt{sdCutoff} but it's nominal, not continuous)
			\end{coi}
		\item \texttt{hot.deck sdCutoff}
			\begin{coi}
				\item No need to change the default
			\end{coi}
		\item Add more variables/NAs
			\begin{coi}
				\item Add NAs to 6 of the variables
				\item Demographics I still have: Age(numeric), employment (5 levels), ideology (liberal, conservative, neither), following public affairs (ordinal, 4 levels), media interest (numeric, accumulative count of activities), participation (numeric, accumulative count of activities)
				\item I can't add them all, since \texttt{mice} and \texttt{Amelia} don't work when there is collinearity
				\item I used code to find and discard variables with high correlation
			\end{coi}
		\item Percentage of NAs
			\begin{coi}
				\item I'm currently using 80 percent. Jeff used 20, 50, and 80
				\item I did one run each for 20, 50, and 80 percent
			\end{coi}
		\item Preliminary results
			\begin{coi}
				\item Across the board, \texttt{hd.ord} performs better than normal \texttt{hot.deck}, but worse than \texttt{Amelia} and \texttt{mice}
				\item Across the board, \texttt{Amelia} is better than \texttt{mice} for \texttt{inc} and \texttt{age}. It's also better for \texttt{interest}, except for 20 percent NAs, where \texttt{mice} wins
				\item For \texttt{Dem}, \texttt{Female}, and \texttt{White}, \texttt{Amelia} also edges first, thought \texttt{mice} sometimes is ahead by one unit or so in the fourth decimal
				\item The difference is less pronounced for the binary variables and most visible in the nominal ones. It is worst for ``age", which has the most unique values
				\item As the percentage of NAs increases, the estimates are further off from the true means. This is to be expected for all methods, but it affects \texttt{hd.ord} and \texttt{hot.deck} much more than \texttt{Amelia} and \texttt{mice}
			\end{coi}
		\item Jeff likens this to the search for the pot of gold at the end of the rainbow. There will be something there, somewhere. I just need to find one specific scenario, one specific set of circumstances where I do particularly well or better. He gave me a bunch of pointers/ideas where to go from here. I'm using \texttt{ampute()} to generates \texttt{NAs} \texttt{MAR}. \texttt{ampute()} is part of \texttt{mice} -- it's not a great surprise that \texttt{mice} then performs really well. \texttt{amelia()} performs even better -- Jeff thinks \texttt{amelia()} calls \texttt{mice} at some point. \texttt{ampute()} has lots of default settings that are not immediately obvious. Jeff says to mess with those
		\item Run it with \texttt{bycases = FALSE} (\texttt{TRUE} introduces a very specific pattern of missingness), \texttt{cont = FALSE}, \texttt{type = "MID"}
			\begin{coi}
				\item I had to take out \texttt{White} because \texttt{ampute()} complained there were too many variables for \texttt{bycases = FALSE}
				\item I ran it for 20 percent \texttt{NAs} and saved \texttt{framing.nas} and \texttt{ampute()} output -- no significant change in results: \texttt{hd.ord} is better than \texttt{hot.deck} across the board. \texttt{Amelia} is better than \texttt{mice} for \texttt{inc}, \texttt{age}, and \texttt{interest}. \texttt{mice} is better for \texttt{Dem} and \texttt{Female}
				\item The same for 10 percent
			\end{coi}
		\item Runnning it with ANES data on Code Ocean
			\begin{coi}
				\item Previous small tests have shown that the results should not be different when compared to the framing data
				\item I have results from running things for 2,363 iterations, but the \texttt{amelia} runtime results were much faster than expected here. I did this run with \texttt{lapply}. I want to rerun this with a loop, as before, before I write anything up here
			\end{coi}
		\item Runtimes
			\begin{coi}
				\item \texttt{hd.ord()} outperforms \texttt{hotdeck()} (minimally) and massively outperforms \texttt{mice} and \texttt{Amelia} on the framing data
				\item I have some ANES runtime results for fewer iterations (1,307)
				\item I also have ANES runtime results for more iterations (2,363) obtained through \texttt{lapply}. Weirdly, \texttt{amelia} is almost as fast as \texttt{hd.ord} here ... I want to rerun this with a loop before I write anything up here
			\end{coi}
		\item Own function to create NAs as MAR
			\begin{coi}
				\item Jeff: ``It's MAR if you `delete' values in column 1 based on values in column 2", e.g. `delete' the values for age where income is 1 and where income is 5
				\item I wrote a function that samples the ID numbers for a specified percentage of each selected variable's unique values, then replaces the corresponding values of a different variable with NAs, based on the sampled ID numbers
				\item I have run this function on the framing data
				\item Seems to be more MCAR than MAR, since \texttt{na.omit} performs really well now
			\end{coi}
		\item Run \texttt{ampute} with MNAR for framing and ANES data
			\begin{coi}
				\item I've written everything up. \texttt{hd.ord} is doing well for binary variables overall, but less so for nominal and ordinal ones
				\item MNAR or MAR makes no significant difference
			\end{coi}
		\item I've written up the \texttt{own.NA.rows} results
		\item It is not possible to run the whole ANES data for 10,000 iterations on Jeff's machine or on Code Ocean. It exhausts the memory. After endless trials, I finally managed to do it by sampling 1,000 observations, then running my code on that smaller ANES data set
		\item I added CCES to my data sets for analysis and set up several coding scripts
		\item With each data set sampled for 1,000 observations (old framing, CCES, ANES), I got great running times where \texttt{hd.ord} consistently performs far better than \texttt{amelia}
		\item What doesn't work so far
			\begin{coi}
				\item Increasing percentage of NAs == worse \texttt{hd.ord} performance
				\item Ordinal and nominal variables == worse \texttt{hd.ord} performance
				\item High number of observations == reduced number of iterations (crashes and/or RAM maxing out)
				\item High number of observations == makes \texttt{amelia} faster relative to \texttt{hd.ord}
				\item 17 ANES education levels == increases needed number of iterations
				\item 17 ANES education levels == causes \texttt{amelia} to stop on CO and Jeff
				\item \texttt{ampute()} with \texttt{bycases=FALSE} and \texttt{cont=FALSE} == worse \texttt{hd.ord} performance, good \texttt{na.omit} performance
				\item \texttt{own.NA()} == \texttt{na.omit} performs best
				\item \texttt{own.NA.rows()}== pretty much everything is zero, incl. \texttt{na.omit} (only \texttt{mice} is awful)
				\item Running \texttt{hd.ord} with \texttt{method = p.draw} == worse \texttt{hd.ord} performance
				\item Running \texttt{hd.ord} with \texttt{method = p.draw} == only works with only binary vars in the data
				\item Increasing \texttt{sdCutoff} == only does something with only binary vars in the data
				\item Only binary vars in data == no gain in \texttt{hd.ord} performance
			\end{coi}
		\item What works so far
			\begin{coi}
				\item Results for binary variables == equal performance of \texttt{hd.ord}, \texttt{mice}, \texttt{amelia}
				\item Increasing number of variables with NAs (all, not just binary) == better \texttt{hd.ord} performance
				\item 1000 observations in data sets == allows 10,000 iterations
				\item 1000 observations in data sets == increases \texttt{amelia} running time
				\item MNAR == MAR in terms of \texttt{hd.ord} performance
			\end{coi}
		\item I ran CCES for 14 NA variables for 10,000 iterations on CO. It took FOREVER because \texttt{mice} took more than 20 hours (!) on its own. I then ran the same code for 100 iterations on my machine. The results are virtually identical, so we really don't need all these thousands of runs. They're good to get good running time numbers, but not for other things. From now on I can test ideas like variables to add etc. on at most 1,000 iterations. No need for any more
		\item In addition, \texttt{mice} is at best identical to \texttt{amelia} regarding the results. Most of the times, \texttt{amelia} is better. If I do need to run things for 10,000-ish iterations again for some reason, consider leaving \texttt{mice} out of it, if possible. That massively cuts down the runtime of the code
		\item I selected and included more variables for ANES and framing, then ran those for 1,000 iterations. The results are getting even closer to zero across the board, including for \texttt{na.omit}
		\item I set up versions of all my functions to work with multiple ordinal variables: \texttt{OPMordMult}, \texttt{OPMcutMult}, \texttt{hot.deck.ord.mult}. I ran this for 1,000 iterations for CCES, ANES, and framing. It produced good results for all binary variables, though not better than before
		\item So far, I have really good results that are on par with \texttt{amelia} for lots of binary variables. I also have great running times where \texttt{hd.ord} is consistently at least twice as fast as \texttt{amelia} for all data sets with 1,000 observations and 10,000 iterations. That's a good step forward
		\item I don't think there is a scenario where \texttt{hd.ord} performs better. I'm reaching the stage where I would state that the theory of the latent underlying variable didn't pan out
		\item Emailed Jeff: I'm at the end of my research. I can't beat \texttt{amelia}, no matter the circumstances. But I get the same results for binary variables, with tons better running time. It looks like the theory of \texttt{polr} to estimate ordinal variables doesn't pay dividends. I want that to be enough for my diss
		\item Jeff responded: ``Go ahead and write it up focusing on the speed and the rigor of your testing of all of these packages. Something like `Quality Comparison of Major Missing Data Solutions for Discrete Data with a Proposed New Method'. That seems to reflect your extensive testing and understanding of software solutions"
	\end{coi}


\section*{Chapter write-up}
	\begin{coi}
		\item Write up the chapter focusing on the performance and speed of the packages under the different circumstances I tested
		\item Reorganize headings and general outline
		\item Identify where things are missing with +++
		\item Gradually keep filling missing sections
		\item \hl{Work in Jeff's feedback (what's left)}
			\begin{coi}
				\item p. 41: ``Rather than assigning ...":
					\begin{coi}
						\item Take research (Dataverse) from prominent research people: Michigan folks, Larry Bartels
						\item Take a glm regression (logit, probit, poisson model) with a RHS variable that is ordinal, perhaps scaled 1-7
						\item Throw away the outcome variable, put the ordinal variable as the DV
						\item Run polr and run linear regression on that
						\item What I'm looking for: How different are the regression coefficients (the RHS) between the two models? This difference shows how important the re-estimating of the ordinal spaces is
						\item If the results show no differences, this makes my speculation at the end, that the uneven spaces might not be that important after all, much stronger $\rightarrow$ and if the results show that there are actually big differences, this then excludes the this-isn't-that-important-after-all explanation as a possible reason why my analysis wasn't working $\rightarrow$ so it's a win-win situation no matter what the results show
					\end{coi}
				\item When I have done this, adjust/expand the Brief Evaluation section as needed
			\end{coi}
		\item When I have implemented all that, Jeff doesn't need to see it again. I can send it out directly to Ryan and Liz
	\end{coi}



	

\end{document}			

