\input{\string~"/Insync/Google Drive/templates/preamble"}

\title{Ordinal Variables and Missing Data}

\date{\today}

\begin{document}

\maketitle

\section*{General}
	\begin{coi}
		\item This paper should be an intersection of ordinal variables and missing data. Jeff thinks there is room there for a contribution
		\item The general idea is that general treatments of missing data (listwise deletion, multiple imputation etc.) lead to different results -- depending on the type of variable. How you treat \texttt{Don't Know} and \texttt{Refused} and how this treatment affects the results depends greatly on the type of variable that you use to impute the missing data. The idea is that you would need to approach things differently depending on whether you use a nominal, interval, or ordinal variable to fill in values for \texttt{Don't Know}/\texttt{Refused}. So I am developing a method to specifically treat \texttt{Don't Know}/\texttt{Refused} with ordinal variables that improves over current uses that are generic for all types of variables
		\item This method should be a customized form of multiple imputation that is closer to the data than general multiple imputation. Jeff and Skyler developed affinity scores and hot decking in their BJPS paper. They used the number of exact matches (in the form of other participants) to calculate the affinity score. Instead, I should use a weighted distance solution between ordinal variable categories. I would use the OP model from the blocking paper to weight the distances between the categories in matches (in the form of other participants). In other words, I would use the underlying ordered probit numbers to create the weights. So this would be a specific ordinal variable adjustment of the affinity score building
		\item Set up chapter structure
	\end{coi}


\section*{Code}
	\begin{coi}
		\item My code functions for one variable with NAs, for 10,000 iterations
			\begin{coi}
				\item I have saved results for \texttt{inc}, \texttt{age}, \texttt{Dem}, \texttt{Rep} for \texttt{hd.ord} and \texttt{hd.norm}
				\item For all 4, \texttt{na.omit} performs the best. This isn't surprising, since deleting observations with missing values shouldn't be a problem with MCAR
			\end{coi}
		\item My code does not function When run for several variables with NAs
			\begin{coi}
				\item It throws up a replacement error somewhere down the line when run for 10,000 iterations. It's never in the same place, so it must be something random
				\item Always the same: \# of NAs overall. Not always the same between some runs: \# of NAs per column, \# of rows with NAs
				\item Find out why my code doesn't work for NAs in several variables
					\begin{coi}
						\item The number of NAs inserted wasn't actually the issue. It was in the \texttt{OPMord} code
						\item I painfully ran 1,000 iterations for two variables, gradually adding one line at a time to the function. I discovered the error was where I assign values to \texttt{df.cases}, specifically where the columns are all combined and the resulting vector added to the data as \texttt{educ.new}
						\item I saved the vector to an empty vector, ran everything for 1,000 iterations, then looked at the vectors and the corresponding versions of \texttt{df.cases}
						\item There were rows with all NAs in \texttt{df.cases}, which makes the vector shorter than the data, which means it can't be assigned to it as a column. I reran the \texttt{df.cases} creation code to find the culprit
						\item I had overlooked that \texttt{int.df\$Values} has one value fewer than there are variable levels, since it lists cutpoints between the levels. For 6 levels, I had assigned \texttt{int.df\$length(levels(...))}, which picks the 6th value of \texttt{int.df\$Values} -- but it only goes to 5. I had to add \texttt{-1}. Now it's working
					\end{coi}
			\end{coi}
		\item Method to insert NAs
			\begin{coi}
				\item I tried \texttt{prodNA}, which works for \texttt{MCAR} but gives me nothing for \texttt{MAR}. And \texttt{MCAR} works well for \texttt{na.omit}, since by definition it's a random sample of missing data, which doesn't matter 
				\item Use \texttt{mice} \texttt{ampute()}
					\begin{coi}
						\item It has \texttt{MCAR} and \texttt{MAR} options. \texttt{MAR} is what I need
						\item It doesn't work on just one variable; it needs at least two
						\item \texttt{ampute()} works very well
					\end{coi}
			\end{coi}
		\item With the code adjusted (\texttt{-1}), \texttt{OPMord} now works. However, some of the \texttt{int.df}s don't have all the education levels: \texttt{int.df} with all levels has 6 rows (one fewer than the levels, since each row shows a cutoff), but some have 5. This means \texttt{OPMcut} now doesn't work
			\begin{coi}
				\item I could adjust \texttt{OPMcut} to work with fewer levels, but then I would be, in the end, taking means of most data with all levels and some data with fewer levels. That's problematic
				\item It's better to discard the iterations where \texttt{int.df} has 5 rows after \texttt{OPMord} has run and then continue with the other functions. I set the code up to do that. However, after about 40 percent of running 12,500 iterations, there was an error: I hadn't factored in that the \texttt{int.df}s could also have fewer rows than that. A few of them had 4 rows, which caused the error. I adjusted the code to now discard the iterations where \texttt{int.df} has anything other than 6 rows
			\end{coi}
		\item The data ran for 12,500 iterations, 80 percent NAs, \texttt{Rep} and \texttt{inc}, \texttt{hot.deck.ord}, \texttt{hot.deck.norm}, \texttt{na.omit}. The results looked promising: \texttt{hd.ord} performed better than \texttt{hd.norm} and \texttt{na.omit}
		\item More methods
			\begin{coi}
				\item Include \texttt{mice}
				\item Include \texttt{Amelia}
				\item Use the defaults for both since that's what over 90\% of people do (even if they shouldn't)
				\item Include \texttt{hot.deck.norm} on the original education values (I ran it on the cutpoints)
			\end{coi}
		\item \texttt{hot.deck impContinuous}
			\begin{coi}
				\item No need to use it, since no variable is continuous (age has more values than \texttt{sdCutoff} but it's nominal, not continuous)
			\end{coi}
		\item \texttt{hot.deck sdCutoff}
			\begin{coi}
				\item No need to change the default
			\end{coi}
		\item Add more variables/NAs
			\begin{coi}
				\item Add NAs to 6 of the variables
				\item Demographics I still have: Age(numeric), employment (5 levels), ideology (liberal, conservative, neither), following public affairs (ordinal, 4 levels), media interest (numeric, accumulative count of activities), participation (numeric, accumulative count of activities)
				\item I can't add them all, since \texttt{mice} and \texttt{Amelia} don't work when there is collinearity
				\item I used code to find and discard variables with high correlation
			\end{coi}
		\item Percentage of NAs
			\begin{coi}
				\item I'm currently using 80 percent. Jeff used 20, 50, and 80
				\item I did one run each for 20, 50, and 80 percent
			\end{coi}
		\item Preliminary results
			\begin{coi}
				\item Across the board, \texttt{hd.ord} performs better than normal \texttt{hot.deck}, but worse than \texttt{Amelia} and \texttt{mice}
				\item Across the board, \texttt{Amelia} is better than \texttt{mice} for \texttt{inc} and \texttt{age}. It's also better for \texttt{interest}, except for 20 percent NAs, where \texttt{mice} wins
				\item For \texttt{Dem}, \texttt{Female}, and \texttt{White}, \texttt{Amelia} also edges first, thought \texttt{mice} sometimes is ahead by one unit or so in the fourth decimal
				\item The difference is less pronounced for the binary variables and most visible in the nominal ones. It is worst for ``age", which has the most unique values
				\item As the percentage of NAs increases, the estimates are further off from the true means. This is to be expected for all methods, but it affects \texttt{hd.ord} and \texttt{hot.deck} much more than \texttt{Amelia} and \texttt{mice}
			\end{coi}
		\item Jeff likens this to the search for the pot of gold at the end of the rainbow. There will be something there, somewhere. I just need to find one specific scenario, one specific set of circumstances where I do particularly well or better. He gave me a bunch of pointers/ideas where to go from here. I'm using \texttt{ampute()} to generates \texttt{NAs} \texttt{MAR}. \texttt{ampute()} is part of \texttt{mice} -- it's not a great surprise that \texttt{mice} then performs really well. \texttt{amelia()} performs even better -- Jeff thinks \texttt{amelia()} calls \texttt{mice} at some point. \texttt{ampute()} has lots of default settings that are not immediately obvious. Jeff says to mess with those
		\item Run it with \texttt{bycases = FALSE} (\texttt{TRUE} introduces a very specific pattern of missingness), \texttt{cont = FALSE}, \texttt{type = "MID"}
			\begin{coi}
				\item I had to take out \texttt{White} because \texttt{ampute()} complained there were too many variables for \texttt{bycases = FALSE}
				\item I ran it for 20 percent \texttt{NAs} and saved \texttt{framing.nas} and \texttt{ampute()} output -- no significant change in results: \texttt{hd.ord} is better than \texttt{hot.deck} across the board. \texttt{Amelia} is better than \texttt{mice} for \texttt{inc}, \texttt{age}, and \texttt{interest}. \texttt{mice} is better for \texttt{Dem} and \texttt{Female}
				\item The same for 10 percent
			\end{coi}
		\item Runnning it with ANES data on Code Ocean
			\begin{coi}
				\item Previous small tests have shown that the results should not be different when compared to the framing data
				\item I have results from running things for 2,363 iterations, but the \texttt{amelia} runtime results were much faster than expected here. I did this run with \texttt{lapply}. I want to rerun this with a loop, as before, before I write anything up here
			\end{coi}
		\item Runtimes
			\begin{coi}
				\item \texttt{hd.ord()} outperforms \texttt{hotdeck()} (minimally) and massively outperforms \texttt{mice} and \texttt{Amelia} on the framing data
				\item I have some ANES runtime results for fewer iterations (1,307)
				\item I also have ANES runtime results for more iterations (2,363) obtained through \texttt{lapply}. Weirdly, \texttt{amelia} is almost as fast as \texttt{hd.ord} here ... I want to rerun this with a loop before I write anything up here
			\end{coi}
		\item Own function to create NAs as MAR
			\begin{coi}
				\item Jeff: ``It's MAR if you `delete' values in column 1 based on values in column 2", e.g. `delete' the values for age where income is 1 and where income is 5
				\item I wrote a function that samples the ID numbers for a specified percentage of each selected variable's unique values, then replaces the corresponding values of a different variable with NAs, based on the sampled ID numbers
				\item I have run this function on the framing data
				\item Seems to be more MCAR than MAR, since \texttt{na.omit} performs really well now
			\end{coi}
		\item \hl{Continue with paper notes}
	\end{coi}



\section*{Theory}
	\begin{coi}
		\item \hl{Step by step fill in the sections on Missing Data, Deletion, and Imputation}
		\item Rework the introduction
			\begin{coi}
				\item Current stuff in there is very broad and nowhere near detailed enough (taken from the Kerwin application)
			\end{coi}
	\end{coi}


\end{document}			

