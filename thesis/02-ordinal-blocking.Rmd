# BLOCKING IN SURVEY EXPERIMENTS WITH A NEW METHOD TO MEASURE ORDINAL VARIABLES {#ordblock}

## Introduction {#ordblock-intro}

Survey experiments collect background information and attempt to uncover treatment effects on public opinion and political attitudes. In order to identify such potential effects, the treatment groups need to be balanced on the potential outcomes. This can be achieved through random assignment of participants to treatment groups. Randomization, i.e. flipping a fair coin to decide which treatment group a participant is assigned to, probabilistically results in balance based on the Law of Large Numbers [@urdan_statistics_2010]. For small samples, however, it can lead to serious imbalance. This can leave experimental results in statistically murky waters [@imai_quantitative_2018;@king_designing_1994;@fox_applied_2015]. In survey experiments, the overall sample size is often split across several treatment groups, which can exacerbate the problem. @chong_framing_2007, for instance, split 869 participants in a framing experiment on urban growth over 17 treatment groups, which leads to an average of just over 50 participants per group. Randomization is unlikely to lead to comparable treatment groups of this size. Researchers need to employ statistical methods to obtain balanced groups here. Blocking, i.e. arranging participants in groups that are equal in terms of participants' covariates and using random allocation within these groups, can alleviate such worries. 

Blocking depends on covariates. In political science, many covariates with high predictive power are categorical variables, i.e. variables where the data can be divided into groups (e.g. race). Others include interval (ordered and evenly spaced, e.g. SAT score) and ordinal (ordered and unevenly spaced, e.g. education) variables. Ordinal variables are ordered variables where the spacing between the values is not the same.

To block, these variables are often made numerical, e.g. by assigning the numbers 1-3 to the variable categories. This is acceptable for interval variables as the evenly spaced numbers correspond to the evenly spaced categories. For ordinal variables, however, this can be problematic. Take for instance the example of education: Each subsequent category has quantitatively more education than the previous, but the exact measure of the distances between the categories is unclear. An arbitrary evenly spaced string of numbers does not correspond to these unevenly spaced ordinal categories and may misrepresent the data. Do evenly spaced numbers really represent the distances between the categories? Perhaps the true spacing between some of the categories is so narrow they should not even be separate categories at all. 

I propose an ordered probit threshold approach to circumvent this problem: This approach estimates an assumed underlying latent continuous structure underneath ordinal variables whose data-driven categories can then be used for blocking. The following sections provide a background on survey experiments and blocking, describe the key aspects of ordinal variables, and outline my proposed ordered probit approach. I then demonstrate the effect of this approach with external survey data. The results are mixed. Some findings support the notion that the re-estimation of ordinal variable categories with an ordered probit approach matters, while others negate it. 



## Theory {#ordblock-theory}

### Preliminary Notations on Survey Experiments {#ordblock-theory-experiments}

The simplest of survey experiments has two potential outcomes for participants $i$, $y_{1i}$ and $y_{0i}$, with 1 denoting the treatment and 0 referring to the control. Consider a simplified version of a famous survey experiment by @tversky_framing_1981, where researchers want to test the effect of the mortality format on participants' choices.\label{death} They provide participants with the following scenario:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent Imagine that the US is preparing for the outbreak of an unusual Asian disease. A program to combat the disease has been proposed. Assume that the exact scientific estimates of the consequences of the program are as follows...
\end{adjustwidth}

Participants in the control group receive the program description in survival format:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent If the program is adopted, 200 out of 600 people will live.
\end{adjustwidth}

Participants in the treatment group receive the program description in mortality format:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent If the program is adopted, 400 out of 600 people will die.
\end{adjustwidth}

@tversky_framing_1981 use this experiment to demonstrate the importance of framing. Support for the program is much higher among respondents who received the survival format, even though the success rate of the program is identical in both formats. Framing the program in a positive light thus dramatically increases support by connecting the program to people's aversion of death and affirmation of life. While these findings stem from an experiment conducted in the 1970s, it is not a big leap to imagine a similar outcome in today's Covid-19 world. There is little reason to assume human behavior has changed to such a degree over the past decades that we would no longer be affected by framing today. If we conducted this experiment now, it would be quite possible to once more find sizable differences between these two groups.

Let $\bm{T}=0$ denote the control group and $\bm{T}=1$ denote the treatment group. After being shown one of the two scenarios, all participants are asked whether they support or oppose the program. The treatment effect for each individual participant $i$ is given by $y_{1i} - y_{0i}$. A comparison of the groups' average support reveals the Average Treatment Effect (ATE) across all participants, $\mathbf{E}[\delta] = \mathbf{E}[y_{1i} - y_{0i}]$. A central characteristic of such a comparison is the fundamental problem of causal inference [@holland_1986_statistics;@rubin_1974_estimating]: We are unable to observe both potential outcomes for the same participant at once. In our case, we cannot observe how much participant $i$ supports the program if given the survival format whilst also observing how much the same participant $i$ would have supported the program if given the mortality format. Since the true average treatment effect is unobservable, we need to use statistical means to assess the unobservable counterfactuals. If $\bm{Y}$ and $\bm{T}$ are independent, we can substitute $\mathbf{E}[\bm{Y_t}|\bm{T}=t]$ for $\mathbf{E}[\bm{Y_t}]$, thereby replacing an unobservable entity with something we can estimate from the data. If this independence is given, we can use the participants who received the mortality format (treatment) to estimate what would have happened to the participants who received the survival format (control) if the survival participants had received the mortality format. The potential outcome of the control group then mirrors what would have happened in the case of treatment, and vice versa. The process of randomization helps justify this independence and allows comparability. Another means to achieve comparability is the process of blocking. 


### Randomization {#ordblock-theory-randomization}

Randomization is equivalent to flipping a fair coin for each participant to be assigned to treatment or control. This chance procedure gives each participant the same probability of being assigned to either group (or groups, in case of multiple treatment groups) [@lachin_1988_properties]. Randomization increases covariate balance as the number of participants, $n$, increases [@imai_2009_essential]. The larger a researcher's sample, the better the resulting balance from randomization in expectation. Probabilistically, randomization enables the comparison of the average treatment effect to be unbiased, which allows the researcher to attribute any treatment effects to the treatment [@king_a-politically_2007]. 

While randomization thus guarantees balance as the sample size reaches infinity, it often does not do so in the naturally finite sample sizes researchers actually work with. With huge samples, the Law of Large Numbers requires that treatment groups selected through randomization will be balanced. With small samples, however, it is possible to get unlucky and end up with unbalanced groups [@imai_2008_misunderstandings]. Blocking can help achieve balance in such scenarios [@epstein_2002_rules].


### Blocking {#ordblock-theory-blocking}

Identical levels in terms of covariates across treatment groups represent the key aspect in experimental studies. In randomization, this is achieved by random chance. In blocking, this is achieved by combining covariate information about the participants with randomization. Specifically, participants are blocked into treatment groups that are similar to one another in terms of their covariates before treatment is assigned. Their similarity is estimated with the Mahalanobis or Euclidian distance. The Mahalanobis distance (MD) is a multivariate distance metric which measures the distance between two vectors (or between a point and a distribution). For two random vectors $\bm{x_i}$ and $\bm{y_i}$ with $i = [1,\ldots,n]$ of the same distribution, the MD is defined as

\begin{align}
MD_{xy} = \sqrt{(x_i - y_i)' S^{-1} (x_i - y_i)},
\end{align}

where $\bm{S}$ denotes the covariance matrix. If the covariance matrix is a diagonal identity matrix, the resulting distance measure becomes the Euclidian distance (ED), 

\begin{align}
ED_{xy} = \sqrt{\sum_{i=1}^n \frac{(x_i - y_i)^2}{s^2}},
\end{align}

with $s_i$ denoting the standard deviation of $x_i$ and $y_i$. The MD accounts for covariances, whereas the ED assumes equal variances and zero covariances. The ED can thus be argued to represent a special case of the MD.

Blocking is better suited to achieving balance in finite samples than randomization, as it "directly controls the estimation error due to differing levels of observed covariates in the treatment and control groups" [@moore_2012_multivariate, p. 463]. This is particularly relevant with small samples and a high number of treatment groups, as the overall number of participants needs to be divided up. Figure \ref{BoxLawLarNum} shows this visually. The following steps outline the estimation process behind the figure: 

\vspace{0.3cm}
\begin{adjustwidth*}{+0.5cm}{+0.5cm}
\begin{enumerate}
\item \noindent Randomly sample a discrete variable $v$ with levels 1 to 5.
\item Randomly assign the sample to a specified number of treatment groups. 
\item Block the sample separately into the same number of treatment groups. 
\item Take the mean of $v$ for each randomly assigned treatment group. 
\item Take the mean of $v$ for each blocked treatment group. 
\item Estimate the distances between the means of the randomized groups.
\item Estimate the distances between the means of the blocked groups.
\item Repeat steps 1 to 7 100 times.
\end{enumerate}
\end{adjustwidth*}
\vspace{0.3cm}

We follow these steps repeatedly for sample sizes up to 1,000 for 2, 3, 5, and 10 treatment groups. Figure \ref{BoxLawLarNum} shows the resulting distributions of maximum mean distances. Each sample size is divisible into integers for each respective number of treatment groups, which results in varying sample sizes depending on the number of treatment groups in question. Blocking outperforms randomization in every scenario. The difference between the two methods is smallest for large samples and a small number of treatment groups. 

```{r Law of Large Numbers Simulations, eval=FALSE, include=FALSE}

### THE CODE THAT CREATES THE SIMULATIONS FOR THE BLOCKING PLOTS IS IN scripts/lln//lln_testing.Rmd. IT TAKES 3-4 DAYS TO RUN THIS CODE, SO THERE IS NO POINT HAVING IT HERE ###

```

```{r Law of Large Numbers Plotting Code Boxplots, include=FALSE}

### I LOAD THE CREATED SIMULATIONS HERE TO CREATE THE PLOTS ###

all_blocked_2 <- read.csv("data/blocking/all_blocked_2.csv")
all_blocked_3 <- read.csv("data/blocking/all_blocked_3.csv")
all_blocked_5 <- read.csv("data/blocking/all_blocked_5.csv")
all_blocked_10 <- read.csv("data/blocking/all_blocked_10.csv")
all_means_variances_2 <- read.csv("data/blocking/all_means_variances_2.csv")
all_means_variances_3 <- read.csv("data/blocking/all_means_variances_3.csv")
all_means_variances_5 <- read.csv("data/blocking/all_means_variances_5.csv")
all_means_variances_10 <- read.csv("data/blocking/all_means_variances_10.csv")

# There are 100 NAs each in "all_means_variances_3", "all_means_variances_5", and "all_means_variances_10"
# They are always for the first respective sampled number: 9, 15, 30
# They're in control for _3 and _5, and in treatment5 for _10
# I don't know why those are happening, but I'm not starting with the first sampled numbers anyway, and it doesn't make any difference for the overall simulations, so I am removing those
all_means_variances_3 <- subset(all_means_variances_3, subset = (sampled_numbers != 9))
all_means_variances_5 <- subset(all_means_variances_5, subset = (sampled_numbers != 15))
all_means_variances_10 <- subset(all_means_variances_10, subset = (sampled_numbers != 30))

# I want the plot legend to read "randomized" instead of "rand"
all_means_variances_2$label <- fct_recode(all_means_variances_2$label, "randomized" = "rand")
all_means_variances_3$label <- fct_recode(all_means_variances_3$label, "randomized" = "rand")
all_means_variances_5$label <- fct_recode(all_means_variances_5$label, "randomized" = "rand")
all_means_variances_10$label <- fct_recode(all_means_variances_10$label, "randomized" = "rand")

together <- list(all_blocked_2, all_means_variances_2, all_blocked_3, all_means_variances_3, all_blocked_5, all_means_variances_5, all_blocked_10, all_means_variances_10) # collect all dfs in a list to loop over
couple <- list() # empty list

for(i in 1:(length(together))){
   couple[[i]] <- subset(together[[i]], select = c(sampled_numbers, diff, label))
   couple[[i]]$sampled_numbers <- as.factor(couple[[i]]$sampled_numbers)
} # subset for 3 columns and turn sampled_numbers into factor

sims_2 <- rbind(couple[[1]],couple[[2]]) # combine blocked and rand for each # of treatment groups
sims_3 <- rbind(couple[[3]],couple[[4]])
sims_5 <- rbind(couple[[5]],couple[[6]])
sims_10 <- rbind(couple[[7]],couple[[8]])

selection <- c(0, 0.1, 0.2, 0.3, 0.5, 1) # the quantiles I want

quantile(as.numeric(levels(sims_2$sampled_numbers)), selection) # quantiles for 2 groups
levels(sims_2$sampled_numbers) # levels for 2 groups
sims_2_range <- subset(sims_2, subset = sampled_numbers %in% c(14, 102, 206, 302, 502, 998)) # hand-select samples for range
sims_2_one <- subset(sims_2, subset = sampled_numbers == as.numeric(levels(sims_2$sampled_numbers)[2])) # select second level for 'intro' plot

quantile(as.numeric(levels(sims_3$sampled_numbers)), selection)
levels(sims_3$sampled_numbers)
sims_3_range <- subset(sims_3, subset = sampled_numbers %in% c(18, 108, 207, 306, 504, 999))
sims_3_one <- subset(sims_3, subset = sampled_numbers == as.numeric(levels(sims_3$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_5$sampled_numbers)), selection)
levels(sims_5$sampled_numbers)
sims_5_range <- subset(sims_5, subset = sampled_numbers %in% c(25, 115, 215, 305, 505, 995))
sims_5_one <- subset(sims_5, subset = sampled_numbers == as.numeric(levels(sims_5$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_10$sampled_numbers)), selection)
levels(sims_10$sampled_numbers)
sims_10_range <- subset(sims_10, subset = sampled_numbers %in% c(40, 130, 220, 300, 500, 1000))
sims_10_one <- subset(sims_10, subset = sampled_numbers == as.numeric(levels(sims_10$sampled_numbers)[2]))

xlab <- "Sample Sizes"
ylab <- "Max. Distances Between Treatment Groups"

plot_first <- ggplot(sims_2_one, aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.15, 0.65)) # first plot outside of the loop because of the legend

sims_plots <- list(sims_2_range, sims_3_one, sims_3_range, sims_5_one, sims_5_range, sims_10_one, sims_10_range) # list of all subsets for plotting
plots <- list()
for(i in 1:(length(sims_plots))){
   plots[[i]]  <- ggplot(sims_plots[[i]], aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + guides(fill=FALSE)
  } # create plot for each data subset

```


```{r Boxplot-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distances Between Treatment Group Means in Randomized and Blocked Data. Increasing Sample Size for 2 (Top Row), 3 (Second Row), 5 (Third Row), and 10 Treatment Groups (Bottom Row). Leftmost Pair on the Right Panel Is the Same as the Pair on the Left Panel\\label{BoxLawLarNum}"}

grid.arrange(plot_first, plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]], plots[[7]], ncol = 2, nrow = 4, bottom=xlab, left=ylab)

```


For $n = 998$ and 2 treatment groups, the largest distance between randomized treatment groups is `r round(max(all_means_variances_2$diff[all_means_variances_2$sampled_numbers == 998]), digits=3)`, while the largest distance between blocked treatment groups is `r round(max(all_blocked_2$diff[all_blocked_2$sampled_numbers == 998]), digits=3)`. For small samples and a large number of treatment groups, the difference between the two methods increases. For $n = 40$ and 10 treatment groups, the largest distance between randomized treatment groups is `r round(max(all_means_variances_10$diff[all_means_variances_10$sampled_numbers == 40]), digits=3)`, while the largest distance between blocked treatment groups is `r round(max(all_blocked_10$diff[all_blocked_10$sampled_numbers == 40]), digits=3)`. 

Figure \ref{HistLawLarNum} shows the count distributions of these imbalances. For 2 treatment groups (top left plot), almost all blocked treatment groups have a maximum distance of zero. Most randomized groups also have a maximum distance of zero, but it is a narrow majority. Almost 50 percent of randomized groups have distances greater than zero, though still lower than one. As the number of treatment groups increases, so do the distances between the treatment groups. Nonetheless, the vast majority of blocked groups still show a maximum distance of zero. Even for 10 treatment groups, more than 60 percent of the blocked groups are not distant from each other at all. This does not hold true for the randomized groups. For 3 treatment groups, the majority of distances are now above zero. For 5 groups, the majority of distances exceed 0.25. For 10 treatment groups, more than 60 percent of groups show a distance larger than 0.5.


```{r Law of Large Numbers Plotting Code Histograms, include=FALSE}

ylab <- "Count"
xlab <- "Max. Distances Between Treatment Groups"

plot_first_more <- ggplot(sims_2, aes(x=diff, fill=label)) + geom_histogram(alpha=0.4, aes(y=..count..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.6, 0.75)) + scale_fill_manual(values=c("red", "blue")) # first plot outside of the loop because of the legend

sims_plots_more <- list(sims_3, sims_5, sims_10) # list of all data sets (minus for 2 groups for plotting)

plots_more <- list()
for(i in 1:(length(sims_plots_more))){
   plots_more[[i]] <- ggplot(sims_plots_more[[i]], aes(x=diff, fill=label)) + geom_histogram(alpha=0.4, aes(y=..count..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + guides(fill=FALSE) + scale_fill_manual(values=c("red", "blue")) # create plot for each data set
}
```

```{r Hist-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=4, fig.cap="Distribution of Treatment Group Differences in Randomized and Blocked Data for 2 (Top Left), 3 (Top Right), 5 (Bottom Left), and 10 Treatment Groups (Bottom Right). Overlapping Area Shown in Dark Purple.\\label{HistLawLarNum}"}

grid.arrange(plot_first_more, plots_more[[1]], plots_more[[2]], plots_more[[3]], ncol = 2, nrow = 2, bottom = xlab, left = ylab)

```


#### Blocking on the Go {#ordblock-theory-blocking-onthego}

Survey experiments can utilize blocking in two ways: with all data present and 'on the go'. The above data represents a simplified example of the former. All respondent covariate data have been collected before respondents are blocked into treatment groups, i.e. covariate data are known for all respondents before assignment to treatment. Blocking 'on the go' involves blocking respondents as they arrive for treatment at differing times. This is the case, for instance, for online survey experiments, where participants complete the survey at differing times, i.e. they 'trickle in' for treatment assignment as the experiment progresses. 'Traditional' blocking can not be used here, since it relies on covariate information about the entire sample, which is not available. Instead, we need to block sequentially, or 'on the go', as the experiment progresses. I use this method in the online survey experiment in chapter \ref{framing}.

Sequential blocking in political science is based on covariate-adaptive randomization, which varies probabilities based on knowledge about previous participants and the current participant [@chow_2007_adaptive]. Traditional covariate-adaptive approaches, such as the biased coin design [@efron_1971_forcing] and minimization [@pocock_1975_sequential], assign the incoming participant to the treatment group with the fewest participants with identical covariate information. For discrete covariates, for instance, this takes the form of assigning all participants except the first one, $q$, for covariate $c$ with value $d$ to a treatment group $g$ with probability

\begin{align}
\text{prob}\left(g* = g\right) = \left(1 - \frac{q_{cdg}}{\sum^G_{g = 1} q_{cdg}}\right) \left(\sum_{g = 1}^G\left(1 - \frac{q_{cdg}}{\sum^G_{g = 1} q_{cdg}}\right)\right)^{-1}.
\end{align}

This works for discrete covariates as the number of possible covariate levels is finite. For continuous covariates, the number of possible covariate levels rises exponentially, which results in rare identical participants as they are unlikely to look the same. Blocking on continuous covariates is not possible with these traditional approaches [@markaryan_2010_exact;@rosenberger_2002_randomization;@eisele_1995_biased]. @moore_blocking_2013 develop a method to do so by exploiting relationships between the current participant's covariate profile and those of all previously assigned participants. They define the similarity between participants with the MD between participants.

To aggregate pairwise similarity, they implement the mean, median, and trimmed mean of the pairwise MDs between the current participant and the participants in each treatment condition: Participants are indexed with treatment condition $t$ using $r \in \{1,...,R\}$. For each condition $t$, they estimate an average MD between the current participant, $q$, and the participants previously assigned. If the distance in terms of MD for the incoming participant is 2 in the control and 5 in the treatment condition, the incoming participant looks more similar to the control condition. To set the probability of assignment, @moore_blocking_2013 calculate the mean MDs for each incoming participant, $q$, for all treatment conditions, $t$, and sort the treatment conditions by these averages. Randomization is biased towards conditions with high scores. For each value of $k$ with $k \in \{2,3,...,6\}$, the condition with the highest average MD is then assigned a probability $k$ times larger than all other assignment probabilities. 

Blocking is thus possible when all covariate information is known at the time of assignment and when it 'trickles in' over time. Covariate information, however, is only one side of the coin. Researchers also need to take into consideration the characteristics of the variable to block on. Not all types of variables can and should be used the same way for blocking. Specifically, the current use of ordinal variables as blocking variables is somewhat problematic.



### Ordinal Variables {#ordblock-theory-ordinal}

Ordinal variables matter in surveys. One of the most important ordinal variables in political science surveys is education. It is widely established that education represents one of the major driving forces behind public opinion and political behavior, such as turnout or donations, in the U.S. [@dawood_campaign_2015; @fiorina_disconnect_2009; @leighley_who_2014; @abramowitz_disappearing_2010; @druckman_how_2013; @fiorina_culture_2011; @king_polarization_1997]. Ordinal variables are part of the larger framework of categorical variables. Categorical variables represent types of data which are commonly divided into three groups: nominal, interval, and ordinal variables. Nominal variables are categorical variables with two or more categories that are not intrinsically ordered. Examples include gender (Female, Male, Transgender etc.), race (African-American, White, Hispanic etc.), and party ID (Democrat, Republican, Independent) where the categories cannot be ordered sensibly into highest or lowest. Interval variables are ordered categorical variables with evenly spaced values. Examples include income (\$20,000, \$40,000, \$60,000, \$80,000 etc.), where the distance between \$20,000 and \$40,000 is the same as the distance between \$60,000 and \$80,000. Ordinal variables are ordered categorical variables where the spacing between values is not the same. Examples include education (Elementary School, Some High School, High School Graduate etc.) where the distance between "Elementary School" and "Some High School" is likely different than the distance between "Some High School" and "High School Graduate". Each subsequent category has quantitatively more education than the previous one, but the exact measure of the distances between the categories is unclear.  


For statistical analysis, the categories of nominal variables are often turned into binary variables. This manipulation does not impose any unnatural ordering onto the variable and thus does not require any theoretical assumptions. Interval variables are often made numerical, which is statistically sound. It makes sense to assign numerical values such as 1, 2, 3, and 4 to income categories of \$20,000, \$40,000, \$60,000, and \$80,000 as the distance between each of these categories is identical for any adjacent pair. This translates perfectly into the numerical values with identical distances, i.e. the distance between \$20,000 and \$40,000 is the same as the distance between 1 and 2. Ordinal variables are often treated as nominal variables by creating a new binary variable for one education category, for instance "High School Graduate". This approach ignores the ordered nature contained in ordinal variables and we would wrongly assume that the ordering of education categories is arbitrary. Ordinal variables are also often made numerical for analytic purposes. This is problematic because of their unevenly spaced categories. If the education categories "Elementary School", "Some High School", and "High School Graduate" were turned into the numerical values 1, 2, and 3, we would wrongly assume that the distances between the education categories correspond to these evenly spaced values. Do the numbers 1 to 3 really represent the distances between these categories? Perhaps the true spacing between some of the categories is so narrow they should not even be separate categories at all. We cannot answer this by making an arbitrary assumption that is not justified by the data. Alternatively, if "Elementary School", "Some High School", and "High School Graduate" were turned into three separate dummy variables, we would wrongly assume that there is no ordering to these values. In both cases, important information would be lost, which could lead to distortion [@obrien_1981_using]. 

Unfortunately, these two approaches are commonly applied to ordinal variables in the social sciences. @liddell_2018_analyzing for instance report that all articles published in 2016 in the Journal of Personality and Social Psychology (JPSP), Psychological Science (PS), and the Journal of Experimental Psychology that mention the term 'Likert' analyze ordinal data with a metric or binary model. Doing so can cause inversions of effects as well as Type I and Type II errors [@lalla_2017_fundamental;@torra_2006_regression], i.e. rejecting the null hypothesis when it is actually true (Type I) or failing to reject the null hypothesis when it is actually false (Type II). It is more likely that we underestimate Type I errors in metric conversions due to the relationship between the latent mean of the underlying continuous variable and the mean of ordinal values in the metric mode. If we for instance assume a Likert scale where the ordinal values range from 1 to 5, the mean of the ordinal values takes the form of an S-shaped function of the latent mean, with the function approaching asymptotes at the lowest and highest levels. In other words, the function squeezes the latent mean into a limited range. This squeeze means extreme latent values get censored by the ordinal values such that the highest and lowest ordinal values lose information about the extremity of the latent value. This makes it more likely that we find effects that aren't actually there, i.e. Type I errors. To truly use the ordinal nature of a variable, we thus need to use both its quantitative and its inherent unevenly spaced ordered aspects to make a more underlying description of the data possible and measure the distances between ordinal categories more appropriately [@agresti_2010_analysis].

Measuring distances between values on a Likert scale raises the issue of subjective perception, i.e. the notion that different ordinal categories mean different things to different people. How I perceive the distances between categories when looking at a Likert scale may not match how another person interprets the distances on the same scale. Measuring these distances with objectively estimated values might not adequately capture these differences. However, this seems to affect different types of Likert scales differently: The risk of subjective differences increases when a scale does not contain a 'neutral' response option in the middle and an equal number of counterparts on both sides of the scale [@dawes_2008_data]. This applies for instance to the scale "Never", "Seldom", "Occasionally", and "Always", where subjective perception can skew responses on every level of the scale. For the scale "Strongly oppose", "Somewhat oppose", "Neither favor nor oppose", "Somewhat support", and "Strongly Support", however, the influence of subjective perception appears negligible [@kuzon_1996_seven]. The balanced option of "Neither favor nor oppose" separates the scale into two sides, while the wording of the opposing/supporting counterparts is identical. This leaves comparatively little room for subjective perception to take effect and thus skew the results [@knapp_1990_treating]. Since all Likert scales used here take the latter form, subjective perception differences should thus not play a major role.

 

   
To more appropriately measure the distances between ordinal variable categories, I propose an ordered probit approach that estimates the latent continuous structure underlying ordinal variables. The next section outlines this approach.



### Ordered Probit Approach {#ordblock-theory-op}

Many approaches in the literature on the analysis of ordinal variables incorporate the distribution of the variable categories [@agresti_1996_introduction]. The most promising suggestions focus on natural extensions of probit and logit models [@winship_1984_regression] by assigning scores to be estimated from the data [@agresti_1990_categorical] and quantifying each non-quantitative variable according to the empirical distributions of the variable, assuming the presence of a continuous underlying variable for each ordinal indicator [@lucadamoa_2014_scaling]. In fact, @agresti_2010_analysis states "that the type of ordinal method used is not that crucial" but that the "results may be quite different, however, from those obtained using methods that treat all the variables as nominal" (p. 3). The same applies to methods which treat ordinal variables as interval [@gertheiss_2008_penalized]. This suggests that a probit or logit model is suitable to uncover the latent continuous variable underlying an ordinal variable, thereby using the ordinal information provided and respecting uneven distances. In the literature, this approach is focused exclusively on the analysis of ordinal variables as a response variable. I propose an ordered probit model that applies to ordinal variables as predictors. The workflow of the model is shown in Figure \ref{op-workflow}.

\vspace{0.2cm}

\ssp

\begin{figure}
\centering
\begin{tikzpicture}
    \node [block2rw] (model) {\scriptsize{Linear Model: Ordinal Variable $\sim$ Predictors}};
    \node [block2rw, below =1cm of model] (train) {\scriptsize{Train model on data}};
    \node [block2rw, below =1cm of train] (cutoff) {\scriptsize{Estimate cutoff thresholds between categories}};
    \node [block2rw, below =1cm of cutoff] (bin) {\scriptsize{Bin cases according to linear predictors}};
    \node [block2rw, below =1cm of bin] (cats) {\scriptsize{Use re-estimated categories}};
	\coordinate[below=0cm of model] (modelc);
	\coordinate[above=0cm of train] (trainac);
	\coordinate[below=0cm of train] (trainbc);
	\coordinate[above=0cm of cutoff] (cutoffac);
	\coordinate[below=0cm of cutoff] (cutoffbc);
	\coordinate[above=0cm of bin] (binac);
	\coordinate[below=0cm of bin] (binbc);
	\coordinate[above=0cm of cats] (catsc);
	\path [line] (modelc) -- (trainac);
	\path [line] (trainbc) -- (cutoffac);
	\path [line] (cutoffbc) -- (binac);
	\path [line] (binbc) -- (catsc);
\end{tikzpicture}
\caption{Ordered Probit Workflow} 
\label{op-workflow}
\end{figure}

\dsp

\vspace{-0.4cm}

We need to estimate a linear combination of meaningful covariates as predictors and an ordinal variable as the dependent variable. We then train this model on externally and internally valid data to estimate cutoff thresholds between the ordinal categories and bin data cases according to the linear predictors. The binned cases determine which variable categories make sense, given the underlying latent continuous variable. We then replace the original categories with these re-estimated categories and conduct the statistical analysis of interest. 

Notationally, let there be $\bm{X}$, an $n \times k$ matrix of explanatory variables. Let further $\bm{Y}$ be observed on the ordered categories $\bm{Y}_i \in [1,\ldots,k]$, for $i=1,\ldots n$, and let $\bm{Y}$ be assumed to be produced by the unobserved latent continuous variable $\bm{Y^{cont}}$. $\bm{Y^{cont}}$ is continuous on $\mathfrak{R}$ from $-\infty$ to $\infty$. The 'response mechanism' for the $r^{th}$ category is $Y=r \Longleftrightarrow \xi_{r-1} < Y^{cont} < \xi_r$. This requires there to be thresholds on $R$:
$Y^{cont}_i: \; \xi_0 \underset{a=1}{\longleftarrow\!\longrightarrow} \xi_1 \underset{a=2}{\longleftarrow\!\longrightarrow} \xi_2 \underset{a=3}{\longleftarrow\!\longrightarrow} \xi_3\ldots \xi_{A-1} \underset{a=A}{\longleftarrow\!\longrightarrow} \xi_A$. The vector of (unseen) utilities across individuals in the sample, $\bm{Y^{cont}}$, is determined by a linear model of explanatory variables, $Y^{cont} = X \beta + \mu$, where $\bm{\beta} =[\beta_1,\beta_2,\ldots,\beta_p]$ does not depend on the $\xi_j$ and $\mu \sim F_{\mu}$. For the observed vector $\bm{Y}$,

\begin{align}
p(Y \leq r|X) &= p(Y^{cont} \leq \xi_r) = p(X \beta + \mu \leq \xi_r) \nonumber\\
&= p(\mu \leq \xi_r - X \beta) = F_{\mu}(\xi_r - X \beta)
\end{align}
            
is called the cumulative model because $p(Y \leq \xi_r|X) = p(Y=1|X) + p(Y=2|X) + \ldots + p(Y=r|X)$. A logistic distributional assumption on the errors produces the ordered logit specification

\begin{align}
F_{\mu}(\xi_r - X \beta) = P(Y \leq r|X) = [1+\exp(-\xi_r + X \beta)]^{-1}. 
\end{align}

The likelihood function is 

\begin{align}
L(\beta, \xi|X,Y) = \prod_{i=1}^{n}\prod_{j=1}^{A-1}\left[\Lambda(\xi_j + X_i' \beta) - \Lambda(\xi_{j-1} + X_i' \beta) \right]^{z_{ij}}
\end{align}

with $\bm{\Lambda}$ denoting the logistic distribution and where $z_{ij}=1$ if the $i^\text{th}$ case is in the $j^\text{th}$ category and $z_{ij}=0$ otherwise. The thresholds on $\mathfrak{R}$ partition the variable into regions corresponding to the ordinal categories. The linear model, $\bm{Y^{cont}}$, bins the observations between these thresholds according to the linear predictors. In `R`, the ordered probit model can be implemented with the `polr` function from the `MASS` package [@ripley_2020_package].







## Data {#ordblock-data}

I use the American National Election Studies (ANES) to implement the proposed ordered probit method. The ANES are the oldest continuous collection of national surveys on electoral behavior and attitudes in the US and are conducted before and after every US presidential and Congressional election by the University of Michigan and Stanford University. Data have been collected since 1948 in the attempt to understand voter behavior and candidate choice, among many others. The list of questions has been continually expanded and refined over the years. All ANES data are publicly available and the ANES are frequently used for high-profile publications [see for instance @jackman_2018_does; @leighley_who_2014]. I apply the proposed ordered probit method to the 2016 ANES data and train a specified regression model on these data. This estimates the thresholds between each existing ANES education category and bins all observations according to the linear predictors to determine the education categories that make sense, based on the underlying latent continuous variable. This results in two sets of education categories: ANES and ordered probit (OP). The data are then blocked into three treatment groups to simulate a survey experiment environment. This process is conducted twice: once with the ANES categories, and once with the OP categories. To simulate sequential blocking, the order of observations is assumed to represent the sequential order of arrival for treatment. The estimation of group means and variances together with statistical tests reveals the suitability of this approach. Finally, I block the ANES data once more into two treatment groups and conduct a placebo regression to test model fitness further.




## Results {#ordblock-results}

Recall that we need to estimate a linear combination of meaningful covariates as predictors of an ordinal variable as the dependent variable. This model needs to be trained on externally and internally valid data. The ANES data have been shown to fulfill these criteria [@krupnikov_2014_cross-sample;@malhotra_2007_effect]. I train the following model on these data, using standard demographics in political science as predictors for all observations $i$ with $i = [1,\ldots,n]$: 

\begin{align}
\text{Education}_i \sim & \,\beta_0 + \beta_1 \text{Gender}_i + \beta_2 \text{Race}_i + \beta_3 \text{Age}_i + \beta_4 \text{Income}_i \,+ \nonumber \\
& \beta_5 \text{Occupation}_i + \beta_6 \text{Party ID}_i + \epsilon_i \label{trainAnes}
\end{align}

This estimates cutoff thresholds between the categories and bins data cases according to the linear predictors. The cutoff coefficients between each of the education categories are shown in Table \ref{education-categories}.


```{r Education Thresholds Table, include=FALSE}
op.model.thresholds <- read.csv("data/blocking/thresholds.csv")
colnames(op.model.thresholds) <- c("Thresholds", "Coefficients", "Standard Errors", "t-values")
stargazer(op.model.thresholds, 
          summary = FALSE, 
          rownames = FALSE, 
          header=FALSE, 
          align = TRUE, 
          title = "Ordered Probit Threshold Estimates", 
          label = "education-categories")
```

\begin{table}[!htbp] \centering 
  \caption{Ordered Probit Threshold Estimates} 
  \label{education-categories} 
\begin{tabular}{r@{}lr@{}lr@{}lr@{}l} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{2}{c}{Thresholds} & \multicolumn{2}{c}{Coefficients} & \multicolumn{2}{c}{Standard Errors} & \multicolumn{2}{c}{t-values} \\ 
\hline \\[-1.8ex] 
Up to 1st $\mid$ & \,1st-4th & -7.&869 & 1.&024 & -7.&681 \\ 
1st-4th $\mid$ & \,5th-6th & -7.&146 & 0.&717 & -9.&965 \\ 
5th-6th $\mid$ & \,7th-8th & -5.&379 & 0.&326 & -16.&515 \\ 
7th-8th $\mid$ & \,9th & -4.&671 & 0.&253 & -18.&472 \\ 
9th $\mid$ & \,10th & -3.&920 & 0.&206 & -19.&070 \\ 
10th $\mid$ & \,11th & -3.&468 & 0.&188 & -18.&489 \\ 
11th $\mid$ & \,12th & -2.&984 & 0.&174 & -17.&100 \\ 
12th $\mid$ & \,HS grad & -2.&511 & 0.&166 & -15.&116 \\ 
HS grad $\mid$ & \,Some college & -0.&711 & 0.&154 & -4.&607 \\ 
Some college $\mid$ & \,Associate & 0.&384 & 0.&154 & 2.&500 \\ 
Associate $\mid$ & \,Bachelor's & 1.&045 & 0.&154 & 6.&766 \\ 
Bachelor's $\mid$ & \,Master's & 2.&478 & 0.&160 & 15.&538 \\ 
Master's $\mid$ & \,Professional & 4.&099 & 0.&177 & 23.&144 \\ 
Professional $\mid$ & \,Doctorate & 4.&838 & 0.&197 & 24.&589 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}


The ordered probit model uses the ordinal information with unevenly spaced distances provided and bins observations according to the estimated threshold coefficients, which in turn determines what education categories make sense given the underlying latent continuous variable. Figure \ref{DensEducLP} shows the distribution of this variable. As we can see, no cases fall lower than "12th|HS grad" or higher than "Master's|Professional", which results in a new set of model-estimated OP education categories that fit the data. We thus have two sets of education categories: the original ANES categories, and the model-estimated OP categories.


```{r Education Categories Linear Predictor Plotting Code, include=FALSE}

ord.list <- readRDS("data/anes/ord_list.rds")
lp.values <- data.frame(ord.list$plr.out$lp)
cut.points <- ord.list$int.df[, c(1,2)] %>% 
  filter(., Values >= min(lp.values) & Values < max(lp.values))
cut.points.plot <- rbind(ord.list$int.df[8, c(1,2)], cut.points, ord.list$int.df[13, c(1,2)])

```

```{r Density-Education-Categories-Linear-Predictors, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Education Linear Predictor Data Cases. Vertical Lines Indicate Threshold Coefficients. No Cases Fall Lower than '12th|HS grad' or Higher than 'Master's|Professional'.\\label{DensEducLP}"}

ggplot(lp.values, aes(x=ord.list.plr.out.lp)) + 
  geom_density() +
  xlim(c(plyr::round_any(cut.points.plot[1,2],1), 
         plyr::round_any(cut.points.plot[6,2],.5, f = ceiling))) + 
  geom_vline(xintercept = cut.points.plot$Values, linetype="solid", color = "red") + 
  annotate(geom = "text", x = cut.points.plot$Values, y = 0.2, label = cut.points.plot$Intercepts, 
           vjust = -1, color = "red", size = 3, angle = 90) + 
  xlab("Linear Predictor Data Cases") + ylab("Density")

```

```{r Education Categories Plotting Code, include=FALSE}

op.model.data <- readRDS("data/anes/anes_education.rds")
plot.orig <- ggplot(op.model.data, aes(x = education)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = "darkred") + theme(axis.title=element_blank(), axis.text.x = element_text(angle = 45))
plot.new <- ggplot(op.model.data, aes(x = education.new)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = "darkblue") + theme(axis.title=element_blank(), axis.text.x = element_text(angle = 45))

```

```{r Barplot-Education-Categories, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Education Categories. Original ANES Categories on the Left, Ordered Probit Estimated Categories on the Right. Categories Below 'High School Graduate' and Above 'Master's' Are Gone. OP Categories Can Now Be Used For Blocking.\\label{BarEducCat}"}

grid.arrange(plot.orig, plot.new, ncol = 2, nrow = 1, bottom = "Education", left = "Percentages")

```


Figure \ref{BarEducCat} shows the distributions of both sets. All categories 'below' "High School Graduate" and 'above' "Master's" are gone in the OP set because they do not fit the data. We can now use these estimated education categories as the basis for blocking.^[Using a variable as the dependent variable in a model whilst subsequently using it as an explanatory variable for substantive analysis, as done here with education, raises questions of potential bias. Since the build-up of the variable in question changes drastically between the two analyses in our case, however, I argue that this is not a concern. Education as the dependent variable in equation (\ref{trainAnes}) differs greatly from education as the explanatory variable in subsequent analysis. The former consists of man-made ordinal categories, while the latter reflects the distribution of a data-driven approach. A 'reusing' of the same variable thus does not actually take place since the two versions of the variable differ greatly in terms of structure, which alleviates concerns regarding bias.] Assigning numerical values to the new categories is now justifiable because they are based on data-driven estimations. This allows us to block on numerical values with the MD, which would not be permitted on theoretical grounds without empirical justification. In our case, we observe a normal distribution of the linear predictor data cases, i.e. a smooth distribution where categories drop off on both ends of the scale and all re-estimated categories are adjacent to each other. This allows us to use sequential numerical values for blocking. This is not the case in a potential scenario that shows re-estimated categories on the ends of the scale. In such a scenario, categories in the middle of the scale drop off, resulting in a gap in the distribution. In such a case, category reassignment should not be done with evenly spaced sequential numbers. Instead, the number of dropped-off categories should be taken into account for the numerical conversion by expanding the distances between the numbers based on the number of categories that are gone. Let a distribution of linear predictor data cases show re-estimated ordinal categories A and B on the left end of the scale. Let the distribution further show re-estimated ordinal category E on the right end of the scale as well as a gap where categories C and D would fall. Instead of assigning the numbers 1, 2, and 3 to the remaining categories A, B, and E, I suggest assigning 1, 2, and 5 here since two categories in the middle are gone. This takes the number of dropped-off categories in the distribution into account.

The following sections demonstrate how the newly estimated categories affect blocking and regression results. 




### Blocking Differences {#ordblock-data-blockdiff}


```{r Block-ANES-OP-Categories, include=FALSE}

#### This block is only needed for list.dfs, which I need here. Everything else is used for stuff in the appendix ####

df.an <- readRDS("data/blocking/df_an.RDS")
df.op <- readRDS("data/blocking/df_op.RDS")

for(i in 1:(df.an$education %>% unique %>% length)){
  df.an[df.an$education == i, "education"] <- op.model.data$education %>% levels %>% .[i]
}

df.an$education <- factor(df.an$education, levels = op.model.data$education %>% levels)

for(i in 1:(df.op$education %>% unique %>% length)){
  df.op[df.op$education == i, "education"] <- op.model.data$education.new %>% levels %>% .[i]
}

df.op$education <- factor(df.op$education, levels = op.model.data$education.new %>% levels)
assigned.an <- readRDS("data/blocking/assigned_an.RDS")
assigned.op <- readRDS("data/blocking/assigned_op.RDS")

an.t1 <- df.an[df.an$id %in% assigned.an$assg[[1]][,1],] %>% subset(., select=-c(id, education))
an.t2 <- df.an[df.an$id %in% assigned.an$assg[[1]][,2],] %>% subset(., select=-c(id, education))
an.t3 <- df.an[df.an$id %in% assigned.an$assg[[1]][,3],] %>% subset(., select=-c(id, education))
# an.t4 <- df.an[df.an$id %in% assigned.an$assg[[1]][,4],] %>% subset(., select=-c(id, education))
# an.t5 <- df.an[df.an$id %in% assigned.an$assg[[1]][,5],] %>% subset(., select=-c(id, education))

op.t1 <- df.op[df.op$id %in% assigned.op$assg[[1]][,1],] %>% subset(., select=-c(id, education))
op.t2 <- df.op[df.op$id %in% assigned.op$assg[[1]][,2],] %>% subset(., select=-c(id, education))
op.t3 <- df.op[df.op$id %in% assigned.op$assg[[1]][,3],] %>% subset(., select=-c(id, education))
# op.t4 <- df.op[df.op$id %in% assigned.op$assg[[1]][,4],] %>% subset(., select=-c(id, education))
# op.t5 <- df.op[df.op$id %in% assigned.op$assg[[1]][,5],] %>% subset(., select=-c(id, education))


### props of factor columns, except education ###

list.props <- list()
list.dfs <- list(an.t1, an.t2, an.t3,
                 op.t1, op.t2, op.t3)
# list.dfs <- list(an.t1, an.t2, an.t3, an.t4, an.t5,
#                  op.t1, op.t2, op.t3, op.t4, op.t5)
n.tr <- 3
# n.tr <- 5
an <- c()
op <- c()

for(w in 1:n.tr){
  an[w] <- paste0("ANES", w)
  op[w] <- paste0("OP", w)
}

names(list.dfs) <- c(an, op)
is.not.num <- function(x) !is.numeric(x)
tmp <- dplyr::select_if(an.t1, is.not.num)

for(x in 1:ncol(tmp)){
  list.props[[x]] <- data.frame(matrix(NA, levels(tmp[, x]) %>% length, length(list.dfs)))
  rownames(list.props[[x]]) <- levels(tmp[,x])
  colnames(list.props[[x]]) <- names(list.dfs)
}

names(list.props) <- colnames(tmp)

for(i in 1:ncol(tmp)){
  for(y in 1:length(list.dfs)){
    tmp.fac <- dplyr::select_if(list.dfs[[y]], is.not.num)
    list.props[[i]][,y] <- prop.table(table(tmp.fac[,i]))
  }
}

df.props <- plyr::ldply(list.props, data.frame, .id = NULL) 
lev.names <- sapply(tmp, levels) %>% unlist
lev.names["pid4"] <- "Something else"
rownames(df.props) <- lev.names
df.props <- round(df.props, digits = 3) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3")]
# df.props <- drop.zero(df.props, digits = 3) %>%
#   .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]


### means of numeric columns ###

list.means <- list()
tmp2 <- dplyr::select_if(op.t1, is.numeric)

for(x in 1:ncol(tmp2)){
  list.means[[x]] <- data.frame(matrix(NA, 1, length(list.dfs)))
  colnames(list.means[[x]]) <- names(list.dfs)
}

names(list.means) <- colnames(tmp2)

for(i in 1:ncol(tmp2)){
  for(y in 1:length(list.dfs)){
    tmp.means <- dplyr::select_if(list.dfs[[y]], is.numeric)
    list.means[[i]][,y] <- mean(tmp.means[,i]) %>% round(., digits = 3)
  }
}

df.means <- plyr::ldply(list.means, data.frame, .id = NULL) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3")]
# df.means <- plyr::ldply(list.means, data.frame, .id = NULL) %>%
#   .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]

rownames(df.means) <- colnames(tmp2) %>%
  gsub("\\.", " ", .) %>%
  tools::toTitleCase(.)


### now education ###

an.ed.t1 <- df.an[df.an$id %in% assigned.an$assg[[1]][,1],] %>% subset(., select=c(education))
an.ed.t2 <- df.an[df.an$id %in% assigned.an$assg[[1]][,2],] %>% subset(., select=c(education))
an.ed.t3 <- df.an[df.an$id %in% assigned.an$assg[[1]][,3],] %>% subset(., select=c(education))
# an.ed.t4 <- df.an[df.an$id %in% assigned.an$assg[[1]][,4],] %>% subset(., select=c(education))
# an.ed.t5 <- df.an[df.an$id %in% assigned.an$assg[[1]][,5],] %>% subset(., select=c(education))

list.eds.an <- list(an.ed.t1, an.ed.t2, an.ed.t3)
# list.eds.an <- list(an.ed.t1, an.ed.t2, an.ed.t3, an.ed.t4, an.ed.t5)

df.ed.an <- data.frame(matrix(NA, levels(an.ed.t1[,1]) %>% length, length(list.eds.an)))
rownames(df.ed.an) <- levels(an.ed.t1[,1])
colnames(df.ed.an) <- an

for(i in 1:ncol(an.ed.t1)){
  for(y in 1:length(list.eds.an)){
    df.ed.an[,y] <- prop.table(table(list.eds.an[[y]][,i]))
  }
}

df.ed.an <- round(df.ed.an, digits = 3)

op.ed.t1 <- df.op[df.op$id %in% assigned.op$assg[[1]][,1],] %>% subset(., select=c(education))
op.ed.t2 <- df.op[df.op$id %in% assigned.op$assg[[1]][,2],] %>% subset(., select=c(education))
op.ed.t3 <- df.op[df.op$id %in% assigned.op$assg[[1]][,3],] %>% subset(., select=c(education))
# op.ed.t4 <- df.op[df.op$id %in% assigned.op$assg[[1]][,4],] %>% subset(., select=c(education))
# op.ed.t5 <- df.op[df.op$id %in% assigned.op$assg[[1]][,5],] %>% subset(., select=c(education))

list.eds.op <- list(op.ed.t1, op.ed.t2, op.ed.t3)
# list.eds.op <- list(op.ed.t1, op.ed.t2, op.ed.t3, op.ed.t4, op.ed.t5)

df.ed.op <- data.frame(matrix(NA, levels(op.ed.t1[,1]) %>% length, length(list.eds.op)))
rownames(df.ed.op) <- levels(op.ed.t1[,1])
colnames(df.ed.op) <- op

for(i in 1:ncol(op.ed.t1)){
  for(y in 1:length(list.eds.op)){
    df.ed.op[,y] <- prop.table(table(list.eds.op[[y]][,i]))
  }
}

df.ed.op <- round(df.ed.op, digits = 3)
top <- data.frame(matrix(NA, 8, length(list.eds.op)))
colnames(top) <- colnames(df.ed.op)
rownames(top) <- rownames(df.ed.an)[1:8]
bottom <- data.frame(matrix(NA, 2, length(list.eds.op)))
colnames(bottom) <- colnames(top)
rownames(bottom) <- rownames(df.ed.an)[14:15]
df.ed.all <-rbind(top, df.ed.op, bottom) %>% cbind(df.ed.an, .) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3")]
# df.ed.all <-rbind(top, df.ed.op, bottom) %>% cbind(df.ed.an, .) %>%
#   .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]

### combine and print (and manually adjust) factors, numerics, education ###

df.all <- rbind(df.props, df.means) # I decided not to include ed.all because I don't want to have to explain the differing numbers (which might well be meaningless anyway)
rownames(df.all)[17] <- "Employed" # sounds better than "Working"

```

```{r Variance-In-Treatment-Groups, include=FALSE}

##### compare treatment groups in AN with treatment groups in OP #####

list.dfs.var <- list.dfs # so I don't mess with the output from above

# add a treatment group column (ANES 1, OP 1 etc.) to each data frame
# result is a list of length 6
# each cat only has one unique value (e.g. ANES 1)
for(x in 1:(length(list.dfs)/2)){
  list.dfs.var[[x]]$cat <- rep(paste0("ANES", x), nrow(list.dfs.var[[x]])) %>% as.factor
  list.dfs.var[[x+3]]$cat <- rep(paste0("OP", x), nrow(list.dfs.var[[x+3]])) %>% as.factor
}

list.dfs.t <- list()

# combine ANES and OP data for each treatment group
# result is a list of length 3
# cat now has two unique values respective of treatment group (e.g. OP 1, ANES 1)
for(x in 1:(length(list.dfs)/2)){
  list.dfs.t[[x]] <- c(list.dfs.var[x], list.dfs.var[x+3]) %>%
    ldply(., data.frame) %>% subset(., select =-c(.id))
}

num.res <- rep(list(list()), 2)
names(num.res) <- c("aov.sum", "aov.tukey")
list.dfs.num.aov <- rep(list(num.res), 3)
names(list.dfs.num.aov) <- c("T1", "T2", "T3")

for(t in 1:length(list.dfs.num.aov)){
  for(w in 1:ncol(dplyr::select_if(list.dfs.t[[1]], is.numeric))){
    aa <- dplyr::select_if(list.dfs.t[[t]], is.numeric) # select only numeric columns
    bb <- cbind(aa, list.dfs.t[[t]]$cat) # combine numeric columns with cat column
    colnames(bb) <- c(colnames(aa), "cat")
    aov.out <- aov(bb[,w] ~ bb$cat) # aov regression of each numeric column on cat
    list.dfs.num.aov[[t]]$aov.sum[[w]] <- summary(aov.out)
    names(list.dfs.num.aov[[t]]$aov.sum)[[w]] <- colnames(bb)[w]
    list.dfs.num.aov[[t]]$aov.tukey[[w]] <- TukeyHSD(aov.out)
    names(list.dfs.num.aov[[t]]$aov.tukey)[[w]] <- colnames(bb)[w]
    # num.aov.sum[[w]] <- summary(aov.out)
    # names(num.aov.sum)[[w]] <- paste0(colnames(bb)[w], ".sum")
    # num.aov.tukey[[w]] <- TukeyHSD(aov.out)
    # names(num.aov.tukey)[[w]] <- paste0(colnames(bb)[w], ".tukey")
  }
}

fac.res <- rep(list(list()), 2)
names(fac.res) <- c("glm.sum", "glm.anov")
list.dfs.fac.aov <- rep(list(fac.res), 3)
treats <- c("T1", "T2", "T3")
names(list.dfs.fac.aov) <- treats

for(t in 1:length(list.dfs.num.aov)){
  for(w in 1:(ncol(dplyr::select_if(list.dfs.t[[1]], is.not.num))-1)){
    my.dat <- dplyr::select_if(list.dfs.t[[t]], is.not.num)
    my.mod <- glm(my.dat[,w] ~ my.dat$cat, family = "binomial")
    list.dfs.fac.aov[[t]]$glm.sum[[w]] <- summary(my.mod)
    names(list.dfs.fac.aov[[t]]$glm.sum)[[w]] <- colnames(my.dat)[w]
    list.dfs.fac.aov[[t]]$glm.anov[[w]] <- anova(my.mod, test = "Chisq")
    names(list.dfs.fac.aov[[t]]$glm.anov)[[w]] <- colnames(my.dat)[w]
  }
}

# These commented out lines are just for me as an overview how all the lists are structured and how to get the numbers as a df
# Lists of length 3, named T1 to T3
# list.dfs.num.aov %>% length
# list.dfs.num.aov %>% names
# list.dfs.fac.aov %>% length
# list.dfs.fac.aov %>% names
# 
# Lists of length 2, named aov.sum, aov.tukey and glm.sum, glm.anov
# list.dfs.num.aov[["T1"]] %>% length
# list.dfs.num.aov[["T1"]] %>% names
# list.dfs.fac.aov[["T1"]] %>% length
# list.dfs.fac.aov[["T1"]] %>% names
# 
# Lists of lengths 2 and 8, named after variables (age, feel.trump and gender, race, income, occupation, pid, pres.approv, min.wage, country.track)
# list.dfs.num.aov[["T1"]][["aov.sum"]] %>% length
# list.dfs.num.aov[["T1"]][["aov.sum"]] %>% names
# list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% length
# list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% names
# 
# Classes and names of each output
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]] %>% class # summary.aov, listof
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]] %>% names # NULL
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]] %>% class # summary.glm
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]] %>% names # lots of useful names
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]] %>% class # TukeyHSD, multicomp
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]] %>% names # bb$cat
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]] %>% class # anova, data.frame
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]] %>% names # lots of names
#
# How to transform each output into a data frame
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]][[1]] %>% data.frame
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]]$coefficients %>% data.frame
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]]$`bb$cat` %>% data.frame
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]][2,] %>% data.frame


# Store the output of each method for all treatment groups, so 4 data frames where each df contains all groups
aov.names <- list.dfs.num.aov[["T1"]][["aov.sum"]] %>% names # could have also used aov.tukey here
aov.length <- list.dfs.num.aov[["T1"]][["aov.sum"]] %>% length
glm.names <- list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% names # could have also used glm.anov here
glm.length <- list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% length # could have also used glm.anov here

list.aov.sum <- list.aov.tukey <- rep(list(list()), length(aov.names))
list.glm.sum <- list.glm.anov <- rep(list(list()), length(glm.names))

for(i in 1:length(treats)){
  tmp.aov.sum <- list.dfs.num.aov[[i]][["aov.sum"]]
  tmp.aov.tukey <- list.dfs.num.aov[[i]][["aov.tukey"]]
  for(x in 1:aov.length){ # this loop does tmp.aov.sum and tmp.aov.tukey
    tmp.sum <- tmp.aov.sum[[names(tmp.aov.sum)[[x]]]][[1]] %>% data.frame
    rownames(tmp.sum) <- c(paste0(names(tmp.aov.sum)[[x]], treats[i]),
                           paste0(names(tmp.aov.sum)[[x]], treats[i], "Resid"))
    list.aov.sum[[x]][[i]] <- tmp.sum
    tmp.tukey <- tmp.aov.tukey[[names(tmp.aov.tukey)[[x]]]]$`bb$cat` %>% data.frame
    rownames(tmp.tukey) <- paste0(names(tmp.aov.tukey)[[x]], treats[i], rownames(tmp.tukey))
    list.aov.tukey[[x]][[i]] <- tmp.tukey
  }
  tmp.glm.sum <- list.dfs.fac.aov[[i]][["glm.sum"]]
  tmp.glm.anov <- list.dfs.fac.aov[[i]][["glm.anov"]]
  for(y in 1:glm.length){ # this loop does tmp.glm.sum and tmp.glm.anov
    tmp.sum <- tmp.glm.sum[[names(tmp.glm.sum)[[y]]]]$coefficients %>% data.frame
    rownames(tmp.sum) <- c(paste0(names(tmp.glm.sum)[[y]], treats[i], "Int"),
                           paste0(names(tmp.glm.sum)[[y]], treats[i]))
    list.glm.sum[[y]][[i]] <- tmp.sum
    tmp.anov <- tmp.glm.anov[[names(tmp.glm.anov)[[y]]]][2,] %>% data.frame
    rownames(tmp.anov) <- paste0(names(tmp.glm.anov)[[y]], treats[i])
    list.glm.anov[[y]][[i]] <- tmp.anov
  }
}

df.aov.sum <- unlist(list.aov.sum, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.aov.sum <- cbind(df.aov.sum[, 1:3] , round(df.aov.sum[, 4:5], digits = 3))
df.aov.tukey <- unlist(list.aov.tukey, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.glm.sum <- unlist(list.glm.sum, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.glm.anov <- unlist(list.glm.anov, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.glm.anov <- cbind(df.glm.anov[, c(1,3,4)], round(df.glm.anov[, c(2, 5)], digits = 3)) %>% .[,c(1,4,2,3,5)]

colnames(df.aov.sum) <- c("Df", "Sum.Sq", "Mean.Sq", "F-value", "p-value")
colnames(df.aov.tukey) <- c("Diff.Means", "CI Lower", "CI Upper", "Adj. p-value")
colnames(df.glm.sum) <- c("Estimate", "Std.Error", "z-value", "p-value")
colnames(df.glm.anov) <- c("Df", "Deviance", "Resid.Df", "Residual Deviance", "Pr.Chi")

# this one is below
tab.aov.sum <- stargazer(df.aov.sum, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Summary of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "aov-sum")
at <- gsub("{c}", "{l}", tab.aov.sum, fixed = TRUE) %>%
  gsub("age", "", ., fixed = TRUE) %>%
  gsub("feel.trump", "", ., fixed = TRUE) %>%
  gsub("Resid", " Residuals", ., fixed = TRUE)
cat(at)

# this one is commented out below
tab.aov.tukey <- stargazer(df.aov.tukey, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Tukey's 'Honest Significant Difference' Test of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "aov-tukey")
bt <- gsub("{c}", "{l}", tab.aov.tukey, fixed = TRUE) %>%
  gsub("age", "", ., fixed = TRUE) %>%
  gsub("feel.trump", "", ., fixed = TRUE) %>%
  gsub("OP", " OP", ., fixed = TRUE)
cat(bt)

# this one is below
tab.glm.sum <- stargazer(df.glm.sum, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Summary of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "glm-sum")
ct <- gsub("{c}", "{l}", tab.glm.sum, fixed = TRUE) %>%
  gsub("gender", "", ., fixed = TRUE) %>%
  gsub("race", "", ., fixed = TRUE) %>%
  gsub("income", "", ., fixed = TRUE) %>%
  gsub("occupation", "", ., fixed = TRUE) %>%
  gsub("pid", "", ., fixed = TRUE) %>%
  gsub("pres.approv", "", ., fixed = TRUE) %>%
  gsub("min.wage", "", ., fixed = TRUE) %>%
  gsub("country.track", "", ., fixed = TRUE) %>%
  gsub("Int", " Intercept", ., fixed = TRUE) %>%
  gsub("\\multicolumn{1}{l}{", "", ., fixed = TRUE)
cat(ct)

# this one is commented out in the appendix
tab.glm.anov <- stargazer(df.glm.anov, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "ANOVA Chisq Test of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "glm-anov")
dt <- gsub("{c}", "{l}", tab.glm.anov, fixed = TRUE) %>%
  gsub("gender", "", ., fixed = TRUE) %>%
  gsub("race", "", ., fixed = TRUE) %>%
  gsub("income", "", ., fixed = TRUE) %>%
  gsub("occupation", "", ., fixed = TRUE) %>%
  gsub("pid", "", ., fixed = TRUE) %>%
  gsub("pres.approv", "", ., fixed = TRUE) %>%
  gsub("min.wage", "", ., fixed = TRUE) %>%
  gsub("country.track", "", ., fixed = TRUE)
cat(dt)

# To align ct along dots: Straighten the cat output out. Copy only the actual rows into a Latex doc. Replace all } with nothing. Replace all . with .&
# Copy rows back into R. Add \multicolumn{2}{l} for the column headings. Add variable title rows. Replace extracolsep etc. with r@{}l for each numerical column.
# Add longtable stuff


```


We block the ANES on education into three treatment groups, once based on the original ANES education categories, and once based on the newly estimated OP education categories. We overall do not observe large differences between the ANES and OP variable proportions/means in each treatment group with the naked eye (see appendix Figures \ref{ANBlock1} to \ref{ANBlock3} for details).

To dig deeper, I run an Analysis of Variance (ANOVA) test on the numerical variables `Age` and `Feel Trump`. An ANOVA regression provides information about levels of variability within a regression model and forms a basis for tests of significance. As the name implies, ANOVA analyzes the variance in the data to look for differences in the means by calculating the the sums of squares to measure the distances of each data point to the mean. The ratio of the sums of squares forms the F-statistic, which is used to gauge statistical significance. 

I form a combined data set of all observations for each education set for each treatment group. This results in three data sets, i.e. one per treatment group. The first data set contains all observations from the ANES set that were assigned to treatment group 1 and all observations from the OP set that were assigned to treatment group 1. The second data set contains all ANES and OP observations that were assigned to treatment group 2. The third data set contains all ANES and OP observations that were assigned to treatment group 3. Each data set contains the column `Education Set` which denotes the education set each observation belongs to. For each data set, I run an ANOVA regression of the numerical variables `Age` and `Feel Trump` on `Education Set` to test whether the choice of education set is associated with these variables. Table \ref{aov-sum} shows a summary of the results. Almost none of the variable intercepts show statistical significance. The only exception is treatment group 2, which shows a $p$-value of `r df.aov.sum["feel.trumpT2", "p-value"]` for `Feel Trump`. Overall, however, the differences between the treatment group means of the numerical variables `Age` and `Feel Trump` are not statistically significant for this sample size.


 \begin{table}[!htbp] \centering  
 \caption{Summary of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}   
 \label{aov-sum}
 \begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{l}{} & \multicolumn{1}{l}{Df} & \multicolumn{1}{l}{Sum.Sq} & \multicolumn{1}{l}{Mean.Sq} & \multicolumn{1}{l}{F-value} & \multicolumn{1}{l}{p-value} \\ 
 \hline \\[-1.8ex] 
\multicolumn{1}{l}{\textbf{Age}} & & & & & \\ 
 \multicolumn{1}{l}{T1} & 1 & 0.107 & 0.107 & 0.000 & 0.985 \\
 \multicolumn{1}{l}{T1 Residuals} & 2,098 & 641,917.600 & 305.966 & & \\
 \multicolumn{1}{l}{T2} & 1 & 414.519 & 414.519 & 1.340 & 0.247 \\ 
 \multicolumn{1}{l}{T2 Residuals} & 2,098 & 648,946.600 & 309.317 & & \\
 \multicolumn{1}{l}{T3} & 1 & 427.954 & 427.954 & 1.401 & 0.237 \\
 \multicolumn{1}{l}{T3 Residuals} & 2,098 & 641,038.400 & 305.547 & & \\ 
 & & & & & \\
\multicolumn{1}{l}{\textbf{Feel Trump}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & 1,164.808 & 1,164.808 & 0.979 & 0.323 \\ 
 \multicolumn{1}{l}{T1 Residuals} & 2,098 & 2,496,492.000 & 1,189.939 & & \\
 \multicolumn{1}{l}{T2} & 1 & 7,054.667 & 7,054.667 & 5.699 & 0.017 \\ 
 \multicolumn{1}{l}{T2 Residuals} & 2,098 & 2,597,204.000 & 1,237.943 & & \\
 \multicolumn{1}{l}{T3} & 1 & 2,486.298 & 2,486.298 & 2.023 & 0.155 \\ 
 \multicolumn{1}{l}{T3 Residuals} & 2,098 & 2,577,984.000 & 1,228.782 & & \\ 
 \hline \\[-1.8ex]
 \end{tabular} 
 \end{table}


This leaves the non-numerical factor variables (e.g. `Gender`, `Race` etc.). Since an ANOVA test is not possible for these variables, I conduct a binomial GLM regression here. The results are shown in Table \ref{glm-sum}. Contrary to the numerical variables, a large number of the $p$-values for the variable intercepts are highly significant. This includes all intercepts of `Race`, `Income`, `Occupation`, `Party ID`, `Min. Wage`, and `Country Track`. The only slight exceptions are `Gender` and `Pres. Approval`, where the intercepts for treatment group 1 (`Gender`) and treatment groups 2 and 3 (`Pres. Approval`) do not reach statistical significance. The large number of statistically distinct categories indicates that a differentiation between education categories may be important for non-numerical factor variables. 


\ssp

\footnotesize

\begin{longtable}{lr@{}lr@{}lr@{}lr@{}l}  
\caption{Summary of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}    
\label{glm-sum}  
\\[-1.8ex]\hline  
\hline \\[-1.8ex]  
  & \multicolumn{2}{l}{Estimate} & \multicolumn{2}{l}{Std.Error} & \multicolumn{2}{l}{z-value} & \multicolumn{2}{l}{p-value} \\  
\hline \\[-1.8ex]  
\textbf{Gender} & & & & \\  
 T1 Intercept & 0.&091 & 0.&062 & 1.&481 & 0.&139 \\ 
 T1 & 0.&038 & 0.&087 & 0.&437 & 0.&662 \\ 
 T2 Intercept & 0.&126 & 0.&062 & 2.&035 & 0.&042 \\
 T2 & -0.&004 & 0.&087 & -0.&044 & 0.&965 \\
 T3 Intercept & 0.&134 & 0.&062 & 2.&159 & 0.&031 \\
 T3 & -0.&034 & 0.&087 & -0.&393 & 0.&694 \\ 
 & & & & \\  
\textbf{Race} & & & & \\  
 T1 Intercept & -1.&061 & 0.&071 & -15.&024 & 0.&000 \\
 T1 & -0.&035 & 0.&100 & -0.&351 & 0.&726 \\ 
 T2 Intercept & -1.&153 & 0.&072 & -15.&952 & 0.&000 \\
 T2 & 0.&041 & 0.&102 & 0.&407 & 0.&684 \\
 T3 Intercept & -1.&206 & 0.&073 & -16.&452 & 0.&000 \\
 T3 & -0.&005 & 0.&104 & -0.&052 & 0.&959 \\
   & & & & \\  
\textbf{Income} & & & & \\  
 T1 Intercept & 1.&334 & 0.&076 & 17.&557 & 0.&000 \\
 T1 & -0.&006 & 0.&107 & -0.&054 & 0.&957 \\
 T2 Intercept & 1.&227 & 0.&074 & 16.&649 & 0.&000 \\ 
 T2 & 0.&038 & 0.&105 & 0.&367 & 0.&714 \\ 
 T3 Intercept & 1.&398 & 0.&077 & 18.&058 & 0.&000 \\ 
 T3 & -0.&036 & 0.&109 & -0.&327 & 0.&744 \\
 & & & & \\  
\textbf{Employment} & & & & \\  
 T1 Intercept & -0.&421 & 0.&063 & -6.&678 & 0.&000 \\
 T1 & -0.&246 & 0.&091 & -2.&713 & 0.&007 \\
 T2 Intercept & -0.&567 & 0.&064 & -8.&831 & 0.&000 \\
 T2 & 0.&193 & 0.&090 & 2.&152 & 0.&031 \\
 T3 Intercept & -0.&588 & 0.&064 & -9.&126 & 0.&000 \\ 
 T3 & 0.&049 & 0.&091 & 0.&545 & 0.&586 \\ 
 & & & & \\  
  \textbf{Party ID} & & & & \\  
 T1 Intercept & 0.&530 & 0.&064 & 8.&297 & 0.&000 \\
 T1 & 0.&125 & 0.&091 & 1.&367 & 0.&172 \\
 T2 Intercept & 0.&655 & 0.&065 & 10.&065 & 0.&000 \\
 T2 & -0.&125 & 0.&091 & -1.&367 & 0.&172 \\
 T3 Intercept & 0.&609 & 0.&065 & 9.&421 & 0.&000 \\ 
 T3 & -0.&000 & 0.&091 & -0.&000 & 1.&000 \\ 
 & & & & \\  
\textbf{President} & & & & \\  
 T1 Intercept & -0.&210 & 0.&062 & -3.&388 & 0.&001 \\ 
 T1 & 0.&058 & 0.&088 & 0.&657 & 0.&511 \\ 
 T2 Intercept & -0.&008 & 0.&062 & -0.&123 & 0.&902 \\ 
 T2 & -0.&134 & 0.&087 & -1.&528 & 0.&126 \\ 
 T3 Intercept & -0.&103 & 0.&062 & -1.&666 & 0.&096 \\
 T3 & 0.&076 & 0.&087 & 0.&873 & 0.&383 \\ 
 & & & & \\  
\textbf{Minimum Wage} & & & & \\  
 T1 Intercept & -0.&613 & 0.&065 & -9.&480 & 0.&000 \\ 
 T1 & 0.&042 & 0.&091 & 0.&456 & 0.&649 \\ 
 T2 Intercept & -0.&465 & 0.&063 & -7.&340 & 0.&000 \\
 T2 & -0.&106 & 0.&090 & -1.&173 & 0.&241 \\
 T3 Intercept & -0.&685 & 0.&065 & -10.&472 & 0.&000 \\ 
 T3 & 0.&068 & 0.&092 & 0.&736 & 0.&462 \\
 & & & & \\  
\textbf{Country} & & & & \\  
 T1 Intercept & 0.&954 & 0.&069 & 13.&850 & 0.&000 \\ 
 T1 & -0.&005 & 0.&097 & -0.&049 & 0.&961 \\ 
 T2 Intercept & 1.&111 & 0.&071 & 15.&544 & 0.&000 \\ 
 T2 & -0.&055 & 0.&100 & -0.&552 & 0.&581 \\ 
 T3 Intercept & 0.&992 & 0.&069 & 14.&282 & 0.&000 \\
 T3 & 0.&059 & 0.&099 & 0.&593 & 0.&553 \\ 
 \hline \\[-1.8ex]
 \end{longtable} 

\dsp

\normalsize


Overall, the evidence is mixed. While blocking on two different sets of education categories does not result in significantly differing means for the numerical variables, it does yield statistically significant differences between the factor variable proportions.




### Placebo Regression {#ordblock-data-plac}

To further test the usefulness of the ordered probit categories, I conduct a placebo regression. Placebo tests are most commonly used for difference-in-differences estimators and are falsification tests to analyze whether an effect exists that should not exist [@bertrand_2004_much; @mills_2009_palgrave; @rothstein_2010_teacher]. The placebo treatment should be unrelated to the model or method being studied [@hartman_2018_equivalence; @rosenbaum_2002_observational; @mora_2019_alternative]. In the effort to conduct an analysis that is separate from the previous section, I block the 2016 ANES data anew on education, this time into two treatment groups. As before, this is done separately for the original ANES and the OP education categories. We then model the following OLS regression on the feeling thermometer towards Donald Trump as the Republican presidential candidate for all observations $i$ with $i = [1,\ldots,n]$:

\begin{align}
\text{Feel Trump}_i \sim & \,\beta_0 + \beta_1 \text{Group}_i + \beta_2 \text{Democrat}_i + \beta_3 \text{Republican}_i + \beta_4 \text{Income}_i \,+ \nonumber \\ 
& \beta_5 \text{Male}_i + \beta_6 \text{White}_i + \beta_7 \text{Black}_i + \beta_8 \text{Hispanic}_i.
\end{align}

The \texttt{Group} variable indicates the two differing sets of education categories and is a placebo treatment, i.e. I did not conduct an experiment and no actual treatment was administered. Instead, I created the \texttt{Group} variable, randomly 'assigned' observations to the two treatment groups, and then used this variable as an explanatory variable. The treatment is thus completely artificial, which means the difference between both treatment groups should be zero, i.e. there should be no significant differences between the \texttt{Group} regression coefficients for the two education sets. The null hypothesis thus assumes that there is no effect. To test this, each blocking/regression process for each set of categories is repeated 1,000 times. The distribution of the placebo treatment indicator (\texttt{Group}) is visualized in Figure \ref{DensPlacTreat}. 


```{r Placebo Treatment Plotting Code, include=FALSE}

repeats <- 1000
list.list.gt2.coeff <- readRDS(paste("data/blocking/list_list_test_ols_trump_gt2_coeff_all_obs_", repeats, "_runs.rds", sep = ""))

gt2.orig <- unlist(list.list.gt2.coeff[[1]], recursive = FALSE)
gt2.new <- unlist(list.list.gt2.coeff[[2]], recursive = FALSE)

gt2.orig.df <- data.frame(cbind(gt2.orig, rep("ANES", length(gt2.orig))))
gt2.new.df <- data.frame(cbind(gt2.new, rep("OP", length(gt2.new))))
colnames(gt2.orig.df) <- colnames(gt2.new.df) <- c("coefficient", "categories")
gt2.df <- rbind(gt2.orig.df, gt2.new.df)
gt2.df$coefficient <- as.numeric(as.character(gt2.df$coefficient))

```


```{r Density-Plot-Placebo-Treatment, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Placebo Treatment Coefficients by Education Set. Overlapping Area Shown in Dark Purple.\\label{DensPlacTreat}"}

ggplot(gt2.df, aes(x=coefficient, fill=categories)) + geom_density(alpha=0.4, aes(y=..density..), position="identity") + xlab("Regression Coefficients for Placebo Treatment Group") + ylab("Density") + theme(legend.title=element_blank()) + theme(legend.position = c(0.85, 0.75)) + theme(plot.title = element_text(hjust = 0.5))+ scale_fill_manual(values=c("red", "blue"))

```


Both distributions center around zero, as is the statistical expectation. Upon closer inspection, however, the ordered probit categories are closer to the true value of zero than the ANES categories on both mean (`r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, mean)[2], digits = 3))` v. `r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, mean)[1], digits = 3))`) and median (`r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, median)[2], digits = 3))` v. `r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, median)[1], digits = 3))`). This indicates slightly superior performance by the ordered probit categories, as they more closely approach the true value when used in a regression.





## Conclusion {#ordblock-conclusion}

I set out to improve the use of ordinal variables to block respondents into treatment groups in survey experiments. Survey experiments depend on balance of covariates between treatment groups to allow the estimation of causal effects. Randomization ensures balance for large samples but becomes problematic for small samples. Blocking greatly alleviates this problem. 

Blocking naturally depends on covariates. One of the most important covariates in political science is education. Out of convenience, education is often converted to a numerical variable for regressions in practice. Due to the special nature of education as an ordinal variable, such an approach is potentially problematic as ordinal variable categories are not evenly spaced. The distance between the education categories "Elementary School" and "Some High School" is very likely not the same as the distance between "Some High School" and "High School Graduate". Converting these three categories to the numerical values 1, 2, and 3, however, assumes evenly spaced distances. This could lead to a misrepresentation of the data.

As an alternative, I proposed an ordered probit approach whereby we estimate the latent underlying continuous variable underneath education. This estimates cutoff thresholds between the education categories, bins observations according to linear model predictors, and results in a new set of education categories that fit the data and represent the latent underlying continuous variable with its unevenly spaced distances. I applied this approach to the 2016 ANES data, resulting in two sets of education categories: the original ANES categories, and the newly estimated OP categories. I subsequently blocked both sets into three treatment groups and analyzed the group means and variances. While the numerical variables do not show statistically distinct intercepts, most of the factor variables do. This indicates that the re-estimation of education categories is potentially meaningful. I also ran a placebo regression 1,000 times to conduct a falsification test. This shows distributions of the placebo treatment variable around the statistically expected zero but also reveals the OP variable to be closer to zero than the ANES variable for both mean and median. This indicates slightly superior performance by the OP method and thus also provides tentative evidence that the re-estimation of education categories could be meaningful.

The most obvious reason for the negative results of the ANOVA regressions for the numerical variables are the large number of distinct numerical values in each of those variables. This aspect separates these variables, which don't show significant intercepts, from the factor variables, which are highly significant. It appears that blocking on re-estimated ordinal categories does not have a meaningful impact for numerical variables with a high count of unique values. The intercepts for the numerical variables seem to be subject to less variance difference than the intercepts for the factor variables. One possible explanation for this could be that differences between variable values in the treatment groups do not possess the same amount of influence on variance estimation when the variable in question is finely grated, as is the case with these numerical variables. Moving from one numerical value to the next does not influence variance profoundly, whereas moving from one factor variable category to another represents a much larger 'step'.

The next chapter will use the OP approach to address multiple imputation with ordinal variables.





