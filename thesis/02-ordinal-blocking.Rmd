# PRECISION IN SURVEY EXPERIMENTS -- A NEW METHOD TO IMPROVE BLOCKING ON ORDINAL VARIABLES {#ordblock}

## Introduction {#ordblock-intro}

```{r include=FALSE}
drop.zero <- function (df, digits = 4, p, s, pad.char = NA, ...){
    library(dplyr)
    ldots <- list(...)
    if (length(ldots) > 0) {
        if (!is.null(ldots[["prefix"]]) & missing(p)) 
            p <- ldots[["prefix"]]
        if (!is.null(ldots[["suffix"]]) & missing(s)) 
            s <- ldots[["suffix"]]
    }
    x <- dplyr::select_if(df, is.numeric)
    for(i in 1:ncol(x)){
      x[,i] <- round(as.numeric(x[,i]), digits)
      x[,i] <- sprintf(paste0("%.", digits, "f"), x[,i])
      x[,i] <- gsub("^0(?=\\.)|(?<=-)0", "", x[,i], perl = TRUE)
    }
    df[, colnames(x)] <- x
    return(df)
}

drop.zero.num <- function(num, digits = 3){
  if(!is.numeric(num)){
    num <- as.numeric(num)
  }
  num <- round(num, digits)
  num <- sprintf(paste0("%.", digits, "f"), num)
  num <- gsub("^0(?=\\.)|(?<=-)0", "", num, perl = TRUE)
  return(num)
}

```


Survey experiments collect background information and attempt to uncover treatment effects on public opinion and political behavior. In order to identify such potential effects, the treatment groups need to be comparable. All treatment groups need to look the same in every measure, i.e. they must be balanced. This can be achieved through random assignment of participants to treatment groups. Randomization, i.e. flipping a coin to decide which treatment group a participant is assigned to, probabilistically results in balance based on the Law of Large Numbers [@urdan_statistics_2010]. For small samples, however, it can lead to serious imbalance. It can easily be that the treatment groups will not look the same. This can leave experimental results in statistically murky waters [@imai_quantitative_2018;@king_designing_1994;@fox_applied_2015]. In survey experiments, the overall sample size is often split across several treatment groups, which can exacerbate the problem. @chong_framing_2007, for instance, split 869 participants in a framing experiment on urban growth over 17 treatment groups, which leads to an average of just over 50 participants per group. Randomization is unlikely to lead to balanced treatment groups of this size. Researchers need to employ statistical methods to obtain balanced groups here. Blocking, i.e. arranging participants in groups that are equal in terms of participants' covariates and using random allocation within these groups, can alleviate such worries. 

Blocking depends on covariates. In political science, many covariates with high predictive power are categorical variables, i.e. variables where the data can be divided into groups. These include interval (ordered and evenly spaced, e.g. income) and ordinal (ordered and unevenly spaced, e.g. education) variables. Ordinal variables are ordered categorical variables where the spacing between the values is not the same.

To block, these variables are often made numerical, e.g. by assigning the numbers 1-4 to the variable categories. This is acceptable for interval variables as the evenly spaced numbers correspond to the evenly spaced categories. For ordinal variables, however, this can be problematic. Take for instance the example of education: Each subsequent category has quantitatively more education than the previous, but the exact measure of the distance between the categories is unclear. An arbitrary evenly spaced string of numbers does not correspond to these unevenly spaced ordinal categories and may misrepresent the data. Do evenly spaced numbers really represent the distances between the categories? Perhaps the true spacing between some of the categories is so narrow they should not even be separate categories at all. 

I propose an ordered probit threshold approach to circumvent this problem: This approach estimates an assumed underlying latent continuous structure underneath ordinal variables whose data-driven categories can then be used for blocking. The following sections provide a background on survey experiments and blocking, describe the key aspects of ordinal variables, and outline my proposed ordered probit approach. I then demonstrate the effect of this approach with external survey data and a placebo regression.


## Theory {#ordblock-theory}

### Preliminary Notations on Survey Experiments {#ordblock-theory-experiments}

The simplest of survey experiments has two potential outcomes for participants $i$, $y_{1i}$ and $y_{0i}$, with 1 denoting the treatment and 0 referring to the control. Consider a simplified version of a famous survey experiment by @tversky_framing_1981, where researchers want to test the effect of the mortality format on participants' choices. They provide participants with the following scenario:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent Imagine that the US is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. A program to combat the disease has been proposed. Assume that the exact scientific estimates of the consequences of the program are as follows...
\end{adjustwidth}

Participants in the control group receive the program description in survival format:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent If the program is adopted, 200 out of 600 people will live.
\end{adjustwidth}

Participants in the treatment group receive the program description in mortality format:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent If the program is adopted, 400 out of 600 people will die.
\end{adjustwidth}

@tversky_framing_1981 use this experiment to demonstrate the importance of framing. Support for the program is much higher among respondents who received the survival format, even though the success rate of the program is identical in both formats. Framing the program in a positive light thus dramatically increases support by connecting the program to people's aversion of death and affirmation of life. While these findings stem from an experiment conducted in the 1970s, it is not a big leap to imagine a similar outcome in today's Covid-19 world. Misinformation abounds in all formats across countless media channels and there is little reason to assume human behavior has drastically changed. If we conducted this experiment today, it would be quite possible to once more find sizable differences between these two groups.

After being shown one of the two formats, all participants are asked whether they support or oppose the program. The treatment effect for each individual participant $i$ is given by $y_{1i} - y_{0i}$. If both groups of participants look the same regarding their covariates (age, education, income etc.), a comparison of the groups' average support reveals the Average Treatment Effect (ATE) across all participants, $\mathbf{E}[\delta] = \mathbf{E}[y_{1i} - y_{0i}]$. A central characteristic of such a comparison is the fundamental problem of causal inference [@holland_1986_statistics;@rubin_1974_estimating]: We are unable to observe both potential outcomes for the same participant at once. In our case, we cannot observe how much participant A supports the program if given the survival format whilst also observing how much the same participant A would have supported the program if given the mortality format. If we could, it would be simple to calculate the true average treatment effect, $\mathbf{E}[\delta] = \mathbf{E}[y_{1i}|T=1] - \mathbf{E}[y_{0i}|T=0]$, with $T=0$ denoting the control and $T=1$ the treatment group. Since the true average treatment effect is unobservable, we need to use statistical means to assess the counterfactuals. This can be done by balancing the treatment and control groups. If both groups of participants look the same in every measure, we can use the participants who received the mortality format (treatment) to estimate what would have happened to the participants who did not receive any format (control). The crucial aspect is whether the two groups do indeed look the same in terms of participants' covariates. The potential outcome of the control group needs to mirror what would have happened in the case of treatment, and vice versa. This was the case in @tversky_framing_1981's study, which is why they were able to accurately detect causal differences between the format effects. There are two main means by which this comparability may be achieved: randomization and blocking.


### Randomization {#ordblock-theory-randomization}

Randomization is equivalent to flipping a coin for each participant to be assigned to treatment or control. This chance procedure gives each participant an equal chance of being assigned to either group (or groups, in case of multiple treatment groups) [@lachin_1988_properties]. Randomization increases covariate balance as the number of participants, $n$, increases [@imai_2009_essential]. The larger a researcher's sample, the better the resulting balance from randomization in expectation. Probabilistically, randomization enables the comparison of the average treatment effect to be unbiased, which allows the researcher to attribute any treatment effects to the treatment [@king_a-politically_2007]. 

While randomization thus guarantees balance as the sample size reaches infinity, it often does not do so in the naturally finite sample sizes researchers actually work with. With huge samples, the Law of Large Numbers predicts that treatment groups selected through randomization will be balanced. With small samples, however, it is possible to get unlucky and end up with unbalanced groups [@imai_2008_misunderstandings]. Blocking can help achieve balance in such scenarios [@epstein_2002_rules].


### Blocking {#ordblock-theory-blocking}

Identical levels in terms of covariates across treatment groups represent the key aspect in experimental studies. In randomization, this is achieved by random chance. In blocking, this is achieved by combining covariate information about the participants with randomization. Specifically, participants are blocked into treatment groups that are similar to one another in terms of the their covariates before treatment is assigned. Their similarity is estimated with the Mahalanobis or Euclidian distance. The Mahalanobis distance (MD) is a multivariate distance metric which measures the distance between two vectors (or between a point and a distribution). For two random vectors $x$ with $x_i = [1,\ldots,n]$ and $y$ with $y_i = [1,\ldots,n]$ of the same distribution, the MD is defined as

\begin{align}
MD_{xy} = \sqrt{(\bm{x}_i - \bm{y}_i)' S^{-1} (\bm{x}_i - \bm{y}_i)},
\end{align}

where $S$ denotes the covariance matrix. If the covariance matrix is a diagonal identity matrix, the resulting distance measure becommes the Euclidian distance (ED), 

\begin{align}
ED_{xy} = \sqrt{\sum_{i=1}^N \frac{(\bm{x}_i - \bm{y}_i)^2}{s_i^2}},
\end{align}

with $s_i$ denoting the standard deviation of $x_i$ and $y_i$. The MD accounts for covariances, whereas the ED assumes equal variances and zero covariances. The ED thus can be argued to represent a special case of the MD.

Blocking is better suited to achieving balance in finite samples than randomization, as it "directly controls the estimation error due to differing levels of observed covariates in the treatment and control groups" [@moore_2012_multivariate, p. 463]. This is particularly relevant with small samples and a high number of treatment groups, as the overall number of participants needs to be divided up. Figures \ref{BoxLawLarNum} and \ref{HistLawLarNum} show this visually. A numerical discrete variable with levels 1 to 5 is randomized and blocked for different sample sizes and different numbers of treatment groups. This is repeated 100 times for each sample size. Figure \ref{BoxLawLarNum} shows the maximum distances between treatment groups across these repetitions for sample sizes up to 1,000 for 2, 3, 5, and 10 treatment groups. Blocking outperforms randomization in every scenario. The difference between the two methods is smallest for large samples and a small number of treatment groups. 

```{r Law of Large Numbers Simulations, eval=FALSE, include=FALSE}

### THE CODE THAT CREATES THE SIMULATIONS FOR THE BLOCKING PLOTS IS IN scripts/lln//lln_testing.Rmd. IT TAKES 3-4 DAYS TO RUN THIS CODE, SO THERE IS NO POINT HAVING IT HERE ###

```

```{r Law of Large Numbers Plotting Code Boxplots, include=FALSE}

### I LOAD THE CREATED SIMULATIONS HERE TO CREATE THE PLOTS ###

all_blocked_2 <- read.csv("data/blocking/all_blocked_2.csv")
all_blocked_3 <- read.csv("data/blocking/all_blocked_3.csv")
all_blocked_5 <- read.csv("data/blocking/all_blocked_5.csv")
all_blocked_10 <- read.csv("data/blocking/all_blocked_10.csv")
all_means_variances_2 <- read.csv("data/blocking/all_means_variances_2.csv")
all_means_variances_3 <- read.csv("data/blocking/all_means_variances_3.csv")
all_means_variances_5 <- read.csv("data/blocking/all_means_variances_5.csv")
all_means_variances_10 <- read.csv("data/blocking/all_means_variances_10.csv")

# There are 100 NAs each in "all_means_variances_3", "all_means_variances_5", and "all_means_variances_10"
# They are always for the first respective sampled number: 9, 15, 30
# They're in control for _3 and _5, and in treatment5 for _10
# I don't know why those are happening, but I'm not starting with the first sampled numbers anyway, and it doesn't make any difference for the overall simulations, so I am removing those
all_means_variances_3 <- subset(all_means_variances_3, subset = (sampled_numbers != 9))
all_means_variances_5 <- subset(all_means_variances_5, subset = (sampled_numbers != 15))
all_means_variances_10 <- subset(all_means_variances_10, subset = (sampled_numbers != 30))

# I want the plot legend to read "randomized" instead of "rand"
all_means_variances_2$label <- fct_recode(all_means_variances_2$label, "randomized" = "rand")
all_means_variances_3$label <- fct_recode(all_means_variances_3$label, "randomized" = "rand")
all_means_variances_5$label <- fct_recode(all_means_variances_5$label, "randomized" = "rand")
all_means_variances_10$label <- fct_recode(all_means_variances_10$label, "randomized" = "rand")

together <- list(all_blocked_2, all_means_variances_2, all_blocked_3, all_means_variances_3, all_blocked_5, all_means_variances_5, all_blocked_10, all_means_variances_10) # collect all dfs in a list to loop over
couple <- list() # empty list

for(i in 1:(length(together))){
   couple[[i]] <- subset(together[[i]], select = c(sampled_numbers, diff, label))
   couple[[i]]$sampled_numbers <- as.factor(couple[[i]]$sampled_numbers)
} # subset for 3 columns and turn sampled_numbers into factor

sims_2 <- rbind(couple[[1]],couple[[2]]) # combine blocked and rand for each # of treatment groups
sims_3 <- rbind(couple[[3]],couple[[4]])
sims_5 <- rbind(couple[[5]],couple[[6]])
sims_10 <- rbind(couple[[7]],couple[[8]])

selection <- c(0, 0.1, 0.2, 0.3, 0.5, 1) # the quantiles I want

quantile(as.numeric(levels(sims_2$sampled_numbers)), selection) # quantiles for 2 groups
levels(sims_2$sampled_numbers) # levels for 2 groups
sims_2_range <- subset(sims_2, subset = sampled_numbers %in% c(14, 102, 206, 302, 502, 998)) # hand-select samples for range
sims_2_one <- subset(sims_2, subset = sampled_numbers == as.numeric(levels(sims_2$sampled_numbers)[2])) # select second level for 'intro' plot

quantile(as.numeric(levels(sims_3$sampled_numbers)), selection)
levels(sims_3$sampled_numbers)
sims_3_range <- subset(sims_3, subset = sampled_numbers %in% c(18, 108, 207, 306, 504, 999))
sims_3_one <- subset(sims_3, subset = sampled_numbers == as.numeric(levels(sims_3$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_5$sampled_numbers)), selection)
levels(sims_5$sampled_numbers)
sims_5_range <- subset(sims_5, subset = sampled_numbers %in% c(25, 115, 215, 305, 505, 995))
sims_5_one <- subset(sims_5, subset = sampled_numbers == as.numeric(levels(sims_5$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_10$sampled_numbers)), selection)
levels(sims_10$sampled_numbers)
sims_10_range <- subset(sims_10, subset = sampled_numbers %in% c(40, 130, 220, 300, 500, 1000))
sims_10_one <- subset(sims_10, subset = sampled_numbers == as.numeric(levels(sims_10$sampled_numbers)[2]))

xlab <- "Sample Sizes"
ylab <- "Max. Distances Between Treatment Groups"

plot_first <- ggplot(sims_2_one, aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.15, 0.65)) # first plot outside of the loop because of the legend

sims_plots <- list(sims_2_range, sims_3_one, sims_3_range, sims_5_one, sims_5_range, sims_10_one, sims_10_range) # list of all subsets for plotting
plots <- list()
for(i in 1:(length(sims_plots))){
   plots[[i]]  <- ggplot(sims_plots[[i]], aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + guides(fill=FALSE)
  } # create plot for each data subset

```


```{r Boxplot-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distances Between Treatment Group Means in Randomized and Blocked Data. Increasing Sample Size for 2 (Top Row), 3 (Second Row), 5 (Third Row), and 10 Treatment Groups (Bottom Row). Leftmost Pair on the Right Panel Is the Same as the Pair on the Left Panel\\label{BoxLawLarNum}"}

grid.arrange(plot_first, plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]], plots[[7]], ncol = 2, nrow = 4, bottom=xlab, left=ylab)

```


For $n = 998$ and 2 treatment groups, the largest distance between randomized treatment groups is `r round(max(all_means_variances_2$diff[all_means_variances_2$sampled_numbers == 998]), digits=3) %>% drop.zero.num`, while the largest distance between blocked treatment groups is `r round(max(all_blocked_2$diff[all_blocked_2$sampled_numbers == 998]), digits=3) %>% drop.zero.num`. For small samples and a large number of treatment groups, the difference is even starker. For $n = 40$ and 10 treatment groups, the largest distance between randomized treatment groups is `r round(max(all_means_variances_10$diff[all_means_variances_10$sampled_numbers == 40]), digits=3)`, while the largest distance between blocked treatment groups is `r round(max(all_blocked_10$diff[all_blocked_10$sampled_numbers == 40]), digits=3)`. 

Figure \ref{HistLawLarNum} shows the count distributions of these imbalances. For 2 treatment groups (top left plot), almost all blocked treatment groups have a maximum distance of zero. Most randomized groups also have a maximum distance of zero, but it is a narrow majority. Almost 50 percent of randomized groups have distances greater than zero, though still lower than one. As the number of treatment groups increases, so do the distances between the treatment groups. Nonetheless, the vast majority of blocked groups still show a maximum distance of zero. Even for 10 treatment groups, more than 60 percent of the blocked groups are not distant from each other at all. This does not hold true for the randomized groups. For 3 treatment groups, the majority of distances are now above zero. For 5 groups, the majority of distances exceed .25. For 10 treatment groups, more than 60 percent of groups show a distance larger than .5.


```{r Law of Large Numbers Plotting Code Histograms, include=FALSE}

ylab <- "Count"
xlab <- "Max. Distances Between Treatment Groups"

plot_first_more <- ggplot(sims_2, aes(x=diff, fill=label)) + geom_histogram(alpha=0.4, aes(y=..count..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.6, 0.75)) + scale_fill_manual(values=c("red", "blue")) # first plot outside of the loop because of the legend

sims_plots_more <- list(sims_3, sims_5, sims_10) # list of all data sets (minus for 2 groups for plotting)

plots_more <- list()
for(i in 1:(length(sims_plots_more))){
   plots_more[[i]] <- ggplot(sims_plots_more[[i]], aes(x=diff, fill=label)) + geom_histogram(alpha=0.4, aes(y=..count..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + guides(fill=FALSE) + scale_fill_manual(values=c("red", "blue")) # create plot for each data set
}
```

```{r Hist-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=4, fig.cap="Distribution of Treatment Group Differences in Randomized and Blocked Data for 2 (Top Left), 3 (Top Right), 5 (Bottom Left), and 10 Treatment Groups (Bottom Right). Overlapping Area Shown in Dark Purple.\\label{HistLawLarNum}"}

grid.arrange(plot_first_more, plots_more[[1]], plots_more[[2]], plots_more[[3]], ncol = 2, nrow = 2, bottom = xlab, left = ylab)

```


#### Blocking On The Go {#ordblock-theory-blocking-onthego}

In political science, researchers often have an already-collected data set in front of them. One example are the American National Election Studies (ANES). The ANES are the oldest continuous collection of national surveys on electoral behavior and attitudes in the US. They are conducted before and after every US presidential and Congressional election by the University of Michigan and Stanford University. Data have been collected since 1948 in the attempt to understand voter behavior and candidate choice, among many others. The list of questions has been continually expanded and refined over the years. All ANES data are publicly available. The ANES are frequently used for high-profile publications [see for instance @jackman_2018_does; @leighley_who_2014]. Working with such a 'ready-made' data set means all covariate information of all participants is known at the time of assignment, which makes blocking straight-forward. Oftentimes, however, the covariate information of all participants is not known at the time of assignment. This is the case, for instance, for online survey experiments, where participants complete the survey at differing times. Participants 'trickle in' for treatment assignment as the experiment progresses. 'Traditional' blocking can not be used here, since it relies on covariate information about the entire sample, which is not available. Instead, we need to block continuously as the experiment progresses, or block 'on the go'. This is called sequential blocking. 

Sequential blocking in political science is based on covariate-adaptive randomization, which varies probabilities based on knowledge about previous participants and the current participant [@chow_2007_adaptive]. Traditional covariate-adaptive approaches, such as the biased coin design [@efron_1971_forcing] and minimization [@pocock_1975_sequential], assign the incoming participant to the treatment group with the fewest participants with identical covariate information. For discrete covariates, for instance, this takes the form of assigning all participants except the first one, $q$, for covariate $c$ with value $d$ to a treatment group $g$ with probability

\begin{align}
\text{prob}(g* = g) = (1 - \frac{q_{cdg}}{\sum^G_{g = 1} q_{cdg}}) (\sum_{g = 1}^G(1 - \frac{q_{cdg}}{\sum^G_{g = 1} q_{cdg}}))^{-1}.
\end{align}

This works for discrete covariates as the number of possible covariate levels is finite. For continuous covariates, the number of possible covariate levels rises exponentially. Participants are unlikely to look the same, and identical participants are rare. Blocking on continuous covariates is not possible with these traditional approaches [@markaryan_2010_exact;@rosenberger_2002_randomization;@eisele_1995_biased]. @moore_blocking_2013 develop a method to do so by exploiting relationships between the current participant's covariate profile and those of all previously assigned participants. They define the similarity between participants with the Mahalanobis distance between participants $q$ and $r$ with covariate vectors $\bm{x}_q$ and $\bm{x}_r$, 

\begin{align}
MD_{qr} = \sqrt{(\bm{x}_q - \bm{x}_r)' S^{-1} (\bm{x}_q - \bm{x}_r)}.
\end{align}

Recall that $S$ denotes the covariance matrix. To aggregate pairwise similarity, they implement the mean, median, and trimmed mean of the pairwise MDs between the current participant and the participants in each treatment condition: Participants are indexed with treatment condition $t$ using $r \in \{1,...,R\}$. For each condition $t$, an average MD between the current participant, $q$, and the participants previously assigned, $t$. If the distance in terms of MD for the incoming participant is 2 in the control and 5 in the treatment condition, the incoming participant looks more similar to the control condition. To set the probability of assignment, @moore_blocking_2013 calculate the mean MDs for each incoming participant, $q$, for all treatment conditions, $t$, and sort the treatment conditions by these averages. Randomization is biased towards conditions with high scores. For each value of $k$, with $k \in \{2,3,...,6\}$, the condition with the highest average MD is then assigned a probability $k$ times larger than all other assignment probabilities. 

Blocking is thus possible when all covariate information is known at the time of assignment and when this information 'trickles in' over time. Covariate information, however, is only one side of the coin. Researchers also need to take into consideration the characteristics of the variable to block on. Not all types of variables can and should be used the same way to be blocked on. Specifically, the current use of ordinal variables as blocking variables is somewhat problematic.



### Ordinal Variables {#ordblock-theory-ordinal}

Ordinal variables matter in surveys. One of the most important ordinal variables in political science surveys is education. It is widely established that education represents one of the major driving forces behind public opinion and political behavior, such as turnout or donations, in the U.S. [@dawood_campaign_2015; @fiorina_disconnect_2009; @leighley_who_2014; @abramowitz_disappearing_2010; @druckman_how_2013; @fiorina_culture_2011; @king_polarization_1997]. Ordinal variables are part of the larger framework of categorical variables. Categorical variables represent types of data which are commonly divided into three groups: nominal, interval, and ordinal variables. Nominal variables are categorical variables with two or more categories that are not intrinsically ordered. Examples include gender (Female, Male, Transgender etc.), race (African-American, White, Hispanic etc.), and party ID (Democrat, Republican, Independent) where the categories cannot be ordered sensibly into highest or lowest. Interval variables are ordered categorical variables with evenly spaced values. Examples include income (\$20,000, \$40,000, \$60,000, \$80,000 etc.), where the distance between \$20,000 and \$40,000 is the same as the distance between \$60,000 and \$80,000. Ordinal variables are ordered categorical variables where the spacing between values is not the same. Examples include education (Elementary School, Some High School, High School Graduate etc.) where the distance between "Elementary School" and "Some High School" is likely different than the distance between "High School Graduate" and "Some College". Each subsequent category has quantitatively more education than the previous one, but the exact measure of the distances between the categories is unclear.  

For statistical analysis, the categories of nominal variables are often turned into binary variables. This manipulation does not impose any unnatural ordering onto the variable and thus does not require any theoretical assumptions. Interval variables are often made numerical, which is statistically sound. It makes sense to assign numerical values such as 1, 2, 3, and 4 to income categories of \$20,000, \$40,000, \$60,000, and \$80,000 as the distance between each of these categories is identical between any adjacent pair. This translates perfectly into the numerical values with identical distances, i.e. the distance between \$20,000 and \$40,000 is the same as the distance between 1 and 2. Ordinal variables are also often made numerical for analytic purposes. This is problematic because of their unevenly spaced categories. If the education categories "Elementary School", "Some High School", and "High School Graduate" were turned into the numerical values 1, 2, and 3, we would wrongly assume that the distances between the education categories correspond to these evenly spaced values. Do the numbers 1 to 3 really represent the distances between these categories? Perhaps the true spacing between some of the categories is so narrow they should not even be separate categories at all. We cannot answer this by making an arbitrary assumption that is not justified by the data. Alternatively, if "Elementary School", "Some High School", and "High School Graduate" were turned into three separate dummy variables, we would wrongly assume that there is no ordering to these values. In both cases, important information would be lost, which could lead to a large degree of distortion [@obrien_1981_using]. To truly use the ordinal nature of a variable, we need to use both its quantitative and its inherent unevenly spaced ordered aspects to make a more underlying description of the data possible [@agresti_2010_analysis]. To fill this gap, I borrow from machine learning, which has close connections to problems of causal inference [@grimmer_2015_social], and propose an ordered probit model that estimates an ordinal variable's underlying latent continuous structure and is trained on external data.



### Ordered Probit Approach {#ordblock-theory-op}

Many approaches in the literature on the analysis of ordinal variables incorporate the distribution of the variable categories [@agresti_1996_introduction]. The most promising suggestions focus on natural extensions of probit and logit models [@winship_1984_regression] by assigning scores to be estimated from the data [@agresti_1990_categorical] and quantifying each non-quantitative variable according to the empirical distributions of the variable, assuming the presence of a continuous underlying variable for each ordinal indicator [@lucadamoa_2014_scaling]. In fact, @agresti_2010_analysis states "that the type of ordinal method used is not that crucial" but that the "results may be quite different, however, from those obtained using methods that treat all the variables as nominal" (p. 3). The same applies to methods which treat ordinal variables as interval [@gertheiss_2008_penalized]. This suggests that a probit or logit model is suitable to uncover the latent continuous variable underlying an ordinal variable, thereby using the ordinal information provided and respecting uneven distances. In the literature, this approach is focused exclusively on the analysis of ordinal variables as a response variable. I propose an ordered probit model that applies to ordinal variables as predictors.

Let there be $\bm{X}$, an $n \times k$ matrix of explanatory variables. Let further $\bm{Y}$ be observed on the ordered categories $\bm{Y}_i \in [1,\ldots,k]$, for $i=1,\ldots n$, and let $\bm{Y}$ be assumed to be produced by the unobserved latent continuous variable $\bm{Y^{cont}}$. $\bm{Y^{cont}}$ is continuous on $R$ from $-\infty$ to $\infty$. The 'response mechanism' for the $r^{th}$ category is $Y=r \Longleftrightarrow \xi_{r-1} < Y^{cont} < \xi_r$. This requires there to be thresholds on $R$:
$Y^{cont}_i: \; \xi_0 \underset{a=1}{\longleftarrow\!\longrightarrow} \xi_1 \underset{a=2}{\longleftarrow\!\longrightarrow} \xi_2 \underset{a=3}{\longleftarrow\!\longrightarrow} \xi_3\ldots \xi_{A-1} \underset{a=A}{\longleftarrow\!\longrightarrow} \xi_A$. The vector of (unseen) utilities across individuals in the sample, $Y^{cont}$, is determined by a linear model of explanatory variables: $\bm{Y^{cont}} = \bm{X} \bm{\beta} + \mu$, where $\bm{\beta} =[\beta_1,\beta_2,\ldots,\beta_p]$ does not depend on the $\xi_j$ and $\mu \sim F_{\mu}$. For the observed vector $\bm{Y}$,

\begin{align}
p(\bm{Y} \leq r|\bm{X}) &= p(\bm{Y^{cont}} \leq \xi_r) = p(\bm{X}\bm{\beta} + \mu \leq \xi_r) \nonumber\\
&= p(\mu \leq \xi_r+\bm{X}\bm{\beta}) = F_{\mu}(\xi_r + \bm{X}\bm{\beta})
\end{align}
            
is called the cumulative model because $p(\bm{Y} \leq \xi_r|\bm{X}) = p(\bm{Y}=1|\bm{X}) + p(\bm{Y}=2|\bm{X}) + \ldots + p(\bm{Y}=r|\bm{X})$. A logistic distributional assumption on the errors produces the ordered logit specification

\begin{align}
F_{\mu}(\xi_r - \bm{X}'\bm{\beta}) = P(\bm{Y} \leq r|\bm{X}) = [1+\exp(-\xi_r-\bm{X}'\bm{\beta})]^{-1}. 
\end{align}

The likelihood function is 

\begin{align}
L(\bm{\beta},\bm{\xi}|\bm{X},\bm{Y}) = \prod_{i=1}^{n}\prod_{j=1}^{A-1}\left[\Lambda(\xi_j + \bm{X}_i'\bm{\beta}) - \Lambda(\xi_{j-1} + \bm{X}_i'\bm{\beta}) \right]^{z_{ij}}
\end{align}

where $z_{ij}=1$ if the $i^\text{th}$ case is in the $j^\text{th}$ category, and $z_{ij}=0$ otherwise. The thresholds on $R$ partition the variable into regions corresponding to the ordinal categories. The linear model, $Y^{cont}$, bins the observations between these thresholds according to the linear predictors. 

Practically, we need to estimate a linear combination of meaningful covariates as predictors and an ordinal variable as the dependent variable. We then train this model on externally and internally valid data. This estimates cutoff thresholds between the ordinal categories and bins data cases according to the linear predictors. The binned cases determine which variable categories make sense, given the underlying latent continuous variable. We then replace the original categories with these re-estimated categories and conduct the statistical analysis of interest. In `R`, the ordered probit model can be implemented with the `polr` function from the `MASS` package [@ripley_2020_package].






## Data {#ordblock-data}

I apply the proposed ordered probit method to the 2016 American National Election Studies and train a specified regression model on these data. This estimates the thresholds between each existing ANES education category. All observations are then binned according to the estimated coefficients to determine the education categories that make sense, based on the underlying latent continuous variable. This results in two sets of education categories: ANES and ordered probit (OP). The ANES data are then blocked into five treatment groups to simulate a survey experiment environment. This process is repeated twice: once with the ANES categories, and once with the OP categories. To simulate sequential blocking, the order of observations is assumed to represent the sequential order of arrival to treatment. The estimation of group means and variances together with statistical tests reveals the suitability of this approach. Finally, I block the ANES data once more and conduct a placebo regression to test model fitness further.




## Results {#ordblock-results}

Recall that we need to estimate a linear combination of meaningful covariates as predictors and an ordinal variable as the dependent variable. This model needs to be trained on externally and internally valid data. The ANES data have been shown to be externally and internally valid in countless publications. I employ the following model with these data, using standard demographics in political science as predictors: 

\begin{align}
\text{Education} \sim \text{Gender} + \text{Race} + \text{Age} + \text{Income} + \text{Occupation} + \text{Party ID}
\end{align}

We then train this model on the ANES data. This estimates cutoff thresholds between the OP categories and bins data cases according to the linear predictors. When trained on these data, this ordinal probit model estimates the thresholds between each of the education categories shown in Table \ref{education-categories}.


```{r Education Thresholds Table, include=FALSE}
op.model.thresholds <- read.csv("data/blocking/thresholds.csv") %>% drop.zero
colnames(op.model.thresholds) <- c("Thresholds", "Coefficients", "Standard Errors", "t-values")
stargazer(op.model.thresholds, 
          summary = FALSE, 
          rownames = FALSE, 
          header=FALSE, 
          align = TRUE, 
          title = "Ordered Probit Threshold Estimates", 
          label = "education-categories")
```

\begin{table}[!htbp] \centering 
  \caption{Ordered Probit Threshold Estimates} 
  \label{education-categories} 
\begin{tabular}{r@{}lr@{}lr@{}lr@{}l} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{2}{c}{Thresholds} & \multicolumn{2}{c}{Coefficients} & \multicolumn{2}{c}{Standard Errors} & \multicolumn{2}{c}{t-values} \\ 
\hline \\[-1.8ex] 
Up to 1st $\mid$ & \,1st-4th & -7.&869 & 1.&024 & -7.&681 \\ 
1st-4th $\mid$ & \,5th-6th & -7.&146 & .&717 & -9.&965 \\ 
5th-6th $\mid$ & \,7th-8th & -5.&379 & .&326 & -16.&515 \\ 
7th-8th $\mid$ & \,9th & -4.&671 & .&253 & -18.&472 \\ 
9th $\mid$ & \,10th & -3.&920 & .&206 & -19.&070 \\ 
10th $\mid$ & \,11th & -3.&468 & .&188 & -18.&489 \\ 
11th $\mid$ & \,12th & -2.&984 & .&174 & -17.&100 \\ 
12th $\mid$ & \,HS grad & -2.&511 & .&166 & -15.&116 \\ 
HS grad $\mid$ & \,Some college & -.&711 & .&154 & -4.&607 \\ 
Some college $\mid$ & \,Associate & .&384 & .&154 & 2.&500 \\ 
Associate $\mid$ & \,Bachelor's & 1.&045 & .&154 & 6.&766 \\ 
Bachelor's $\mid$ & \,Master's & 2.&478 & .&160 & 15.&538 \\ 
Master's $\mid$ & \,Professional & 4.&099 & .&177 & 23.&144 \\ 
Professional $\mid$ & \,Doctorate & 4.&838 & .&197 & 24.&589 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}


The observations in the data are binned according to the estimated threshold coefficients, which in turn determines what education categories make sense, given the underlying latent continuous variable. This results in two sets of education categories: the original ANES categories, and the model-estimated OP categories. Figure \ref{BarEducCat} shows the distribution of both sets. The ordered probit model uses the ordinal information with unevenly spaced distances provided and returns categories that fit the data. 


```{r Education Categories Plotting Code Histograms, include=FALSE}

op.model.data <- readRDS("data/anes/anes_education.rds")
plot.orig <- ggplot(op.model.data, aes(x = education)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = "darkred") + theme(axis.title=element_blank())
plot.new <- ggplot(op.model.data, aes(x = education.new)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = "darkblue") + theme(axis.title=element_blank())

```

```{r Barplot-Education-Categories, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Education Categories. Original ANES Categories on the Left, Ordered Probit Estimated Categories on the Right\\label{BarEducCat}"}

grid.arrange(plot.orig, plot.new, ncol = 2, nrow = 1, bottom = "Education", left = "Percentages")

```


As we can see, all categories 'below' "High School Graduate" and 'above' "Master's" are collapsed in the OP education variable because they do not fit the data. We can now use these estimated education categories as the basis for blocking. Assigning numerical values to the new categories is now justifiable because they are based on data-driven estimations. This allows us to block on numerical values with the MD, which would not be permitted on theoretical grounds without empirical justification. The following sections demonstrate how the newly estimated categories affect blocking and regression results. 




### Blocking Differences {#ordblock-data-blockdiff}

```{r Block-ANES-OP-categories, include=FALSE}

df.an <- readRDS("data/blocking/df_an.RDS")
df.op <- readRDS("data/blocking/df_op.RDS")

for(i in 1:(df.an$education %>% unique %>% length)){
  df.an[df.an$education == i, "education"] <- op.model.data$education %>% levels %>% .[i]
}

df.an$education <- factor(df.an$education, levels = op.model.data$education %>% levels)

for(i in 1:(df.op$education %>% unique %>% length)){
  df.op[df.op$education == i, "education"] <- op.model.data$education.new %>% levels %>% .[i]
}

df.op$education <- factor(df.op$education, levels = op.model.data$education.new %>% levels)
assigned.an <- readRDS("data/blocking/assigned_an.RDS")
assigned.op <- readRDS("data/blocking/assigned_op.RDS")

an.t1 <- df.an[df.an$id %in% assigned.an$assg[[1]][,1],] %>% subset(., select=-c(id, education))
an.t2 <- df.an[df.an$id %in% assigned.an$assg[[1]][,2],] %>% subset(., select=-c(id, education))
an.t3 <- df.an[df.an$id %in% assigned.an$assg[[1]][,3],] %>% subset(., select=-c(id, education))
an.t4 <- df.an[df.an$id %in% assigned.an$assg[[1]][,4],] %>% subset(., select=-c(id, education))
an.t5 <- df.an[df.an$id %in% assigned.an$assg[[1]][,5],] %>% subset(., select=-c(id, education))

op.t1 <- df.op[df.op$id %in% assigned.op$assg[[1]][,1],] %>% subset(., select=-c(id, education))
op.t2 <- df.op[df.op$id %in% assigned.op$assg[[1]][,2],] %>% subset(., select=-c(id, education))
op.t3 <- df.op[df.op$id %in% assigned.op$assg[[1]][,3],] %>% subset(., select=-c(id, education))
op.t4 <- df.op[df.op$id %in% assigned.op$assg[[1]][,4],] %>% subset(., select=-c(id, education))
op.t5 <- df.op[df.op$id %in% assigned.op$assg[[1]][,5],] %>% subset(., select=-c(id, education))


### props of factor columns, except education ###

list.props <- list()
list.dfs <- list(an.t1, an.t2, an.t3, an.t4, an.t5,
                 op.t1, op.t2, op.t3, op.t4, op.t5)
n.tr <- 5
an <- c()
op <- c()

for(w in 1:n.tr){
  an[w] <- paste0("ANES", w)
  op[w] <- paste0("OP", w)
}

names(list.dfs) <- c(an, op)
is.not.num <- function(x) !is.numeric(x)
tmp <- dplyr::select_if(an.t1, is.not.num)

for(x in 1:ncol(tmp)){
  list.props[[x]] <- data.frame(matrix(NA, levels(tmp[, x]) %>% length, length(list.dfs)))
  rownames(list.props[[x]]) <- levels(tmp[,x])
  colnames(list.props[[x]]) <- names(list.dfs)
}

names(list.props) <- colnames(tmp)

for(i in 1:ncol(tmp)){
  for(y in 1:length(list.dfs)){
    tmp.fac <- dplyr::select_if(list.dfs[[y]], is.not.num)
    list.props[[i]][,y] <- prop.table(table(tmp.fac[,i]))
  }
}

df.props <- plyr::ldply(list.props, data.frame, .id = NULL) 
lev.names <- sapply(tmp, levels) %>% unlist
lev.names["pid4"] <- "Something else"
rownames(df.props) <- lev.names
df.props <- drop.zero(df.props, digits = 3) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]


### means of numeric columns ###

list.means <- list()
tmp2 <- dplyr::select_if(op.t1, is.numeric)

for(x in 1:ncol(tmp2)){
  list.means[[x]] <- data.frame(matrix(NA, 1, length(list.dfs)))
  colnames(list.means[[x]]) <- names(list.dfs)
}

names(list.means) <- colnames(tmp2)

for(i in 1:ncol(tmp2)){
  for(y in 1:length(list.dfs)){
    tmp.means <- dplyr::select_if(list.dfs[[y]], is.numeric)
    list.means[[i]][,y] <- mean(tmp.means[,i]) %>% round(., digits = 3)
  }
}

df.means <- plyr::ldply(list.means, data.frame, .id = NULL) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]
rownames(df.means) <- colnames(tmp2) %>%
  gsub("\\.", " ", .) %>%
  tools::toTitleCase(.)


### now education ###

an.ed.t1 <- df.an[df.an$id %in% assigned.an$assg[[1]][,1],] %>% subset(., select=c(education))
an.ed.t2 <- df.an[df.an$id %in% assigned.an$assg[[1]][,2],] %>% subset(., select=c(education))
an.ed.t3 <- df.an[df.an$id %in% assigned.an$assg[[1]][,3],] %>% subset(., select=c(education))
an.ed.t4 <- df.an[df.an$id %in% assigned.an$assg[[1]][,4],] %>% subset(., select=c(education))
an.ed.t5 <- df.an[df.an$id %in% assigned.an$assg[[1]][,5],] %>% subset(., select=c(education))

list.eds.an <- list(an.ed.t1, an.ed.t2, an.ed.t3, an.ed.t4, an.ed.t5)

df.ed.an <- data.frame(matrix(NA, levels(an.ed.t1[,1]) %>% length, length(list.eds.an)))
rownames(df.ed.an) <- levels(an.ed.t1[,1])
colnames(df.ed.an) <- an

for(i in 1:ncol(an.ed.t1)){
  for(y in 1:length(list.eds.an)){
    df.ed.an[,y] <- prop.table(table(list.eds.an[[y]][,i]))
  }
}

df.ed.an <- drop.zero(df.ed.an, digits = 3)

op.ed.t1 <- df.op[df.op$id %in% assigned.op$assg[[1]][,1],] %>% subset(., select=c(education))
op.ed.t2 <- df.op[df.op$id %in% assigned.op$assg[[1]][,2],] %>% subset(., select=c(education))
op.ed.t3 <- df.op[df.op$id %in% assigned.op$assg[[1]][,3],] %>% subset(., select=c(education))
op.ed.t4 <- df.op[df.op$id %in% assigned.op$assg[[1]][,4],] %>% subset(., select=c(education))
op.ed.t5 <- df.op[df.op$id %in% assigned.op$assg[[1]][,5],] %>% subset(., select=c(education))

list.eds.op <- list(op.ed.t1, op.ed.t2, op.ed.t3, op.ed.t4, op.ed.t5)
df.ed.op <- data.frame(matrix(NA, levels(op.ed.t1[,1]) %>% length, length(list.eds.op)))
rownames(df.ed.op) <- levels(op.ed.t1[,1])
colnames(df.ed.op) <- op

for(i in 1:ncol(op.ed.t1)){
  for(y in 1:length(list.eds.op)){
    df.ed.op[,y] <- prop.table(table(list.eds.op[[y]][,i]))
  }
}

df.ed.op <- drop.zero(df.ed.op, digits = 3)
top <- data.frame(matrix(NA, 8, length(list.eds.op)))
colnames(top) <- colnames(df.ed.op)
rownames(top) <- rownames(df.ed.an)[1:8]
bottom <- data.frame(matrix(NA, 2, length(list.eds.op)))
colnames(bottom) <- colnames(top)
rownames(bottom) <- rownames(df.ed.an)[14:15]
df.ed.all <-rbind(top, df.ed.op, bottom) %>% cbind(df.ed.an, .) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]

### combine and print (and manually adjust) factors, numerics, education ###

df.all <- rbind(df.props, df.means) # I decided not to include ed.all because I don't want to have to explain the differing numbers (which might well be meaningless anyway)

rownames(df.all)[17] <- "Employed" # sounds better than "Working"

tab <- stargazer(df.all, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories. Differentiated by Treatment Group",
                 label = "education-blocked")
st <- gsub("{c}", "{l}", tab, fixed = TRUE)
cat(st)

```


Table \ref{education-blocked} shows the variable proportions or means, depending on the type of variable in question, after blocking the ANES data on education into five treatment groups. As outlined above, this is done twice, once based on the original ANES education categories, and once based on the newly estimated OP education categories.


\begin{table}[!htbp] \centering    
\caption{Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories. Differentiated by Treatment Group}
\label{education-blocked} 
\resizebox{11.5cm}{!}{%
\begin{tabular}{cc c @{\hspace{1.3cm}} c c @{\hspace{1.3cm}} c c @{\hspace{1.3cm}} c c @{\hspace{1.3cm}} c c } \\[-1.8ex]\hline  
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\hspace{-1cm}Group 1} & \multicolumn{2}{c}{\hspace{-1.3cm}Group 2} & \multicolumn{2}{c}{\hspace{-1.3cm}Group 3} & \multicolumn{2}{c}{\hspace{-1.3cm}Group 4} & \multicolumn{2}{c}{\hspace{-0.2cm}Group 5}\\ 
\hline \\[-1.8ex]  
 & ANES & OP & ANES & OP & ANES & OP & ANES & OP & ANES & OP \\  
\cline{2-3}
\cline{4-5}
\cline{6-7}
\cline{8-9}
\cline{10-11} \\[-1.8ex]
\textbf{Gender} & & & & & & & & & & \\  
Male & .467 & .468 & .473 & .481 & .475 & .463 & .463 & .468 & .476 & .473 \\  
Female & .533 & .529 & .524 & .514 & .522 & .535 & .533 & .532 & .521 & .524 \\  
Other & .000 & .003 & .003 & .005 & .003 & .002 & 
.003 & .000 & .003 & .003 \\  
 & & & & & & & & & & \\
\textbf{Race} & & & & & & & & & & \\  
White & .763 & .751 & .770 & .778 & .752 & .743 & .749 & .746 & .752 & .770 \\  
African American & .095 & .089 & .090 & .089 & .106 & .100 & .094 & .117 & .110 & .100 \\  
Asian & .032 & .035 & .041 & .027 & .027 & .035 & .027 & .032 & .030 & .029 \\  
Native American & .008 & .000 & .003 & .005 & .003 & .006 & .006 & .008 & .006 & .008 \\  
Hispanic & .102 & .125 & .095 & .102 & .111 & .116 & .124 & .097 & .102 & .094 \\  
 & & & & & & & & & & \\
\textbf{Income} & & & & & & & & & & \\  
Under 25,000 & .206 & .203 & .198 & .219 & .224 & .206 & .216 & .211 & .211 & .216 \\  
25,000-49,999 & .246 & .237 & .246 & .205 & .190 & .225 & .211 & .206 & .217 & .238 \\ 
50,000-749,999 & .167 & .175 & .187 & .178 & .210 & .173 & .189 & .208 & .162 & .181 \\  
75,000-99,999 & .119 & .135 & .148 & .133 & .130 & .151 & .133 & .127 & .135 & .119 \\  
100,000-124,999 & .102 & .090 & .094 & .095 & .094 & .092 & .079 & .087 & .089 & .092 \\  
125,000-149,999 & .038 & .040 & .033 & .052 & .040 & .035 & .049 & .041 & .052 & .044 \\  
150,000-174,999 & .046 & .038 & .030 & .035 & .041 & .038 & .043 & .040 & .044 & .054 \\  
175,000 or more & .076 & .083 & .063 & .083 & .071 & .079 & .079 & .079 & .089 & .056 \\  
 & & & & & & & & & & \\
\textbf{Employment} & & & & & & & & & & \\  
Employed & .646 & .646 & .649 & .608 & .600 & .652 & .624 & .613 & .622 & .622 \\  
Unemployed & .056 & .065 & .057 & .054 & .063 & .051 & .051 & .059 & .075 & .073 \\  
Retired & .183 & .181 & .184 & .213 & .203 & .179 & .214 & .210 & .192 & .194 \\  
Disabled & .033 & .035 & .043 & .046 & .051 & .037 & .027 & .033 & .038 & .041 \\  
Homemaker & .049 & .049 & .052 & .054 & .052 & .052 & .060 & .063 & .054 & .049 \\  
Student & .033 & .024 & .014 & .025 & .030 & .029 & .024 & .022 & .019 & .021 \\  
 & & & & & & & & & & \\
\textbf{Party ID} & & & & & & & & & & \\  
Democrat & .314 & .378 & .367 & .348 & .338 & .341 & .378 & .344 & .378 & .363 \\  
Republican & .279 & .292 & .279 & .294 & .317 & .281 & .308 & .321 & .295 & .292 \\  
Independent & .362 & .300 & .330 & .324 & .311 & .333 & .284 & .302 & .292 & .321 \\  
Something else & .044 & .030 & .024 & .035 & .033 & .044 & .030 & .033 & .035 & .024 \\  
 & & & & & & & & & & \\
\textbf{President} & & & & & & & & & & \\  
Approve & .551 & .533 & .514 & .524 & .502 & .535 & .522 & .522 & .544 & .519 \\  
Disapprove & .449 & .467 & .486 & .476 & .498 & .465 & .478 & .478 & .456 & .481 \\  
 & & & & & & & & & & \\
\textbf{Min. Wage} & & & & & & & & & & \\  
Raised & .648 & .635 & .621 & .630 & .644 & .662 & .619 & .637 & .681 & .649 \\  
Kept the same & .302 & .303 & .302 & .289 & .308 & .273 & .310 & .308 & .251 & .298 \\  
Lowered & .019 & .021 & .027 & .029 & .013 & .027 & .025 & .021 & .025 & .013 \\  
Eliminated & .032 & .041 & .051 & .052 & .035 & .038 & .046 & .035 & .043 & .040 \\  
 & & & & & & & & & & \\
\textbf{Country} & & & & & & & & & & \\  
Right direction & .290 & .292 & .252 & .251 & .249 & .252 & .267 & .265 & .268 & .267 \\  
Wrong track & .710 & .708 & .748 & .749 & .751 & .748 & .733 & .735 & .732 & .733 \\ 
 & & & & & & & & & & \\
\textbf{Age} & \hspace{-0.4cm}48.548 & \hspace{-0.4cm}48.957 & \hspace{-0.4cm}48.949 & \hspace{-0.4cm}49.540 & \hspace{-0.4cm}48.811 & \hspace{-0.4cm}47.157 & \hspace{-0.4cm}49.702 & \hspace{-0.4cm}49.773 & \hspace{-0.4cm}49.100 & \hspace{-0.4cm}49.683 \\  
 & & & & & & & & & & \\
\textbf{Feel Trump} & \hspace{-0.4cm}34.695 & \hspace{-0.4cm}34.378 & \hspace{-0.4cm}37.278 & \hspace{-0.4cm}36.394 & \hspace{-0.4cm}40.579 & \hspace{-0.4cm}36.241 & \hspace{-0.4cm}35.767 & \hspace{-0.4cm}38.584 & \hspace{-0.4cm}34.590 & \hspace{-0.4cm}37.313 \\  
\hline \\[-1.8ex]  
\end{tabular}}
\end{table}



<!--
Gender: Even across all groups for both sets
A few small distances for White (.752 vs. .770), African American (.094 vs. .117), Hispanic (.124 vs. .097)
Under 25: .198 vs. .219
25-50: .246 vs. .205
50-75: .210 vs. .173
Working: .600 vs. 652
Retired: .184 vs. .213
Democrat: .378 vs. 344
Republican: .317 vs. .281
Approve: .502 vs. 535
Disapprove: .456 vs. 482
Raised: .681 vs. 649
Kept the same: .251 vs. .298
Age: 48.811 vs. 47.151
Feel Trump: 40.579 vs. 36.241, 35.767 vs. 38.584, 34.590 vs. 37.313
-->

The focus here lies on the differences between the two sets of blocked education categories for each treatment group, i.e. the proportions/means of all variables for the ANES set in treatment group 1 should look the same as the proportions/means of all variables for the OP set in treatment group 1. We overall do not observe large differences. All `Gender` proportions for instance are virtually identical between the two sets for each group. Scattered throughout, somewhat more noticeable differences occur: The proportion of white respondents in group 5 for example is `r df.all["White", "ANES5"]` for ANES but `r df.all["White", "OP5"]` for OP. The proportions for African-American and Hispanic respondents similarly differ (`r df.all["African-American", "ANES4"]` vs. `r df.all["African-American", "OP4"]` and `r df.all["Hispanic", "ANES4"]` vs. `r df.all["Hispanic", "OP4"]`, both in group 4). Other differences include respondents who earn more than \$25,000 and up to \$50,000 (`r df.all["$25,000-49,999", "ANES2"]` vs. `r df.all["$25,000-49,999", "OP2"]` in group 2), are employed (`r df.all["Employed", "ANES3"]` vs. `r df.all["Employed", "OP3"]` in group 3), and identify as Republican (`r df.all["Republican", "ANES3"]` vs. `r df.all["Republican", "OP3"]` in group 3). The largest differences can be observed for the numerical variables, `Age` and `Feel Trump`: Respondents in group 3 are on average `r df.all["Age", "ANES3"]` years old in the ANES set, but only `r df.all["Age", "OP3"]` years old in the OP set. For `Feel Trump`, respondents in group 3 feel more than `r ((df.all["Feel Trump", "ANES3"] %>% as.numeric) - (df.all["Feel Trump", "OP3"] %>% as.numeric)) %>% round(., digits = 0)` points more sympathetic towards Trump in the ANES set. This is reversed in groups 4 and 5, where OP respondents are `r ((df.all["Feel Trump", "OP4"] %>% as.numeric) - (df.all["Feel Trump", "ANES4"] %>% as.numeric)) %>% round(., digits = 0)` points more favorable towards Trump.

\clearpage

The somewhat larger differences in `Age` and `Feel Trump` could be cause for concern. To analyze this, I run an ANOVA test on both variables. I form a combined data set of all observations for each education set for each treatment group. This results in five data sets, i.e. one per treatment group. The first data set contains all observations from the ANES set that were assigned to treatment group 1 and all observations from the OP set that were assigned to treatment group 1. It also contains the column `Education Set` which denotes the education set each observation belongs to. For treatment group 1, `Education Set` contains the unique values `ANES1` and `OP1`. I then run an ANOVA regression of each numerical variable on `Education Set`. The corresponding models with the respective `R` function read

\begin{align}
\text{aov}(\text{Age} \sim \text{Education Set})
\end{align}

and 

\begin{align}
\text{aov}(\text{Feel Trump} \sim \text{Education Set}).
\end{align}


```{r Variance-In-Treatment-Groups, include=FALSE}

##### compare treatment groups in AN with treatment groups in OP #####

list.dfs.var <- list.dfs # so I don't mess with the output from above

# add a treatment group column (ANES 1, OP 1 etc.) to each data frame
# result is a list of length 10
# each cat only has one unique value (e.g. ANES 1)
for(x in 1:(length(list.dfs)/2)){
  list.dfs.var[[x]]$cat <- rep(paste0("ANES", x), nrow(list.dfs.var[[x]])) %>% as.factor
  list.dfs.var[[x+5]]$cat <- rep(paste0("OP", x), nrow(list.dfs.var[[x+5]])) %>% as.factor
}

list.dfs.t <- list()

# combine ANES and OP data for each treatment group
# result is a list of length 5
# cat now has two unique values respective of treatment group (e.g. OP 1, ANES 1)
for(x in 1:(length(list.dfs)/2)){
  list.dfs.t[[x]] <- c(list.dfs.var[x], list.dfs.var[x+5]) %>%
    ldply(., data.frame) %>% subset(., select =-c(.id))
}

num.res <- rep(list(list()), 2)
names(num.res) <- c("aov.sum", "aov.tukey")
list.dfs.num.aov <- rep(list(num.res), 5)
names(list.dfs.num.aov) <- c("T1", "T2", "T3", "T4", "T5")

for(t in 1:length(list.dfs.num.aov)){
  for(w in 1:ncol(dplyr::select_if(list.dfs.t[[1]], is.numeric))){
    aa <- dplyr::select_if(list.dfs.t[[t]], is.numeric) # select only numeric columns
    bb <- cbind(aa, list.dfs.t[[t]]$cat) # combine numeric columns with cat column
    colnames(bb) <- c(colnames(aa), "cat")
    aov.out <- aov(bb[,w] ~ bb$cat) # aov regression of each numeric column on cat
    list.dfs.num.aov[[t]]$aov.sum[[w]] <- summary(aov.out)
    names(list.dfs.num.aov[[t]]$aov.sum)[[w]] <- colnames(bb)[w]
    list.dfs.num.aov[[t]]$aov.tukey[[w]] <- TukeyHSD(aov.out)
    names(list.dfs.num.aov[[t]]$aov.tukey)[[w]] <- colnames(bb)[w]
    # num.aov.sum[[w]] <- summary(aov.out)
    # names(num.aov.sum)[[w]] <- paste0(colnames(bb)[w], ".sum")
    # num.aov.tukey[[w]] <- TukeyHSD(aov.out)
    # names(num.aov.tukey)[[w]] <- paste0(colnames(bb)[w], ".tukey")
  }
}

fac.res <- rep(list(list()), 2)
names(fac.res) <- c("glm.sum", "glm.anov")
list.dfs.fac.aov <- rep(list(fac.res), 5)
treats <- c("T1", "T2", "T3", "T4", "T5")
names(list.dfs.fac.aov) <- treats

for(t in 1:length(list.dfs.num.aov)){
  for(w in 1:(ncol(dplyr::select_if(list.dfs.t[[1]], is.not.num))-1)){
    my.dat <- dplyr::select_if(list.dfs.t[[t]], is.not.num)
    my.mod <- glm(my.dat[,w] ~ my.dat$cat, family = "binomial")
    list.dfs.fac.aov[[t]]$glm.sum[[w]] <- summary(my.mod)
    names(list.dfs.fac.aov[[t]]$glm.sum)[[w]] <- colnames(my.dat)[w]
    list.dfs.fac.aov[[t]]$glm.anov[[w]] <- anova(my.mod, test = "Chisq")
    names(list.dfs.fac.aov[[t]]$glm.anov)[[w]] <- colnames(my.dat)[w]
  }
}

# These commented out lines are just for me as an overview how all the lists are structured and how to get the numbers as a df
# Lists of length 5, named T1 to T5
# list.dfs.num.aov %>% length
# list.dfs.num.aov %>% names
# list.dfs.fac.aov %>% length
# list.dfs.fac.aov %>% names
# 
# Lists of length 2, named aov.sum, aov.tukey and glm.sum, glm.anov
# list.dfs.num.aov[["T1"]] %>% length
# list.dfs.num.aov[["T1"]] %>% names
# list.dfs.fac.aov[["T1"]] %>% length
# list.dfs.fac.aov[["T1"]] %>% names
# 
# Lists of lengths 2 and 8, named after variables (age, feel.trump and gender, race, income, occupation, pid, pres.approv, min.wage, country.track)
# list.dfs.num.aov[["T1"]][["aov.sum"]] %>% length
# list.dfs.num.aov[["T1"]][["aov.sum"]] %>% names
# list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% length
# list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% names
# 
# Classes and names of each output
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]] %>% class # summary.aov, listof
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]] %>% names # NULL
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]] %>% class # summary.glm
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]] %>% names # lots of useful names
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]] %>% class # TukeyHSD, multicomp
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]] %>% names # bb$cat
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]] %>% class # anova, data.frame
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]] %>% names # lots of names
#
# How to transform each output into a data frame
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]][[1]] %>% data.frame
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]]$coefficients %>% data.frame
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]]$`bb$cat` %>% data.frame
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]][2,] %>% data.frame


# Store the output of each method for all treatment groups, so 4 data frames where each df contains all groups
aov.names <- list.dfs.num.aov[["T1"]][["aov.sum"]] %>% names # could have also used aov.tukey here
aov.length <- list.dfs.num.aov[["T1"]][["aov.sum"]] %>% length
glm.names <- list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% names # could have also used glm.anov here
glm.length <- list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% length # could have also used glm.anov here

list.aov.sum <- list.aov.tukey <- rep(list(list()), length(aov.names))
list.glm.sum <- list.glm.anov <- rep(list(list()), length(glm.names))

for(i in 1:length(treats)){
  tmp.aov.sum <- list.dfs.num.aov[[i]][["aov.sum"]]
  tmp.aov.tukey <- list.dfs.num.aov[[i]][["aov.tukey"]]
  for(x in 1:aov.length){ # this loop does tmp.aov.sum and tmp.aov.tukey
    tmp.sum <- tmp.aov.sum[[names(tmp.aov.sum)[[x]]]][[1]] %>% data.frame
    rownames(tmp.sum) <- c(paste0(names(tmp.aov.sum)[[x]], treats[i]),
                           paste0(names(tmp.aov.sum)[[x]], treats[i], "Resid"))
    list.aov.sum[[x]][[i]] <- tmp.sum
    tmp.tukey <- tmp.aov.tukey[[names(tmp.aov.tukey)[[x]]]]$`bb$cat` %>% data.frame
    rownames(tmp.tukey) <- paste0(names(tmp.aov.tukey)[[x]], treats[i], rownames(tmp.tukey))
    list.aov.tukey[[x]][[i]] <- tmp.tukey
  }
  tmp.glm.sum <- list.dfs.fac.aov[[i]][["glm.sum"]]
  tmp.glm.anov <- list.dfs.fac.aov[[i]][["glm.anov"]]
  for(y in 1:glm.length){ # this loop does tmp.glm.sum and tmp.glm.anov
    tmp.sum <- tmp.glm.sum[[names(tmp.glm.sum)[[y]]]]$coefficients %>% data.frame
    rownames(tmp.sum) <- c(paste0(names(tmp.glm.sum)[[y]], treats[i], "Int"),
                           paste0(names(tmp.glm.sum)[[y]], treats[i]))
    list.glm.sum[[y]][[i]] <- tmp.sum
    tmp.anov <- tmp.glm.anov[[names(tmp.glm.anov)[[y]]]][2,] %>% data.frame
    rownames(tmp.anov) <- paste0(names(tmp.glm.anov)[[y]], treats[i])
    list.glm.anov[[y]][[i]] <- tmp.anov
  }
}

df.aov.sum <- unlist(list.aov.sum, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.aov.sum <- cbind(df.aov.sum[, 1:3] , drop.zero(df.aov.sum[, 4:5], digits = 3))
df.aov.tukey <- unlist(list.aov.tukey, recursive = FALSE) %>% do.call("rbind", .) %>% drop.zero(., digits = 3)
df.glm.sum <- unlist(list.glm.sum, recursive = FALSE) %>% do.call("rbind", .) %>% drop.zero(., digits = 3)
df.glm.anov <- unlist(list.glm.anov, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.glm.anov <- cbind(df.glm.anov[, c(1,3,4)], drop.zero(df.glm.anov[, c(2, 5)], digits = 3)) %>% .[,c(1,4,2,3,5)]

colnames(df.aov.sum) <- c("Df", "Sum.Sq", "Mean.Sq", "F-value", "p-value")
colnames(df.aov.tukey) <- c("Diff.Means", "CI Lower", "CI Upper", "Adj. p-value")
colnames(df.glm.sum) <- c("Estimate", "Std.Error", "z-value", "p-value")
colnames(df.glm.anov) <- c("Df", "Deviance", "Resid.Df", "Residual Deviance", "Pr.Chi")

tab.aov.sum <- stargazer(df.aov.sum, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Summary of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "aov-sum")
at <- gsub("{c}", "{l}", tab.aov.sum, fixed = TRUE) %>%
  gsub("age", "", ., fixed = TRUE) %>%
  gsub("feel.trump", "", ., fixed = TRUE) %>%
  gsub("Resid", " Residuals", ., fixed = TRUE)
cat(at)

tab.aov.tukey <- stargazer(df.aov.tukey, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Tukey's 'Honest Significant Difference' Test of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "aov-tukey")
bt <- gsub("{c}", "{l}", tab.aov.tukey, fixed = TRUE) %>%
  gsub("age", "", ., fixed = TRUE) %>%
  gsub("feel.trump", "", ., fixed = TRUE) %>%
  gsub("OP", " OP", ., fixed = TRUE)
cat(bt)

tab.glm.sum <- stargazer(df.glm.sum, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Summary of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "glm-sum")
ct <- gsub("{c}", "{l}", tab.glm.sum, fixed = TRUE) %>%
  gsub("gender", "", ., fixed = TRUE) %>%
  gsub("race", "", ., fixed = TRUE) %>%
  gsub("income", "", ., fixed = TRUE) %>%
  gsub("occupation", "", ., fixed = TRUE) %>%
  gsub("pid", "", ., fixed = TRUE) %>%
  gsub("pres.approv", "", ., fixed = TRUE) %>%
  gsub("min.wage", "", ., fixed = TRUE) %>%
  gsub("country.track", "", ., fixed = TRUE) %>%
  gsub("Int", " Intercept", ., fixed = TRUE) %>%
  gsub("\\multicolumn{1}{l}{", "", ., fixed = TRUE)
cat(ct)

# To align ct along dots: Straighten the cat output out. Copy only the actual rows into a Latex doc. Replace all } with nothing. Replace all . with .&
# Copy rows back into R. Add \multicolumn{2}{l} for the column headings. Add variable title rows. Replace extracolsep etc. with r@{}l for each numerical column.
# Add longtable stuff


```


This is repeated for each of the five data sets. Tables \ref{aov-sum} shows a summary of the results. Almost all variable intercepts do not show statistical significance. An exception here is treatment group 3, which shows $p$-values of `r df.aov.sum["ageT3", "p-value"]` for `Age` and `r df.aov.sum["feel.trumpT3", "p-value"]` for `Feel Trump`, with the latter showing statistical significance. Overall, however, the differences between the treatment group means of `Age` and `Feel Trump` are not statistically significant, i.e. blocking on two different sets of education categories does not result in significantly differing means for the numerical variables.


\begin{table}[!htbp] \centering    
\caption{Summary of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}    
\label{aov-sum}  
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }  
\\[-1.8ex]\hline
\hline \\[-1.8ex]  
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Df} & \multicolumn{1}{l}{Sum.Sq} & \multicolumn{1}{l}{Mean.Sq} & \multicolumn{1}{l}{F-value} & \multicolumn{1}{l}{p-value} \\  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{\textbf{Age}} & & & & & \\ 
\multicolumn{1}{l}{T1} & 1 & 52.829 & 52.829 & .173 & .678 \\  
\multicolumn{1}{l}{T1 Residuals} & 1,258 & 384,359.900 & 305.533 &  &  \\  
\multicolumn{1}{l}{T2} & 1 & 109.829 & 109.829 & .355 & .551 \\  
\multicolumn{1}{l}{T2 Residuals} & 1,258 & 388,724.900 & 309.002 &  &  \\  
\multicolumn{1}{l}{T3} & 1 & 861.717 & 861.717 & 2.805 & .094 \\ 
\multicolumn{1}{l}{T3 Residuals} & 1,258 & 386,400.000 & 307.154 &  &  \\  
\multicolumn{1}{l}{T4} & 1 & 1.607 & 1.607 & .005 & .942 \\  
\multicolumn{1}{l}{T4 Residuals} & 1,258 & 387,498.400 & 308.027 &  &  \\  
\multicolumn{1}{l}{T5} & 1 & 106.896 & 106.896 & .351 & .553 \\  
\multicolumn{1}{l}{T5 Residuals} & 1,258 & 382,695.200 & 304.209 &  &  \\  
 & & & & & \\
\multicolumn{1}{l}{\textbf{Feel Trump}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & 31.746 & 31.746 & .027 & .869 \\  
\multicolumn{1}{l}{T1 Residuals} & 1,258 & 1,465,958.000 & 1,165.308 &  &  \\  
\multicolumn{1}{l}{T2} & 1 & 246.229 & 246.229 & .196 & .658 \\ 
\multicolumn{1}{l}{T2 Residuals} & 1,258 & 1,576,959.000 & 1,253.544 &  &  \\  
\multicolumn{1}{l}{T3} & 1 & 5,928.007 & 5,928.007 & 4.871 & .027 \\ 
\multicolumn{1}{l}{T3 Residuals} & 1,258 & 1,530,847.000 & 1,216.889 &  &  \\ 
\multicolumn{1}{l}{T4} & 1 & 2,500.496 & 2,500.496 & 2.044 & .153 \\  
\multicolumn{1}{l}{T4 Residuals} & 1,258 & 1,539,114.000 & 1,223.461 &  &  \\ 
\multicolumn{1}{l}{T5} & 1 & 2,334.306 & 2,334.306 & 1.894 & .169 \\ 
\multicolumn{1}{l}{T5 Residuals} & 1,258 & 1,550,046.000 & 1,232.151 &  &  \\ 
\hline \\[-1.8ex]  
\end{tabular}  
\end{table} 


<!--
To investigate this further, I estimate Tukey's 'Honest Significant Difference' test, also know as Tukey's test, for these ANOVA results. Tukey's HSD test compares pairs of means based on a studentized range distribution. It is similar to a $t$-test but corrects for the probability of making false discoveries (i.e. type I errors). It is mathematically formulated as

\vspace{-1cm}
$$q_t = \frac{M_I - M_J}{SE},$$

with $M_I$ and $M_J$ denoting two means (with $M_I$ being the larger one) and $SE$ the standard error of the sum of the means. The resulting value $q_t$ is compared to the critical value of the distribution. Table \ref{aov-tukey} displays the results. Each row shows the comparison of OP and ANES means per treatment group. The adjusted $p$-values confirm the results from Table \ref{aov-sum}. With the exception of treatment group 3, the differences between the treatment group means of `Age` and `Feel Trump` are not statistically significant, i.e. the two blocking mechanisms overall do not result in significantly differing means for the numerical variables.


\begin{table}[!htbp] \centering    
\caption{Tukey's `Honest Significant Difference' Test of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}    
\label{aov-tukey}  
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }  
\\[-1.8ex]\hline  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Diff.Means} & \multicolumn{1}{l}{CI Lower} & \multicolumn{1}{l}{CI Upper} & \multicolumn{1}{l}{Adj. p-value} \\  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{\textbf{Age}} & & & & \\  
\multicolumn{1}{l}{T1 OP1-ANES1} & \multicolumn{1}{l}{.410} & \multicolumn{1}{l}{-1.523} & \multicolumn{1}{l}{2.342} & \multicolumn{1}{l}{.678} \\  
\multicolumn{1}{l}{T2 OP2-ANES2} & \multicolumn{1}{l}{.590} & \multicolumn{1}{l}{-1.353} & \multicolumn{1}{l}{2.534} & \multicolumn{1}{l}{.551} \\  
\multicolumn{1}{l}{T3 OP3-ANES3} & \multicolumn{1}{l}{-1.654} & \multicolumn{1}{l}{-3.591} & \multicolumn{1}{l}{.283} & \multicolumn{1}{l}{.094} \\  
\multicolumn{1}{l}{T4 OP4-ANES4} & \multicolumn{1}{l}{.071} & \multicolumn{1}{l}{-1.869} & \multicolumn{1}{l}{2.011} & \multicolumn{1}{l}{.942} \\  
\multicolumn{1}{l}{T5 OP5-ANES5} & \multicolumn{1}{l}{.583} & \multicolumn{1}{l}{-1.345} & \multicolumn{1}{l}{2.510} & \multicolumn{1}{l}{.553} \\  
 & & & & \\
\multicolumn{1}{l}{\textbf{Feel Trump}} & & & & \\  
\multicolumn{1}{l}{T1 OP1-ANES1} & \multicolumn{1}{l}{-.317} & \multicolumn{1}{l}{-4.091} & \multicolumn{1}{l}{3.456} & \multicolumn{1}{l}{.869} \\  
\multicolumn{1}{l}{T2 OP2-ANES2} & \multicolumn{1}{l}{-.884} & \multicolumn{1}{l}{-4.798} & \multicolumn{1}{l}{3.030} & \multicolumn{1}{l}{.658} \\  
\multicolumn{1}{l}{T3 OP3-ANES3} & \multicolumn{1}{l}{-4.338} & \multicolumn{1}{l}{-8.194} & \multicolumn{1}{l}{-.482} & \multicolumn{1}{l}{.027} \\  
\multicolumn{1}{l}{T4 OP4-ANES4} & \multicolumn{1}{l}{2.817} & \multicolumn{1}{l}{-1.049} & \multicolumn{1}{l}{6.684} & \multicolumn{1}{l}{.153} \\  
\multicolumn{1}{l}{T5 OP5-ANES5} & \multicolumn{1}{l}{2.722} & \multicolumn{1}{l}{-1.158} & \multicolumn{1}{l}{6.602} & \multicolumn{1}{l}{.169} \\  
\hline \\[-1.8ex]  
\end{tabular}  
\end{table} 
-->

\clearpage


This leaves the factor variables. Since an ANOVA test is not possible for non-numerical factor variables (e.g. `Gender`, `Race` etc.), I conduct a binomial GLM regression here. The model formula is the same as above, with the `R` function `glm` replacing `aov`. The results are shown in Table \ref{glm-sum}.


\ssp

\footnotesize

\begin{longtable}{lr@{}lr@{}lr@{}lr@{}l}  
\caption{Summary of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}    
\label{glm-sum}  
\\[-1.8ex]\hline  
\hline \\[-1.8ex]  
  & \multicolumn{2}{l}{Estimate} & \multicolumn{2}{l}{Std.Error} & \multicolumn{2}{l}{z-value} & \multicolumn{2}{l}{p-value} \\  
\hline \\[-1.8ex]  
\textbf{Gender} & & & & \\  
  T1 Intercept & .&134 & .&080 & 1.&672 & .&095 \\ 
  T1 & -.&006 & .&113 & -.&056 & .&955 \\  
  T2 Intercept & .&108 & .&080 & 1.&354 & .&176 \\ 
  T2 & -.&032 & .&113 & -.&282 & .&778 \\ 
  T3 Intercept & .&102 & .&080 & 1.&274 & .&203 \\ 
  T3 & .&045 & .&113 & .&395 & .&693 \\  
  T4 Intercept & .&146 & .&080 & 1.&831 & .&067 \\
  T4 & -.&019 & .&113 & -.&169 & .&865 \\
  T5 Intercept & .&095 & .&080 & 1.&195 & .&232 \\
  T5 & .&013 & .&113 & .&113 & .&910 \\
 & & & & \\  
\textbf{Race} & & & & \\  
  T1 Intercept & -1.&172 & .&094 & -12.&500 & .&000 \\
  T1 & .&069 & .&131 & .&526 & .&599 \\
  T2 Intercept & -1.&207 & .&095 & -12.&757 & .&000 \\ 
  T2 & -.&045 & .&135 & -.&337 & .&736 \\
  T3 Intercept & -1.&111 & .&092 & -12.&040 & .&000 \\
  T3 & .&050 & .&130 & .&389 & .&697 \\
  T4 Intercept & -1.&094 & .&092 & -11.&907 & .&000 \\ 
  T4 & .&017 & .&130 & .&130 & .&897 \\
  T5 Intercept & -1.&111 & .&092 & -12.&040 & .&000 \\
  T5 & -.&096 & .&132 & -.&727 & .&467 \\
   & & & & \\  
\textbf{Income} & & & & \\  
  T1 Intercept & 1.&347 & .&098 & 13.&683 & .&000 \\
  T1 & .&019 & .&140 & .&140 & .&889 \\
  T2 Intercept & 1.&396 & .&100 & 13.&976 & .&000 \\
  T2 & -.&125 & .&139 & -.&901 & .&368 \\
  T3 Intercept & 1.&244 & .&096 & 13.&010 & .&000 \\
  T3 & .&103 & .&137 & .&754 & .&451 \\
  T4 Intercept & 1.&290 & .&097 & 13.&320 & .&000 \\
  T4 & .&028 & .&138 & .&206 & .&837 \\
  T5 Intercept & 1.&318 & .&098 & 13.&503 & .&000 \\
  T5 & -.&028 & .&138 & -.&206 & .&837 \\
 & & & & \\  
\textbf{Occupation} & & & & \\  
  T1 Intercept & -.&602 & .&083 & -7.&221 & .&000 \\
  T1 & -.&000 & .&118 & -.&000 & 1.&000 \\
  T2 Intercept & -.&616 & .&083 & -7.&373 & .&000 \\
  T2 & .&177 & .&117 & 1.&515 & .&130 \\
  T3 Intercept & -.&405 & .&081 & -4.&986 & .&000 \\
  T3 & -.&224 & .&117 & -1.&920 & .&055 \\
  T4 Intercept & -.&506 & .&082 & -6.&149 & .&000 \\ 
  T4 & .&047 & .&116 & .&406 & .&685 \\ 
  T5 Intercept & -.&499 & .&082 & -6.&072 & .&000 \\ 
  T5 & .&000 & .&116 & .&000 & 1.&000 \\
 & & & & \\  
  \textbf{Party ID} & & & & \\  
  T1 Intercept & .&780 & .&086 & 9.&090 & .&000 \\
  T1 & -.&281 & .&119 & -2.&366 & .&018 \\
  T2 Intercept & .&547 & .&083 & 6.&611 & .&000 \\
  T2 & .&083 & .&118 & .&705 & .&481 \\
  T3 Intercept & .&672 & .&084 & 7.&977 & .&000 \\
  T3 & -.&014 & .&119 & -.&119 & .&905 \\
  T4 Intercept & .&499 & .&082 & 6.&072 & .&000 \\
  T4 & .&145 & .&117 & 1.&231 & .&218 \\ 
  T5 Intercept & .&499 & .&082 & 6.&072 & .&000 \\ 
  T5 & .&061 & .&117 & .&525 & .&600 \\
 & & & & \\  
\textbf{Pres. Approval} & & & & \\  
  T1 Intercept & -.&204 & .&080 & -2.&545 & .&011 \\
  T1 & .&070 & .&113 & .&622 & .&534 \\
  T2 Intercept & -.&057 & .&080 & -.&717 & .&473 \\
  T2 & -.&038 & .&113 & -.&338 & .&735 \\
  T3 Intercept & -.&006 & .&080 & -.&080 & .&936 \\
  T3 & -.&134 & .&113 & -1.&184 & .&236 \\
  T4 Intercept & -.&089 & .&080 & -1.&115 & .&265 \\
  T4 & .&000 & .&113 & .&000 & 1.&000 \\
  T5 Intercept & -.&178 & .&080 & -2.&228 & .&026 \\
  T5 & .&102 & .&113 & .&903 & .&366 \\
 & & & & \\  
\textbf{Min. Wage} & & & & \\  
  T1 Intercept & -.&609 & .&083 & -7.&297 & .&000 \\
  T1 & .&055 & .&117 & .&470 & .&638 \\
  T2 Intercept & -.&492 & .&082 & -5.&995 & .&000 \\
  T2 & -.&041 & .&116 & -.&349 & .&727 \\
  T3 Intercept & -.&595 & .&083 & -7.&145 & .&000 \\
  T3 & -.&077 & .&118 & -.&651 & .&515 \\
  T4 Intercept & -.&486 & .&082 & -5.&918 & .&000 \\
  T4 & -.&075 & .&117 & -.&641 & .&522 \\ 
  T5 Intercept & -.&758 & .&085 & -8.&870 & .&000 \\
  T5 & .&143 & .&119 & 1.&193 & .&233 \\
 & & & & \\  
\textbf{Country Track} & & & & \\  
  T1 Intercept & .&893 & .&088 & 10.&176 & .&000 \\
  T1 & -.&008 & .&124 & -.&062 & .&951 \\
  T2 Intercept & 1.&086 & .&092 & 11.&840 & .&000 \\
  T2 & .&008 & .&130 & .&065 & .&948 \\
  T3 Intercept & 1.&103 & .&092 & 11.&974 & .&000 \\
  T3 & -.&017 & .&130 & -.&130 & .&897 \\
  T4 Intercept & 1.&012 & .&090 & 11.&228 & .&000 \\
  T4 & .&008 & .&128 & .&064 & .&949 \\
  T5 Intercept & 1.&003 & .&090 & 11.&159 & .&000 \\
  T5 & .&008 & .&127 & .&064 & .&949 \\ 
 \hline \\[-1.8ex]
 \end{longtable} 

\dsp

\normalsize


Contrary to the numerical variables, a large number of the $p$-values for the variable intercepts are highly significant here. This includes all intercepts of `Race`, `Income`, `Occupation`, `Party ID`, `Min. Wage`, and `Country Track`. The only exceptions are `Gender` and `Pres. Approval`. None of the `Gender` intercepts in any treatment group reach statistical significance, while only the intercepts in treatment groups 1 and 5 do so for `Pres. Approval` (`r df.glm.sum["pres.approvT1Int", "p-value"]` and `r df.glm.sum["pres.approvT5Int", "p-value"]`, respectively). These reliably highly significant intercept values indicate that a differentiation in education categories is important for non-numerical factor variables as the categories are statistically distinct. 




### Placebo Regression {#ordblock-data-plac}

To further test the usefulness of the ordered probit categories, I conduct a placebo regression. Placebo tests are most commonly used for difference-in-differences estimators and are falsification tests to analyze whether an effect exists that should not exist [@bertrand_2004_much; @mills_2009_palgrave; @rothstein_2010_teacher]. The placebo treatment should be unrelated to the model or method being studied [@hartman_2018_equivalence; @rosenbaum_2002_observational; @mora_2019_alternative]. In the effort to conduct an analysis that is separate from the previous section, I block the 2016 ANES data anew, this time into two treatment groups. As before, this is done separately for the original ANES and the OP education categories. We then model the following OLS regression on the feeling thermometer towards Donald Trump as the Republican presidential candidate:

\begin{align}
\text{Feel Trump} \sim \text{Group} + \text{Democrat} + \text{Republican} + \text{Income} + \text{Male} + \text{White} + \text{Black} + \text{Hispanic}
\end{align}

The \texttt{Group} variable is added after the data was collected. This means no actual treatment was administered, i.e. \texttt{Group} is a placebo treatment. I did not conduct an experiment on the ANES data. Instead, I created the \texttt{Group} variable, randomly 'assigned' observations to the two treatment groups, and then used this variable as an explanatory variable. The treatment is thus completely artificial. In the absence of actual treatment, the difference between both treatment groups should be zero, i.e. there should be no significant differences between the \texttt{Group} regression coefficients. The null hypothesis thus assumes that there is no effect. To test this, each blocking/regression process for each set of categories is repeated 1,000 times. The distribution of the placebo treatment indicator (\texttt{Group}) is visualized in Figure \ref{DensPlacTreat}. 


```{r Placebo Treatment Plotting Code, include=FALSE}

repeats <- 1000

list.list.gt2.coeff <- readRDS(paste("data/blocking/list_list_test_ols_trump_gt2_coeff_all_obs_", repeats, "_runs.rds", sep = ""))

gt2.orig <- unlist(list.list.gt2.coeff[[1]], recursive = FALSE)
gt2.new <- unlist(list.list.gt2.coeff[[2]], recursive = FALSE)

gt2.orig.df <- data.frame(cbind(gt2.orig, rep("ANES", length(gt2.orig))))
gt2.new.df <- data.frame(cbind(gt2.new, rep("OP", length(gt2.new))))
colnames(gt2.orig.df) <- colnames(gt2.new.df) <- c("coefficient", "categories")
gt2.df <- rbind(gt2.orig.df, gt2.new.df)
gt2.df$coefficient <- as.numeric(as.character(gt2.df$coefficient))

```


```{r Density-Plot-Placebo-Treatment, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Placebo Treatment Coefficients by Education Set. Overlapping Area Shown in Dark Purple.\\label{DensPlacTreat}"}

ggplot(gt2.df, aes(x=coefficient, fill=categories)) + geom_density(alpha=0.4, aes(y=..density..), position="identity") + xlab("Regression Coefficients for Placebo Treatment Group") + ylab("Density") + theme(legend.title=element_blank()) + theme(legend.position = c(0.85, 0.75)) + theme(plot.title = element_text(hjust = 0.5))+ scale_fill_manual(values=c("red", "blue"))

```


Both distributions center around zero, as is the statistical expectation. Upon closer inspection, however, the ordered probit categories are closer to the true value of zero than the ANES categories on both mean (`r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, mean)[2], digits = 3)) %>% drop.zero.num` v. `r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, mean)[1], digits = 3)) %>% drop.zero.num`) and median (`r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, median)[2], digits = 3)) %>% drop.zero.num` v. `r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, median)[1], digits = 3)) %>% drop.zero.num`). This indicates slightly superior performance by the ordered probit categories, as they more closely approach the true value when used in a regression.





## Conclusion {#ordblock-conclusion}

I set out to improve the use of ordinal variables to block respondents into treatment groups in survey experiments. Survey experiments depend on balance of covariates between treatment groups to allow the estimation of causal effects. Randomization ensures balance for large samples but becomes problematic for small samples. Blocking greatly alleviates this problem. 

Blocking naturally depends on covariates. One of the most important covariates in political science is education. Out of convenience, education is often converted to a numerical variable for regressions in practice. Due to education's special nature as an ordinal variable, such an approach is potentially problematic as ordinal variable levels are not evenly spaced. The distance between the education categories "Elementary School" and "Some High School" is very likely not the same as the distance between "Some High School" and "High School Graduate". Converting these three categories to the numerical values 1, 2, and 3, however, assumes evenly spaced distances. This could lead to a misrepresentation of the data.

As an alternative, I proposed an ordered probit approach whereby we estimate the latent underlying continuous variable underneath education. This estimates cutoff thresholds between the education categories, bins observations according to linear model predictors, and results in a new set of education categories that fit the data and, most importantly, represent the latent underlying continuous variable with its unevenly spaced distances. I applied this approach to the 2016 ANES data, resulting in two sets of education categories: the original ANES categories, and the newly estimated ordered probit (OP) categories. I subsequently blocked both sets into five treatment groups and analyzed the group means and variances. While the numerical variables do not show statistically distinct intercepts, almost all intercepts for the non-numerical factor variables are statistically significant. This indicates that the two sets of covariates are statistically distinct and that the re-estimation of education categories is meaningful. I also ran a placebo regression 1,000 times to conduct a falsification test. This shows distributions of the placebo treatment variable around the statistically expected zero but also reveals the OP variable to be closer to zero than the ANES variable for both mean and median. This also indicates slightly superior performance of the OP method. Together, these tests thus show that the re-estimation of ordinal variable categories with an ordered probit approach matters. The next chapter will use the OP approach to address multiple imputation with ordinal variables.




