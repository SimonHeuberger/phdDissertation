# PRECISION IN SURVEY EXPERIMENTS -- A NEW METHOD TO IMPROVE BLOCKING ON ORDINAL VARIABLES {#ordblock}

## Introduction {#ordblock-intro}

```{r include=FALSE}
drop.zero <- function (df, digits = 4, p, s, pad.char = NA, ...){
    library(dplyr)
    ldots <- list(...)
    if (length(ldots) > 0) {
        if (!is.null(ldots[["prefix"]]) & missing(p)) 
            p <- ldots[["prefix"]]
        if (!is.null(ldots[["suffix"]]) & missing(s)) 
            s <- ldots[["suffix"]]
    }
    x <- dplyr::select_if(df, is.numeric)
    for(i in 1:ncol(x)){
      x[,i] <- round(as.numeric(x[,i]), digits)
      x[,i] <- sprintf(paste0("%.", digits, "f"), x[,i])
      x[,i] <- gsub("^0(?=\\.)|(?<=-)0", "", x[,i], perl = TRUE)
    }
    df[, colnames(x)] <- x
    return(df)
}
```


Survey experiments collect background information and attempt to uncover treatment effects on public opinion and political behavior. In order to identify such potential effects, the treatment groups need to be comparable. All treatment groups need to look the same in every measure, i.e. they must be balanced. This can be achieved through random assignment of participants to treatment groups. Randomization, i.e. flipping a coin to decide which treatment group a participant is assigned to, probabilistically results in balance based on the Law of Large Numbers [@urdan_statistics_2010]. For small samples, however, it can lead to serious imbalance. It can easily be that the treatment groups will not look the same. This can leave experimental results in statistically murky waters [@imai_quantitative_2018;@king_designing_1994;@fox_applied_2015]. In survey experiments, the overall sample size is often split across several treatment groups, which can exacerbate the problem. @chong_framing_2007, for instance, split 869 participants in a framing experiment on urban growth over 17 treatment groups, which leads to an average of just over 50 participants per group. Randomization is unlikely to lead to balanced treatment groups of this size. Researchers need to employ statistical methods to obtain balanced groups here. Blocking, i.e. arranging participants in groups that are equal in terms of participants' covariates and using random allocation within these groups, can alleviate such worries. 

Blocking depends on covariates. In political science, many covariates with high predictive power are categorical variables, i.e. variables where the data can be divided into groups. These include  interval (ordered and evenly spaced, e.g. income) and ordinal (ordered and unevenly spaced, e.g. education) variables. To block, these variables are often made numeric, e.g. by assigning the numbers 1-4 to the variable categories. This is acceptable for interval variables as the evenly spaced numbers correspond to the evenly spaced categories. For ordinal variables, however, this can be problematic. An arbitrary evenly spaced string of numbers does not correspond to the unevenly spaced ordinal categories and may misrepresent the data. I propose an ordered probit threshold approach to circumvent this problem: This approach estimates an assumed underlying latent continuous structure underneath ordinal variables whose data-driven categories can then be used for blocking. By training a linear model on meaningful data, it creates numerical thresholds which partition the variable into regions corresponding to the ordinal categories and bins the observations between these thresholds according to the explanatory variables. These binned cases determine which of the original categories make sense given the underlying latent continuous structure. The result is a data-based and non-arbitrary re-estimated set of variable categories. Because of their data-driven estimation, these categories can be safely used for blocking. This approach allows researchers to block on ordinal variables in survey experiments without making unwarranted assumptions in terms of arbitrary numeric values whilst fully utilizing the ordinal information provided and respecting uneven spaces.

The following sections provide a background on survey experiments and blocking, describe the key aspects of ordinal variables, and outline my proposed ordered probit approach. I then demonstrate the benefits and implications of this approach with external survey data and simulations. Since there currently is no available tool to block online survey experiments, I create my own survey environment in `R shiny`, which is outlined in appendix \ref{app-ordblock}.


## Theory {#ordblock-theory}

### Preliminary Notations on Survey Experiments {#ordblock-theory-experiments}

The simplest of survey experiments has two potential outcomes for participants $i$, $y_{1i}$ and $y_{0i}$, with 1 denoting the treatment and 0 referring to the control. Consider a simplified version of a famous survey experiment by @tversky_framing_1981, where researchers want to test the effect of the mortality format on participants' choices. They provide participants with the following scenario:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent Imagine that the US is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. A program to combat the disease has been proposed. Assume that the exact scientific estimates of the consequences of the program are as follows...
\end{adjustwidth}

Participants in the control group receive the program description in survival format:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent If the program is adopted, 200 out of 600 people will live.
\end{adjustwidth}

Participants in the treatment group receive the program description in mortality format:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent If the program is adopted, 400 out of 600 people will die.
\end{adjustwidth}

All participants are subsequently asked whether they support or oppose the program. The treatment effect for each individual participant $i$ is given by $y_{1i} - y_{0i}$. If both groups of participants look the same regarding their covariates (age, education, income etc.), a comparison of the groups' average support reveals the Average Treatment Effect (ATE) across all participants, $\mathbf{E}[\delta] = \mathbf{E}[y_{1i} - y_{0i}]$. A central characteristic of such a comparison is the fundamental problem of causal inference [@holland_1986_statistics;@rubin_1974_estimating]: We are unable to observe both potential outcomes for the same participant at once. In our case, we cannot observe how much participant A supports the program if given the survival format whilst also observing how much the same participant A would have supported the program if given the mortality format. If we could, it would be simple to calculate the true average treatment effect, $\mathbf{E}[\delta] = \mathbf{E}[y_{1i}|T=1] - \mathbf{E}[y_{0i}|T=0]$, with $T=0$ denoting the control and $T=1$ the treatment group. Since the true average treatment effect is unobservable, we need to use statistical means to assess the counterfactuals. This can be done by balancing the treatment and control groups. If both groups of participants look the same in every measure, we can use the participants who received the mortality format (treatment) to estimate what would have happened to the participants who did not receive the mortality format (control). The crucial aspect is whether the two groups do indeed look the same in terms of participants' covariates. The potential outcome of the control needs to mirror what would have happened in the case of treatment, and vice versa. There are two main means by which this may be achieved: Randomization and blocking.


### Randomization {#ordblock-theory-randomization}

Randomization is equivalent to flipping a coin for each participant to be assigned to treatment or control. This chance procedure gives each participant an equal chance of being assigned to either group (or groups, in case of multiple treatment groups) [@lachin_1988_properties]. Randomization increases covariate balance as the number of participants, $n$, increases [@imai_2009_essential]. The larger a researcher's sample, the better the resulting balance from randomization in expectation. Probabilistically, randomization enables the comparison of the average treatment effect to be unbiased, which allows the researcher to attribute any treatment effects to the treatment [@king_a-politically_2007]. 

While randomization thus guarantees balance as the sample size reaches infinity, it often does not do so in the naturally finite sample sizes researchers actually work with. With huge samples, the Law of Large Numbers predicts that treatment groups selected through randomization will be balanced. With small samples, however, it is possible to get unlucky and end up with unbalanced groups [@imai_2008_misunderstandings]. Blocking can help achieve balance in such scenarios [@epstein_2002_rules].


### Blocking {#ordblock-theory-blocking}

Identical levels in terms of covariates across treatment groups represent the key aspect in experimental studies. In randomization, this is achieved by random chance. In blocking, this is achieved by combining covariate information about the participants with randomization. Specifically, participants are blocked into treatment groups that are similar to one another in terms of the their covariates before treatment is assigned. Their similarity is estimated with the Mahalanobis or Euclidian distance. Blocking is better suited to achieving balance in finite samples than randomization, as it "directly controls the estimation error due to differing levels of observed covariates in the treatment and control groups" [@moore_2012_multivariate, p. 463]. This is particularly relevant with small samples and a high number of treatment groups, as the overall number of participants needs to be divided up. Figures \ref{BoxLawLarNum} and \ref{HistLawLarNum} show this visually. A numeric discrete variable with levels 1 to 5 is randomized and blocked for different sample sizes and numbers of treatment groups. This is repeated 100 times for each sample size. Figure \ref{BoxLawLarNum} shows the maximum distances between treatment groups across these repetitions for sample sizes up to 1,000 for two, three, five, and ten treatment groups. Blocking outperforms randomization in every scenario. The difference between the two methods is smallest for large samples and a small number of treatment groups. 

```{r Law of Large Numbers Simulations, eval=FALSE, include=FALSE}

### THE CODE THAT CREATES THE SIMULATIONS FOR THE BLOCKING PLOTS IS IN ___ordinal_blocking/law_large_numbers/__lln_testing.Rmd. IT TAKES 3-4 DAYS TO RUN THIS CODE, SO THERE IS NO POINT HAVING IT HERE ###

```

```{r Law of Large Numbers Plotting Code Boxplots, include=FALSE}

### I LOAD THE CREATED SIMULATIONS HERE TO CREATE THE PLOTS ###

all_blocked_2 <- read.csv("data/blocking/all_blocked_2.csv")
all_blocked_3 <- read.csv("data/blocking/all_blocked_3.csv")
all_blocked_5 <- read.csv("data/blocking/all_blocked_5.csv")
all_blocked_10 <- read.csv("data/blocking/all_blocked_10.csv")
all_means_variances_2 <- read.csv("data/blocking/all_means_variances_2.csv")
all_means_variances_3 <- read.csv("data/blocking/all_means_variances_3.csv")
all_means_variances_5 <- read.csv("data/blocking/all_means_variances_5.csv")
all_means_variances_10 <- read.csv("data/blocking/all_means_variances_10.csv")

# There are 100 NAs each in "all_means_variances_3", "all_means_variances_5", and "all_means_variances_10"
# They are always for the first respective sampled number: 9, 15, 30
# They're in control for _3 and _5, and in treatment5 for _10
# I don't know why those are happening, but I'm not starting with the first sampled numbers anyway, and it doesn't make any difference for the overall simulations, so I am removing those
all_means_variances_3 <- subset(all_means_variances_3, subset = (sampled_numbers != 9))
all_means_variances_5 <- subset(all_means_variances_5, subset = (sampled_numbers != 15))
all_means_variances_10 <- subset(all_means_variances_10, subset = (sampled_numbers != 30))

# I want the plot legend to read "randomized" instead of "rand"
all_means_variances_2$label <- fct_recode(all_means_variances_2$label, "randomized" = "rand")
all_means_variances_3$label <- fct_recode(all_means_variances_3$label, "randomized" = "rand")
all_means_variances_5$label <- fct_recode(all_means_variances_5$label, "randomized" = "rand")
all_means_variances_10$label <- fct_recode(all_means_variances_10$label, "randomized" = "rand")

together <- list(all_blocked_2, all_means_variances_2, all_blocked_3, all_means_variances_3, all_blocked_5, all_means_variances_5, all_blocked_10, all_means_variances_10) # collect all dfs in a list to loop over
couple <- list() # empty list

for(i in 1:(length(together))){
   couple[[i]] <- subset(together[[i]], select = c(sampled_numbers, diff, label))
   couple[[i]]$sampled_numbers <- as.factor(couple[[i]]$sampled_numbers)
} # subset for 3 columns and turn sampled_numbers into factor

sims_2 <- rbind(couple[[1]],couple[[2]]) # combine blocked and rand for each # of treatment groups
sims_3 <- rbind(couple[[3]],couple[[4]])
sims_5 <- rbind(couple[[5]],couple[[6]])
sims_10 <- rbind(couple[[7]],couple[[8]])

selection <- c(0, 0.1, 0.2, 0.3, 0.5, 1) # the quantiles I want

quantile(as.numeric(levels(sims_2$sampled_numbers)), selection) # quantiles for 2 groups
levels(sims_2$sampled_numbers) # levels for 2 groups
sims_2_range <- subset(sims_2, subset = sampled_numbers %in% c(14, 102, 206, 302, 502, 998)) # hand-select samples for range
sims_2_one <- subset(sims_2, subset = sampled_numbers == as.numeric(levels(sims_2$sampled_numbers)[2])) # select second level for 'intro' plot

quantile(as.numeric(levels(sims_3$sampled_numbers)), selection)
levels(sims_3$sampled_numbers)
sims_3_range <- subset(sims_3, subset = sampled_numbers %in% c(18, 108, 207, 306, 504, 999))
sims_3_one <- subset(sims_3, subset = sampled_numbers == as.numeric(levels(sims_3$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_5$sampled_numbers)), selection)
levels(sims_5$sampled_numbers)
sims_5_range <- subset(sims_5, subset = sampled_numbers %in% c(25, 115, 215, 305, 505, 995))
sims_5_one <- subset(sims_5, subset = sampled_numbers == as.numeric(levels(sims_5$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_10$sampled_numbers)), selection)
levels(sims_10$sampled_numbers)
sims_10_range <- subset(sims_10, subset = sampled_numbers %in% c(40, 130, 220, 300, 500, 1000))
sims_10_one <- subset(sims_10, subset = sampled_numbers == as.numeric(levels(sims_10$sampled_numbers)[2]))

xlab <- "Sample Sizes"
ylab <- "Max. Distances Between Treatment Groups"

plot_first <- ggplot(sims_2_one, aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.15, 0.65)) # first plot outside of the loop because of the legend

sims_plots <- list(sims_2_range, sims_3_one, sims_3_range, sims_5_one, sims_5_range, sims_10_one, sims_10_range) # list of all subsets for plotting
plots <- list()
for(i in 1:(length(sims_plots))){
   plots[[i]]  <- ggplot(sims_plots[[i]], aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + guides(fill=FALSE)
  } # create plot for each data subset

```


```{r Boxplot-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distances between treatment group means in randomized and blocked data. Increasing sample size for 2 (top row), 3 (second row), 5 (third row), and 10 treatment groups (bottom row). Leftmost pair on right panel is exactly the pair on the left panel\\label{BoxLawLarNum}"}

grid.arrange(plot_first, plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]], plots[[7]], ncol = 2, nrow = 4, bottom=xlab, left=ylab)

```


For $n = 998$ and two treatment groups, the largest distance between randomized treatment groups is `r round(max(all_means_variances_2$diff[all_means_variances_2$sampled_numbers == 998]), digits=3)`, and the largest distance between blocked treatment groups is `r round(max(all_blocked_2$diff[all_blocked_2$sampled_numbers == 998]), digits=3)`. For small samples and a large number of treatment groups, however, the difference is much starker. For $n = 40$ and ten treatment groups, the largest distance between randomized treatment groups is `r round(max(all_means_variances_10$diff[all_means_variances_10$sampled_numbers == 40]), digits=3)`, and the largest distance between blocked treatment groups is `r round(max(all_blocked_10$diff[all_blocked_10$sampled_numbers == 40]), digits=3)`. Figure \ref{HistLawLarNum} shows the distribution of these imbalances.


```{r Law of Large Numbers Plotting Code Histograms, include=FALSE}

ylab <- "Density"
xlab <- "Max. Distances Between Treatment Groups"

plot_first_more <- ggplot(sims_2, aes(x=diff, fill=label)) + geom_histogram(alpha=0.2, aes(y=..density..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.6, 0.75)) # first plot outside of the loop because of the legend

sims_plots_more <- list(sims_3, sims_5, sims_10) # list of all data sets (minus for 2 groups for plotting)

plots_more <- list()
for(i in 1:(length(sims_plots_more))){
   plots_more[[i]] <- ggplot(sims_plots_more[[i]], aes(x=diff, fill=label)) + geom_histogram(alpha=0.2, aes(y=..density..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + guides(fill=FALSE) # create plot for each data set
}
```

```{r Hist-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=4, fig.cap="Distribution of treatment group differences in randomized and blocked data for 2 (top left), 3 (top right), 5 (bottom left), and 10 (bottom right) treatment groups\\label{HistLawLarNum}"}

grid.arrange(plot_first_more, plots_more[[1]], plots_more[[2]], plots_more[[3]], ncol = 2, nrow = 2, bottom = xlab, left = ylab)

```


#### Blocking On The Go {#ordblock-theory-blocking-onthego}

In political science, researchers often have an already-collected data set in front of them. One example are the American National Election Studies (ANES), a pre-existing survey database, which is often used to analyze voter turnout [see for instance @jackman_2018_does; @leighley_who_2014], among many others. This setup means all covariate information on all participants is known at the time of assignment, which makes blocking straight-forward. Oftentimes, however, the covariate information of all participants is not known at the time of assignment. This is the case, for instance, for online survey experiments, where each participant completes the survey at differing times. Participants 'trickle in' for treatment assignment as the experiment progresses. 'Traditional' blocking can not be used here, since it relies on covariate information about the entire sample. Instead, we need to block continuously as the experiment progresses, or block 'on the go'. This is called sequential blocking. 

Sequential blocking in political science is based on covariate-adaptive randomization, which varies probabilities based on knowledge about previous participants and the current participant [@chow_2007_adaptive]. Traditional covariate-adaptive approaches, such as the biased coin design [@efron_1971_forcing] and minimization [@pocock_1975_sequential], assign the incoming participant to the treatment group with the fewest participants with identical covariate information. This works for discrete covariates as the number of possible covariate levels is finite. For continuous covariates, the number of possible covariate levels rises exponentially. Participants are unlikely to look the same, and identical participants are rare. Blocking on continuous covariates is not possible with these traditional approaches [@markaryan_2010_exact;@rosenberger_2002_randomization;@eisele_1995_biased]. @moore_blocking_2013 develop a method to do so by exploiting relationships between the current participant's covariate profile and those of all previously assigned participants. They define the similarity between participants with the Mahalanobis distance (MD) between participants $q$ and $r$ with covariate vectors $\bm{x}_q$ and $\bm{x}_r$, \newline \noindent $MD_{qr} = \sqrt{(\bm{x}_q - \bm{x}_r)' \reallywidehat{\sum}^{-1} (\bm{x}_q - \bm{x}_r)}$. To aggregate pairwise similarity, they implement the mean, median, and trimmed mean of the pairwise MDs between the current participant and the participants in each treatment condition: Participants are indexed with treatment condition $t$ using $r \in \{1,...,R\}$. For each condition $t$, an average MD between the current participant, $q$, and the participants previously assigned, $t$. If the distance in terms of MD for the incoming participant is 2 in the control and 5 for the treatment condition, the incoming participant looks more similar to the control condition. To set the probability of assignment, @moore_blocking_2013 calculate the mean Mahalanobis distances for each incoming participant, $q$, for all treatment conditions, $t$, and sort the treatment conditions by these averages. Randomization is biased towards conditions with high scores. For each value of $k$, with $k \in \{2,3,...,6\}$, the condition with the highest average MD is then assigned a probability $k$ times larger than all other assignment probabilities. 

Blocking is thus possible when all covariate information is known at the time of assignment and when this information 'trickles in' over time. Covariate information, however, is only one side of the coin. Researchers also need to take into consideration the characteristics of the variable to block on. Not all types of variables can and should be used the same way to be blocked on. Specifically, the current use of ordinal variables as blocking variables is somewhat problematic. I describe these problems and my proposed solution in section \ref{intro} above. The following sections will apply this solution to external survey data in simulations.





## Data {#ordblock-data}

One of the most respected and recognized externally and internally valid data sets are the American National Election Studies. I thus choose the following ordered probit model with the 2016 ANES data (the predictors are standard linear predictors in political science literature): 

\vspace{-1cm}
$$Education \sim Gender + Race + Age + Income + Occupation + Party ID$$

When trained on the 2016 ANES data, this ordinal probit model estimates the thresholds between each of the education categories shown in Table \ref{education-categories}.

```{r Education Thresholds Table, results='asis', echo=FALSE}
op.model.thresholds <- read.csv("data/blocking/thresholds.csv")
colnames(op.model.thresholds) <- c("Thresholds", "Coefficients", "Standard Errors", "t-values")
stargazer(op.model.thresholds, 
          summary = FALSE, 
          rownames = FALSE, 
          header=FALSE, 
          align = TRUE, 
          title = "Ordered Probit Threshold Estimates", 
          label = "education-categories")
```


The observations in the data are binned according to the estimated threshold coefficients, which in turn determines what education categories make sense, given the underlying latent continuous variable. Figure \ref{BarEducCat} shows the distribution of both the original and the model-estimated education categories. 


```{r Education Categories Plotting Code Histograms, include=FALSE}

op.model.data <- readRDS("data/anes/anes_education.rds")
plot.orig <- ggplot(op.model.data, aes(x = education)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = "darkred") + theme(axis.title=element_blank())
plot.new <- ggplot(op.model.data, aes(x = education.new)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = "darkred") + theme(axis.title=element_blank())

```

```{r Barplot-Education-Categories, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Education Categories. Original 2016 ANES categories on the left, ordered probit estimated categories on the right\\label{BarEducCat}"}

grid.arrange(plot.orig, plot.new, ncol = 2, nrow = 1, bottom = "Education", left = "Percentages")

```


As we can see, all categories 'below' "High school graduate" and 'above' "Master's" are collapsed because they do not fit the data. The ordered probit model uses the ordinal information with unevenly spaced distances provided and returns categories that do fit the data. We can now use these estimated education categories as the basis for blocking. Assigning numeric values to the new categories is now justifiable because they are based on data-driven estimations. This allows us to block on numerical values with the Mahalanobis distance, which would not theoretically permitted without empirical justification. The following sections show that the new estimated categories affect analyses and results. 



### Simulations {#ordblock-data-sims}

I conduct various simulations to compare the Ordered Probit Model and its resulting re-estimated education categories with the original ANES categories.


### Placebo Regression {#ordblock-data-plac}

We separately block the 2016 ANES on the original and the ordered probit education categories into two treatment groups. We then model the following OLS regression on an interval response variable, a feeling thermometer towards Donald Trump as the Republican presidential candidate:

\vspace{-1cm}
$$Feel.Trump \sim Group + Dem + Rep + Income + Male + White + Black + Hispanic$$

\texttt{Group} indicates a placebo treatment, as no actual treatment is administered. In the absence of actual treatment, the difference between both treatment groups should thus be zero.



## Results {#ordblock-results}

### Simulations {#ordblock-results-sims}


### Placebo Regression {#ordblock-results-plac}

To test this, each blocking/regression process for each set of categories is repeated 1,000 times. The distribution of the placebo treatment indicator (\texttt{Group}) is visualized in Figure \ref{DensPlacTreat}. 


```{r Placebo Treatment Plotting Code, include=FALSE}

repeats <- 1000

list.list.gt2.coeff <- readRDS(paste("data/blocking/list_list_test_ols_trump_gt2_coeff_all_obs_", repeats, "_runs.rds", sep = ""))

gt2.orig <- unlist(list.list.gt2.coeff[[1]], recursive = FALSE)
gt2.new <- unlist(list.list.gt2.coeff[[2]], recursive = FALSE)

gt2.orig.df <- data.frame(cbind(gt2.orig, rep("original", length(gt2.orig))))
gt2.new.df <- data.frame(cbind(gt2.new, rep("ordinal probit", length(gt2.new))))
colnames(gt2.orig.df) <- colnames(gt2.new.df) <- c("coefficient", "categories")
gt2.df <- rbind(gt2.orig.df, gt2.new.df)
gt2.df$coefficient <- as.numeric(as.character(gt2.df$coefficient))

```


```{r Density-Plot-Placebo-Treatment, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of placebo treatment coefficients by education model\\label{DensPlacTreat}"}

ggplot(gt2.df, aes(x=coefficient, fill=categories)) + geom_density(alpha=0.2, aes(y=..density..), position="identity") + xlab("Regression Coefficients for Placebo Treatment Group") + ylab("Density") + theme(legend.title=element_blank()) + theme(legend.position = c(0.85, 0.75)) + theme(plot.title = element_text(hjust = 0.5))

```

Both distributions center around zero, as is the statistical expectation. Upon closer inspection, the ordered probit categories are closer to the true values than the original categories on both mean (`r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, mean)[2], digits = 3))` v. `r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, mean)[1], digits = 3))`) and median (`r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, median)[2], digits = 3))` v. `r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, median)[1], digits = 3))`). This indicates slightly superior performance by the ordered probit categories.



```{r Block-ANES-OP-categories, include=FALSE}

df.an <- readRDS("data/blocking/df_an.RDS")
df.op <- readRDS("data/blocking/df_op.RDS")

for(i in 1:(df.an$education %>% unique %>% length)){
  df.an[df.an$education == i, "education"] <- op.model.data$education %>% levels %>% .[i]
}

df.an$education <- factor(df.an$education, levels = op.model.data$education %>% levels)

for(i in 1:(df.op$education %>% unique %>% length)){
  df.op[df.op$education == i, "education"] <- op.model.data$education.new %>% levels %>% .[i]
}

df.op$education <- factor(df.op$education, levels = op.model.data$education.new %>% levels)
assigned.an <- readRDS("data/blocking/assigned_an.RDS")
assigned.op <- readRDS("data/blocking/assigned_op.RDS")

an.t1 <- df.an[df.an$id %in% assigned.an$assg[[1]][,1],] %>% subset(., select=-c(id, education))
an.t2 <- df.an[df.an$id %in% assigned.an$assg[[1]][,2],] %>% subset(., select=-c(id, education))
an.t3 <- df.an[df.an$id %in% assigned.an$assg[[1]][,3],] %>% subset(., select=-c(id, education))
an.t4 <- df.an[df.an$id %in% assigned.an$assg[[1]][,4],] %>% subset(., select=-c(id, education))
an.t5 <- df.an[df.an$id %in% assigned.an$assg[[1]][,5],] %>% subset(., select=-c(id, education))

op.t1 <- df.op[df.op$id %in% assigned.op$assg[[1]][,1],] %>% subset(., select=-c(id, education))
op.t2 <- df.op[df.op$id %in% assigned.op$assg[[1]][,2],] %>% subset(., select=-c(id, education))
op.t3 <- df.op[df.op$id %in% assigned.op$assg[[1]][,3],] %>% subset(., select=-c(id, education))
op.t4 <- df.op[df.op$id %in% assigned.op$assg[[1]][,4],] %>% subset(., select=-c(id, education))
op.t5 <- df.op[df.op$id %in% assigned.op$assg[[1]][,5],] %>% subset(., select=-c(id, education))


### props of factor columns, except education ###

list.props <- list()
list.dfs <- list(an.t1, an.t2, an.t3, an.t4, an.t5,
                 op.t1, op.t2, op.t3, op.t4, op.t5)
n.tr <- 5
an <- c()
op <- c()

for(w in 1:n.tr){
  an[w] <- paste0("AN", w)
  op[w] <- paste0("OP", w)
}

names(list.dfs) <- c(an, op)
is.not.num <- function(x) !is.numeric(x)
tmp <- dplyr::select_if(an.t1, is.not.num)

for(x in 1:ncol(tmp)){
  list.props[[x]] <- data.frame(matrix(NA, levels(tmp[, x]) %>% length, length(list.dfs)))
  rownames(list.props[[x]]) <- levels(tmp[,x])
  colnames(list.props[[x]]) <- names(list.dfs)
}

names(list.props) <- colnames(tmp)

for(i in 1:ncol(tmp)){
  for(y in 1:length(list.dfs)){
    tmp.fac <- dplyr::select_if(list.dfs[[y]], is.not.num)
    list.props[[i]][,y] <- prop.table(table(tmp.fac[,i]))
  }
}

df.props <- plyr::ldply(list.props, data.frame, .id = NULL) 
lev.names <- sapply(tmp, levels) %>% unlist
lev.names["pid4"] <- "Something else"
rownames(df.props) <- lev.names
df.props <- drop.zero(df.props, digits = 3) %>%
  .[c("AN1", "OP1", "AN2", "OP2", "AN3", "OP3", "AN4", "OP4", "AN5", "OP5")]


### means of numeric columns ###

list.means <- list()
tmp2 <- dplyr::select_if(op.t1, is.numeric)

for(x in 1:ncol(tmp2)){
  list.means[[x]] <- data.frame(matrix(NA, 1, length(list.dfs)))
  colnames(list.means[[x]]) <- names(list.dfs)
}

names(list.means) <- colnames(tmp2)

for(i in 1:ncol(tmp2)){
  for(y in 1:length(list.dfs)){
    tmp.means <- dplyr::select_if(list.dfs[[y]], is.numeric)
    list.means[[i]][,y] <- mean(tmp.means[,i]) %>% round(., digits = 3)
  }
}

df.means <- plyr::ldply(list.means, data.frame, .id = NULL) %>%
  .[c("AN1", "OP1", "AN2", "OP2", "AN3", "OP3", "AN4", "OP4", "AN5", "OP5")]
rownames(df.means) <- colnames(tmp2) %>%
  gsub("\\.", " ", .) %>%
  tools::toTitleCase(.)


### now education ###

an.ed.t1 <- df.an[df.an$id %in% assigned.an$assg[[1]][,1],] %>% subset(., select=c(education))
an.ed.t2 <- df.an[df.an$id %in% assigned.an$assg[[1]][,2],] %>% subset(., select=c(education))
an.ed.t3 <- df.an[df.an$id %in% assigned.an$assg[[1]][,3],] %>% subset(., select=c(education))
an.ed.t4 <- df.an[df.an$id %in% assigned.an$assg[[1]][,4],] %>% subset(., select=c(education))
an.ed.t5 <- df.an[df.an$id %in% assigned.an$assg[[1]][,5],] %>% subset(., select=c(education))

list.eds.an <- list(an.ed.t1, an.ed.t2, an.ed.t3, an.ed.t4, an.ed.t5)

df.ed.an <- data.frame(matrix(NA, levels(an.ed.t1[,1]) %>% length, length(list.eds.an)))
rownames(df.ed.an) <- levels(an.ed.t1[,1])
colnames(df.ed.an) <- an

for(i in 1:ncol(an.ed.t1)){
  for(y in 1:length(list.eds.an)){
    df.ed.an[,y] <- prop.table(table(list.eds.an[[y]][,i]))
  }
}

df.ed.an <- drop.zero(df.ed.an, digits = 3)

op.ed.t1 <- df.op[df.op$id %in% assigned.op$assg[[1]][,1],] %>% subset(., select=c(education))
op.ed.t2 <- df.op[df.op$id %in% assigned.op$assg[[1]][,2],] %>% subset(., select=c(education))
op.ed.t3 <- df.op[df.op$id %in% assigned.op$assg[[1]][,3],] %>% subset(., select=c(education))
op.ed.t4 <- df.op[df.op$id %in% assigned.op$assg[[1]][,4],] %>% subset(., select=c(education))
op.ed.t5 <- df.op[df.op$id %in% assigned.op$assg[[1]][,5],] %>% subset(., select=c(education))

list.eds.op <- list(op.ed.t1, op.ed.t2, op.ed.t3, op.ed.t4, op.ed.t5)
df.ed.op <- data.frame(matrix(NA, levels(op.ed.t1[,1]) %>% length, length(list.eds.op)))
rownames(df.ed.op) <- levels(op.ed.t1[,1])
colnames(df.ed.op) <- op

for(i in 1:ncol(op.ed.t1)){
  for(y in 1:length(list.eds.op)){
    df.ed.op[,y] <- prop.table(table(list.eds.op[[y]][,i]))
  }
}

df.ed.op <- drop.zero(df.ed.op, digits = 3)
top <- data.frame(matrix(NA, 8, length(list.eds.op)))
colnames(top) <- colnames(df.ed.op)
rownames(top) <- rownames(df.ed.an)[1:8]
bottom <- data.frame(matrix(NA, 2, length(list.eds.op)))
colnames(bottom) <- colnames(top)
rownames(bottom) <- rownames(df.ed.an)[14:15]
df.ed.all <-rbind(top, df.ed.op, bottom) %>% cbind(df.ed.an, .) %>%
  .[c("AN1", "OP1", "AN2", "OP2", "AN3", "OP3", "AN4", "OP4", "AN5", "OP5")]

### combine and print (and manually adjust) factors, numerics, education ###

df.all <- rbind(df.props, df.means, df.ed.all)
tab <- stargazer(df.all, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories, Differentiated by Treatment Group",
                 label = "education-blocked")
st <- gsub("{c}", "{l}", tab, fixed = TRUE)
cat(st)

```


\begin{table}[!htbp] \centering    
\caption{Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories, Differentiated by Treatment Group}
\label{education-blocked} 
\resizebox{11.5cm}{!}{%
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }  
\\[-1.8ex]\hline  
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{Group 1} & \multicolumn{2}{c}{Group 2} & \multicolumn{2}{c}{Group 3} & \multicolumn{2}{c}{Group 4} & \multicolumn{2}{c}{Group 5}\\ 
\hline \\[-1.8ex]  
\multicolumn{1}{l}{} & \multicolumn{1}{l}{AN} & \multicolumn{1}{l}{OP} & \multicolumn{1}{l}{AN} & \multicolumn{1}{l}{OP} & \multicolumn{1}{l}{AN} & \multicolumn{1}{l}{OP} & \multicolumn{1}{l}{AN} & \multicolumn{1}{l}{OP} & \multicolumn{1}{l}{AN} & \multicolumn{1}{l}{OP} \\  
\cline{2-3}
\cline{4-5}
\cline{6-7}
\cline{8-9}
\cline{10-11} \\[-1.8ex]
\multicolumn{1}{l}{\textbf{Gender}} & & & & & & & & & & \\  
\multicolumn{1}{l}{Male} & \multicolumn{1}{l}{.467} & \multicolumn{1}{l}{.468} & \multicolumn{1}{l}{.473} & \multicolumn{1}{l}{.481} & \multicolumn{1}{l}{.475} & \multicolumn{1}{l}{.463} & \multicolumn{1}{l}{.463} & \multicolumn{1}{l}{.468} & \multicolumn{1}{l}{.476} & \multicolumn{1}{l}{.473} \\  
\multicolumn{1}{l}{Female} & \multicolumn{1}{l}{.533} & \multicolumn{1}{l}{.529} & \multicolumn{1}{l}{.524} & \multicolumn{1}{l}{.514} & \multicolumn{1}{l}{.522} & \multicolumn{1}{l}{.535} & \multicolumn{1}{l}{.533} & \multicolumn{1}{l}{.532} & \multicolumn{1}{l}{.521} & \multicolumn{1}{l}{.524} \\  
\multicolumn{1}{l}{Other} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{.005} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{.002} & 
\multicolumn{1}{l}{.003} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{.003} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Race}} & & & & & & & & & & \\  
\multicolumn{1}{l}{White} & \multicolumn{1}{l}{.763} & \multicolumn{1}{l}{.751} & \multicolumn{1}{l}{.770} & \multicolumn{1}{l}{.778} & \multicolumn{1}{l}{.752} & \multicolumn{1}{l}{.743} & \multicolumn{1}{l}{.749} & \multicolumn{1}{l}{.746} & \multicolumn{1}{l}{.752} & \multicolumn{1}{l}{.770} \\  
\multicolumn{1}{l}{African American} & \multicolumn{1}{l}{.095} & \multicolumn{1}{l}{.089} & \multicolumn{1}{l}{.090} & \multicolumn{1}{l}{.089} & \multicolumn{1}{l}{.106} & \multicolumn{1}{l}{.100} & \multicolumn{1}{l}{.094} & \multicolumn{1}{l}{.117} & \multicolumn{1}{l}{.110} & \multicolumn{1}{l}{.100} \\  
\multicolumn{1}{l}{Asian} & \multicolumn{1}{l}{.032} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.041} & \multicolumn{1}{l}{.027} & \multicolumn{1}{l}{.027} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.027} & \multicolumn{1}{l}{.032} & \multicolumn{1}{l}{.030} & \multicolumn{1}{l}{.029} \\  
\multicolumn{1}{l}{Native American} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{.005} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{.006} & \multicolumn{1}{l}{.006} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{.006} & \multicolumn{1}{l}{.008} \\  
\multicolumn{1}{l}{Hispanic} & \multicolumn{1}{l}{.102} & \multicolumn{1}{l}{.125} & \multicolumn{1}{l}{.095} & \multicolumn{1}{l}{.102} & \multicolumn{1}{l}{.111} & \multicolumn{1}{l}{.116} & \multicolumn{1}{l}{.124} & \multicolumn{1}{l}{.097} & \multicolumn{1}{l}{.102} & \multicolumn{1}{l}{.094} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Income}} & & & & & & & & & & \\  
\multicolumn{1}{l}{Under 25,000} & \multicolumn{1}{l}{.206} & \multicolumn{1}{l}{.203} & \multicolumn{1}{l}{.198} & \multicolumn{1}{l}{.219} & \multicolumn{1}{l}{.224} & \multicolumn{1}{l}{.206} & \multicolumn{1}{l}{.216} & \multicolumn{1}{l}{.211} & \multicolumn{1}{l}{.211} & \multicolumn{1}{l}{.216} \\  
\multicolumn{1}{l}{25,000-49,999} & \multicolumn{1}{l}{.246} & \multicolumn{1}{l}{.237} & \multicolumn{1}{l}{.246} & \multicolumn{1}{l}{.205} & \multicolumn{1}{l}{.190} & \multicolumn{1}{l}{.225} & \multicolumn{1}{l}{.211} & \multicolumn{1}{l}{.206} & \multicolumn{1}{l}{.217} & \multicolumn{1}{l}{.238} \\ 
\multicolumn{1}{l}{50,000-749,999} & \multicolumn{1}{l}{.167} & \multicolumn{1}{l}{.175} & \multicolumn{1}{l}{.187} & \multicolumn{1}{l}{.178} & \multicolumn{1}{l}{.210} & \multicolumn{1}{l}{.173} & \multicolumn{1}{l}{.189} & \multicolumn{1}{l}{.208} & \multicolumn{1}{l}{.162} & \multicolumn{1}{l}{.181} \\  
\multicolumn{1}{l}{75,000-99,999} & \multicolumn{1}{l}{.119} & \multicolumn{1}{l}{.135} & \multicolumn{1}{l}{.148} & \multicolumn{1}{l}{.133} & \multicolumn{1}{l}{.130} & \multicolumn{1}{l}{.151} & \multicolumn{1}{l}{.133} & \multicolumn{1}{l}{.127} & \multicolumn{1}{l}{.135} & \multicolumn{1}{l}{.119} \\  
\multicolumn{1}{l}{100,000-124,999} & \multicolumn{1}{l}{.102} & \multicolumn{1}{l}{.090} & \multicolumn{1}{l}{.094} & \multicolumn{1}{l}{.095} & \multicolumn{1}{l}{.094} & \multicolumn{1}{l}{.092} & \multicolumn{1}{l}{.079} & \multicolumn{1}{l}{.087} & \multicolumn{1}{l}{.089} & \multicolumn{1}{l}{.092} \\  
\multicolumn{1}{l}{125,000-149,999} & \multicolumn{1}{l}{.038} & \multicolumn{1}{l}{.040} & \multicolumn{1}{l}{.033} & \multicolumn{1}{l}{.052} & \multicolumn{1}{l}{.040} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.049} & \multicolumn{1}{l}{.041} & \multicolumn{1}{l}{.052} & \multicolumn{1}{l}{.044} \\  
\multicolumn{1}{l}{150,000-174,999} & \multicolumn{1}{l}{.046} & \multicolumn{1}{l}{.038} & \multicolumn{1}{l}{.030} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.041} & \multicolumn{1}{l}{.038} & \multicolumn{1}{l}{.043} & \multicolumn{1}{l}{.040} & \multicolumn{1}{l}{.044} & \multicolumn{1}{l}{.054} \\  
\multicolumn{1}{l}{175,000 or more} & \multicolumn{1}{l}{.076} & \multicolumn{1}{l}{.083} & \multicolumn{1}{l}{.063} & \multicolumn{1}{l}{.083} & \multicolumn{1}{l}{.071} & \multicolumn{1}{l}{.079} & \multicolumn{1}{l}{.079} & \multicolumn{1}{l}{.079} & \multicolumn{1}{l}{.089} & \multicolumn{1}{l}{.056} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Employment}} & & & & & & & & & & \\  
\multicolumn{1}{l}{Working} & \multicolumn{1}{l}{.646} & \multicolumn{1}{l}{.646} & \multicolumn{1}{l}{.649} & \multicolumn{1}{l}{.608} & \multicolumn{1}{l}{.600} & \multicolumn{1}{l}{.652} & \multicolumn{1}{l}{.624} & \multicolumn{1}{l}{.613} & \multicolumn{1}{l}{.622} & \multicolumn{1}{l}{.622} \\  
\multicolumn{1}{l}{Unemployed} & \multicolumn{1}{l}{.056} & \multicolumn{1}{l}{.065} & \multicolumn{1}{l}{.057} & \multicolumn{1}{l}{.054} & \multicolumn{1}{l}{.063} & \multicolumn{1}{l}{.051} & \multicolumn{1}{l}{.051} & \multicolumn{1}{l}{.059} & \multicolumn{1}{l}{.075} & \multicolumn{1}{l}{.073} \\  
\multicolumn{1}{l}{Retired} & \multicolumn{1}{l}{.183} & \multicolumn{1}{l}{.181} & \multicolumn{1}{l}{.184} & \multicolumn{1}{l}{.213} & \multicolumn{1}{l}{.203} & \multicolumn{1}{l}{.179} & \multicolumn{1}{l}{.214} & \multicolumn{1}{l}{.210} & \multicolumn{1}{l}{.192} & \multicolumn{1}{l}{.194} \\  
\multicolumn{1}{l}{Disabled} & \multicolumn{1}{l}{.033} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.043} & \multicolumn{1}{l}{.046} & \multicolumn{1}{l}{.051} & \multicolumn{1}{l}{.037} & \multicolumn{1}{l}{.027} & \multicolumn{1}{l}{.033} & \multicolumn{1}{l}{.038} & \multicolumn{1}{l}{.041} \\  
\multicolumn{1}{l}{Homemaker} & \multicolumn{1}{l}{.049} & \multicolumn{1}{l}{.049} & \multicolumn{1}{l}{.052} & \multicolumn{1}{l}{.054} & \multicolumn{1}{l}{.052} & \multicolumn{1}{l}{.052} & \multicolumn{1}{l}{.060} & \multicolumn{1}{l}{.063} & \multicolumn{1}{l}{.054} & \multicolumn{1}{l}{.049} \\  
\multicolumn{1}{l}{Student} & \multicolumn{1}{l}{.033} & \multicolumn{1}{l}{.024} & \multicolumn{1}{l}{.014} & \multicolumn{1}{l}{.025} & \multicolumn{1}{l}{.030} & \multicolumn{1}{l}{.029} & \multicolumn{1}{l}{.024} & \multicolumn{1}{l}{.022} & \multicolumn{1}{l}{.019} & \multicolumn{1}{l}{.021} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Party ID}} & & & & & & & & & & \\  
\multicolumn{1}{l}{Democrat} & \multicolumn{1}{l}{.314} & \multicolumn{1}{l}{.378} & \multicolumn{1}{l}{.367} & \multicolumn{1}{l}{.348} & \multicolumn{1}{l}{.338} & \multicolumn{1}{l}{.341} & \multicolumn{1}{l}{.378} & \multicolumn{1}{l}{.344} & \multicolumn{1}{l}{.378} & \multicolumn{1}{l}{.363} \\  
\multicolumn{1}{l}{Republican} & \multicolumn{1}{l}{.279} & \multicolumn{1}{l}{.292} & \multicolumn{1}{l}{.279} & \multicolumn{1}{l}{.294} & \multicolumn{1}{l}{.317} & \multicolumn{1}{l}{.281} & \multicolumn{1}{l}{.308} & \multicolumn{1}{l}{.321} & \multicolumn{1}{l}{.295} & \multicolumn{1}{l}{.292} \\  
\multicolumn{1}{l}{Independent} & \multicolumn{1}{l}{.362} & \multicolumn{1}{l}{.300} & \multicolumn{1}{l}{.330} & \multicolumn{1}{l}{.324} & \multicolumn{1}{l}{.311} & \multicolumn{1}{l}{.333} & \multicolumn{1}{l}{.284} & \multicolumn{1}{l}{.302} & \multicolumn{1}{l}{.292} & \multicolumn{1}{l}{.321} \\  
\multicolumn{1}{l}{Something else} & \multicolumn{1}{l}{.044} & \multicolumn{1}{l}{.030} & \multicolumn{1}{l}{.024} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.033} & \multicolumn{1}{l}{.044} & \multicolumn{1}{l}{.030} & \multicolumn{1}{l}{.033} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.024} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{President}} & & & & & & & & & & \\  
\multicolumn{1}{l}{Approve} & \multicolumn{1}{l}{.551} & \multicolumn{1}{l}{.533} & \multicolumn{1}{l}{.514} & \multicolumn{1}{l}{.524} & \multicolumn{1}{l}{.502} & \multicolumn{1}{l}{.535} & \multicolumn{1}{l}{.522} & \multicolumn{1}{l}{.522} & \multicolumn{1}{l}{.544} & \multicolumn{1}{l}{.519} \\  
\multicolumn{1}{l}{Disapprove} & \multicolumn{1}{l}{.449} & \multicolumn{1}{l}{.467} & \multicolumn{1}{l}{.486} & \multicolumn{1}{l}{.476} & \multicolumn{1}{l}{.498} & \multicolumn{1}{l}{.465} & \multicolumn{1}{l}{.478} & \multicolumn{1}{l}{.478} & \multicolumn{1}{l}{.456} & \multicolumn{1}{l}{.481} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Min. Wage}} & & & & & & & & & & \\  
\multicolumn{1}{l}{Raised} & \multicolumn{1}{l}{.648} & \multicolumn{1}{l}{.635} & \multicolumn{1}{l}{.621} & \multicolumn{1}{l}{.630} & \multicolumn{1}{l}{.644} & \multicolumn{1}{l}{.662} & \multicolumn{1}{l}{.619} & \multicolumn{1}{l}{.637} & \multicolumn{1}{l}{.681} & \multicolumn{1}{l}{.649} \\  
\multicolumn{1}{l}{Kept the same} & \multicolumn{1}{l}{.302} & \multicolumn{1}{l}{.303} & \multicolumn{1}{l}{.302} & \multicolumn{1}{l}{.289} & \multicolumn{1}{l}{.308} & \multicolumn{1}{l}{.273} & \multicolumn{1}{l}{.310} & \multicolumn{1}{l}{.308} & \multicolumn{1}{l}{.251} & \multicolumn{1}{l}{.298} \\  
\multicolumn{1}{l}{Lowered} & \multicolumn{1}{l}{.019} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{.027} & \multicolumn{1}{l}{.029} & \multicolumn{1}{l}{.013} & \multicolumn{1}{l}{.027} & \multicolumn{1}{l}{.025} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{.025} & \multicolumn{1}{l}{.013} \\  
\multicolumn{1}{l}{Eliminated} & \multicolumn{1}{l}{.032} & \multicolumn{1}{l}{.041} & \multicolumn{1}{l}{.051} & \multicolumn{1}{l}{.052} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.038} & \multicolumn{1}{l}{.046} & \multicolumn{1}{l}{.035} & \multicolumn{1}{l}{.043} & \multicolumn{1}{l}{.040} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Country}} & & & & & & & & & & \\  
\multicolumn{1}{l}{Right direction} & \multicolumn{1}{l}{.290} & \multicolumn{1}{l}{.292} & \multicolumn{1}{l}{.252} & \multicolumn{1}{l}{.251} & \multicolumn{1}{l}{.249} & \multicolumn{1}{l}{.252} & \multicolumn{1}{l}{.267} & \multicolumn{1}{l}{.265} & \multicolumn{1}{l}{.268} & \multicolumn{1}{l}{.267} \\  
\multicolumn{1}{l}{Wrong track} & \multicolumn{1}{l}{.710} & \multicolumn{1}{l}{.708} & \multicolumn{1}{l}{.748} & \multicolumn{1}{l}{.749} & \multicolumn{1}{l}{.751} & \multicolumn{1}{l}{.748} & \multicolumn{1}{l}{.733} & \multicolumn{1}{l}{.735} & \multicolumn{1}{l}{.732} & \multicolumn{1}{l}{.733} \\ 
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Age}} & \multicolumn{1}{l}{48.548} & \multicolumn{1}{l}{48.957} & \multicolumn{1}{l}{48.949} & \multicolumn{1}{l}{49.54} & \multicolumn{1}{l}{48.811} & \multicolumn{1}{l}{47.157} & \multicolumn{1}{l}{49.702} & \multicolumn{1}{l}{49.773} & \multicolumn{1}{l}{49.1} & \multicolumn{1}{l}{49.683} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Feel Trump}} & \multicolumn{1}{l}{34.695} & \multicolumn{1}{l}{34.378} & \multicolumn{1}{l}{37.278} & \multicolumn{1}{l}{36.394} & \multicolumn{1}{l}{40.579} & \multicolumn{1}{l}{36.241} & \multicolumn{1}{l}{35.767} & \multicolumn{1}{l}{38.584} & \multicolumn{1}{l}{34.59} & \multicolumn{1}{l}{37.313} \\  
 & & & & & & & & & & \\
\multicolumn{1}{l}{\textbf{Education}} & & & & & & & & & & \\  
\multicolumn{1}{l}{Up to 1st} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.002} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{1st-4th} & \multicolumn{1}{l}{.002} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{5th-6th} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{7th-8th} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.005} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.005} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.003} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{9th} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.010} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{10th} & \multicolumn{1}{l}{.010} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.010} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{11th} & \multicolumn{1}{l}{.013} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.014} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.014} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.014} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.014} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{12th} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{HS grad} & \multicolumn{1}{l}{.190} & \multicolumn{1}{l}{.068} & \multicolumn{1}{l}{.190} & \multicolumn{1}{l}{.067} & \multicolumn{1}{l}{.190} & \multicolumn{1}{l}{.068} & \multicolumn{1}{l}{.190} & \multicolumn{1}{l}{.068} & \multicolumn{1}{l}{.190} & \multicolumn{1}{l}{.067} \\  
\multicolumn{1}{l}{Some college} & \multicolumn{1}{l}{.214} & \multicolumn{1}{l}{.389} & \multicolumn{1}{l}{.214} & \multicolumn{1}{l}{.390} & \multicolumn{1}{l}{.216} & \multicolumn{1}{l}{.389} & \multicolumn{1}{l}{.214} & \multicolumn{1}{l}{.390} & \multicolumn{1}{l}{.214} & \multicolumn{1}{l}{.392} \\  
\multicolumn{1}{l}{Associate} & \multicolumn{1}{l}{.138} & \multicolumn{1}{l}{.240} & \multicolumn{1}{l}{.138} & \multicolumn{1}{l}{.238} & \multicolumn{1}{l}{.137} & \multicolumn{1}{l}{.240} & \multicolumn{1}{l}{.138} & \multicolumn{1}{l}{.238} & \multicolumn{1}{l}{.138} & \multicolumn{1}{l}{.238} \\  
\multicolumn{1}{l}{Bachelor's} & \multicolumn{1}{l}{.233} & \multicolumn{1}{l}{.265} & \multicolumn{1}{l}{.233} & \multicolumn{1}{l}{.265} & \multicolumn{1}{l}{.233} & \multicolumn{1}{l}{.263} & \multicolumn{1}{l}{.233} & \multicolumn{1}{l}{.265} & \multicolumn{1}{l}{.233} & \multicolumn{1}{l}{.265} \\  
\multicolumn{1}{l}{Master's} & \multicolumn{1}{l}{.121} & \multicolumn{1}{l}{.038} & \multicolumn{1}{l}{.121} & \multicolumn{1}{l}{.040} & \multicolumn{1}{l}{.122} & \multicolumn{1}{l}{.040} & \multicolumn{1}{l}{.121} & \multicolumn{1}{l}{.038} & \multicolumn{1}{l}{.122} & \multicolumn{1}{l}{.038} \\  
\multicolumn{1}{l}{Professional} & \multicolumn{1}{l}{.022} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.024} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} \\  
\multicolumn{1}{l}{Doctorate} & \multicolumn{1}{l}{.022} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.022} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.022} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.021} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{.022} & \multicolumn{1}{l}{} \\ 
\hline \\[-1.8ex]  
\end{tabular}}
\end{table}

\clearpage


```{r Variance-In-Treatment-Groups, include=FALSE}

##### compare treatment groups in AN with treatment groups in OP #####

list.dfs.var <- list.dfs # so I don't mess with the output from above

for(x in 1:(length(list.dfs)/2)){
  list.dfs.var[[x]]$cat <- rep(paste0("AN", x), nrow(list.dfs.var[[x]])) %>% as.factor
  list.dfs.var[[x+5]]$cat <- rep(paste0("OP", x), nrow(list.dfs.var[[x+5]])) %>% as.factor
}

list.dfs.t <- list()

for(x in 1:(length(list.dfs)/2)){
  list.dfs.t[[x]] <- c(list.dfs.var[x], list.dfs.var[x+5]) %>%
    ldply(., data.frame) %>% subset(., select =-c(.id))
}

num.res <- rep(list(list()), 2)
names(num.res) <- c("aov.sum", "aov.tukey")
list.dfs.num.aov <- rep(list(num.res), 5)
names(list.dfs.num.aov) <- c("T1", "T2", "T3", "T4", "T5")

for(t in 1:length(list.dfs.num.aov)){
  for(w in 1:ncol(dplyr::select_if(list.dfs.t[[1]], is.numeric))){
    aa <- dplyr::select_if(list.dfs.t[[t]], is.numeric)
    bb <- cbind(aa, list.dfs.t[[t]]$cat)
    colnames(bb) <- c(colnames(aa), "cat")
    aov.out <- aov(bb[,w] ~ bb$cat)
    list.dfs.num.aov[[t]]$aov.sum[[w]] <- summary(aov.out)
    names(list.dfs.num.aov[[t]]$aov.sum)[[w]] <- colnames(bb)[w]
    list.dfs.num.aov[[t]]$aov.tukey[[w]] <- TukeyHSD(aov.out)
    names(list.dfs.num.aov[[t]]$aov.tukey)[[w]] <- colnames(bb)[w]
    # num.aov.sum[[w]] <- summary(aov.out)
    # names(num.aov.sum)[[w]] <- paste0(colnames(bb)[w], ".sum")
    # num.aov.tukey[[w]] <- TukeyHSD(aov.out)
    # names(num.aov.tukey)[[w]] <- paste0(colnames(bb)[w], ".tukey")
  }
}

fac.res <- rep(list(list()), 2)
names(fac.res) <- c("glm.sum", "glm.anov")
list.dfs.fac.aov <- rep(list(fac.res), 5)
treats <- c("T1", "T2", "T3", "T4", "T5")
names(list.dfs.fac.aov) <- treats

for(t in 1:length(list.dfs.num.aov)){
  for(w in 1:(ncol(dplyr::select_if(list.dfs.t[[1]], is.not.num))-1)){
    my.dat <- dplyr::select_if(list.dfs.t[[t]], is.not.num)
    my.mod <- glm(my.dat[,w] ~ my.dat$cat, family = "binomial")
    list.dfs.fac.aov[[t]]$glm.sum[[w]] <- summary(my.mod)
    names(list.dfs.fac.aov[[t]]$glm.sum)[[w]] <- colnames(my.dat)[w]
    list.dfs.fac.aov[[t]]$glm.anov[[w]] <- anova(my.mod, test = "Chisq")
    names(list.dfs.fac.aov[[t]]$glm.anov)[[w]] <- colnames(my.dat)[w]
  }
}

# These commented out lines are just for me as an overview how all the lists are structured and how to get the numbers as a df
# Lists of length 5, named T1 to T5
# list.dfs.num.aov %>% length
# list.dfs.num.aov %>% names
# list.dfs.fac.aov %>% length
# list.dfs.fac.aov %>% names
# 
# Lists of length 2, named aov.sum, aov.tukey and glm.sum, glm.anov
# list.dfs.num.aov[["T1"]] %>% length
# list.dfs.num.aov[["T1"]] %>% names
# list.dfs.fac.aov[["T1"]] %>% length
# list.dfs.fac.aov[["T1"]] %>% names
# 
# Lists of lengths 2 and 8, named after variables (age, feel.trump and gender, race, income, occupation, pid, pres.approv, min.wage, country.track)
# list.dfs.num.aov[["T1"]][["aov.sum"]] %>% length
# list.dfs.num.aov[["T1"]][["aov.sum"]] %>% names
# list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% length
# list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% names
# 
# Classes and names of each output
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]] %>% class # summary.aov, listof
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]] %>% names # NULL
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]] %>% class # summary.glm
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]] %>% names # lots of useful names
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]] %>% class # TukeyHSD, multicomp
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]] %>% names # bb$cat
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]] %>% class # anova, data.frame
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]] %>% names # lots of names
#
# How to transform each output into a data frame
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]][[1]] %>% data.frame
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]]$coefficients %>% data.frame
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]]$`bb$cat` %>% data.frame
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]][2,] %>% data.frame


# Store the output of each method for all treatment groups, so 4 data frames where each df contains all groups
aov.names <- list.dfs.num.aov[["T1"]][["aov.sum"]] %>% names # could have also used aov.tukey here
aov.length <- list.dfs.num.aov[["T1"]][["aov.sum"]] %>% length
glm.names <- list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% names # could have also used glm.anov here
glm.length <- list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% length # could have also used glm.anov here

list.aov.sum <- list.aov.tukey <- rep(list(list()), length(aov.names))
list.glm.sum <- list.glm.anov <- rep(list(list()), length(glm.names))

for(i in 1:length(treats)){
  tmp.aov.sum <- list.dfs.num.aov[[i]][["aov.sum"]]
  tmp.aov.tukey <- list.dfs.num.aov[[i]][["aov.tukey"]]
  for(x in 1:aov.length){ # this loop does tmp.aov.sum and tmp.aov.tukey
    tmp.sum <- tmp.aov.sum[[names(tmp.aov.sum)[[x]]]][[1]] %>% data.frame
    rownames(tmp.sum) <- c(paste0(names(tmp.aov.sum)[[x]], treats[i]),
                           paste0(names(tmp.aov.sum)[[x]], treats[i], "Resid"))
    list.aov.sum[[x]][[i]] <- tmp.sum
    tmp.tukey <- tmp.aov.tukey[[names(tmp.aov.tukey)[[x]]]]$`bb$cat` %>% data.frame
    rownames(tmp.tukey) <- paste0(names(tmp.aov.tukey)[[x]], treats[i], rownames(tmp.tukey))
    list.aov.tukey[[x]][[i]] <- tmp.tukey
  }
  tmp.glm.sum <- list.dfs.fac.aov[[i]][["glm.sum"]]
  tmp.glm.anov <- list.dfs.fac.aov[[i]][["glm.anov"]]
  for(y in 1:glm.length){ # this loop does tmp.glm.sum and tmp.glm.anov
    tmp.sum <- tmp.glm.sum[[names(tmp.glm.sum)[[y]]]]$coefficients %>% data.frame
    rownames(tmp.sum) <- c(paste0(names(tmp.glm.sum)[[y]], treats[i], "Int"),
                           paste0(names(tmp.glm.sum)[[y]], treats[i]))
    list.glm.sum[[y]][[i]] <- tmp.sum
    tmp.anov <- tmp.glm.anov[[names(tmp.glm.anov)[[y]]]][2,] %>% data.frame
    rownames(tmp.anov) <- paste0(names(tmp.glm.anov)[[y]], treats[i])
    list.glm.anov[[y]][[i]] <- tmp.anov
  }
}

df.aov.sum <- unlist(list.aov.sum, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.aov.sum <- cbind(df.aov.sum[, 1:3] , drop.zero(df.aov.sum[, 4:5], digits = 3))
df.aov.tukey <- unlist(list.aov.tukey, recursive = FALSE) %>% do.call("rbind", .) %>% drop.zero(., digits = 3)
df.glm.sum <- unlist(list.glm.sum, recursive = FALSE) %>% do.call("rbind", .) %>% drop.zero(., digits = 3)
df.glm.anov <- unlist(list.glm.anov, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.glm.anov <- cbind(df.glm.anov[, c(1,3,4)], drop.zero(df.glm.anov[, c(2, 5)], digits = 3)) %>% .[,c(1,4,2,3,5)]

colnames(df.aov.sum) <- c("Df", "Sum.Sq", "Mean.Sq", "F-value", "p-value")
colnames(df.aov.tukey) <- c("Diff.Means", "CI Lower", "CI Upper", "Adj. p-value")
colnames(df.glm.sum) <- c("Estimate", "Std.Error", "z-value", "p-value")
colnames(df.glm.anov) <- c("Df", "Deviance", "Resid.Df", "Residual Deviance", "Pr.Chi")

tab.aov.sum <- stargazer(df.aov.sum, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Summary of ANOVA Regression of Variable on ANES/OP Indicator, Differentiated by Treatment Group",
                 label = "aov-sum")
at <- gsub("{c}", "{l}", tab.aov.sum, fixed = TRUE) %>%
  gsub("age", "", ., fixed = TRUE) %>%
  gsub("feel.trump", "", ., fixed = TRUE) %>%
  gsub("Resid", " Residuals", ., fixed = TRUE)
cat(at)

tab.aov.tukey <- stargazer(df.aov.tukey, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Tukey's 'Honest Significant Difference' Test of ANOVA Regression of Variable on ANES/OP Indicator, Differentiated by Treatment Group",
                 label = "aov-tukey")
bt <- gsub("{c}", "{l}", tab.aov.tukey, fixed = TRUE) %>%
  gsub("age", "", ., fixed = TRUE) %>%
  gsub("feel.trump", "", ., fixed = TRUE) %>%
  gsub("OP", " OP", ., fixed = TRUE)
cat(bt)

tab.glm.sum <- stargazer(df.glm.sum, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Summary of GLM Regression of Variable on ANES/OP Indicator, Differentiated by Treatment Group",
                 label = "glm-sum")
ct <- gsub("{c}", "{l}", tab.glm.sum, fixed = TRUE) %>%
  gsub("gender", "", ., fixed = TRUE) %>%
  gsub("race", "", ., fixed = TRUE) %>%
  gsub("income", "", ., fixed = TRUE) %>%
  gsub("occupation", "", ., fixed = TRUE) %>%
  gsub("pid", "", ., fixed = TRUE) %>%
  gsub("pres.approv", "", ., fixed = TRUE) %>%
  gsub("min.wage", "", ., fixed = TRUE) %>%
  gsub("country.track", "", ., fixed = TRUE) %>%
  gsub("Int", " Intercept", ., fixed = TRUE)
cat(ct)

tab.glm.anov <- stargazer(df.glm.anov, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "ANOVA Chisq Test of GLM Regression of Variable on ANES/OP Indicator, Differentiated by Treatment Group",
                 label = "glm-anov")
dt <- gsub("{c}", "{l}", tab.glm.anov, fixed = TRUE) %>%
  gsub("gender", "", ., fixed = TRUE) %>%
  gsub("race", "", ., fixed = TRUE) %>%
  gsub("income", "", ., fixed = TRUE) %>%
  gsub("occupation", "", ., fixed = TRUE) %>%
  gsub("pid", "", ., fixed = TRUE) %>%
  gsub("pres.approv", "", ., fixed = TRUE) %>%
  gsub("min.wage", "", ., fixed = TRUE) %>%
  gsub("country.track", "", ., fixed = TRUE)
cat(dt)


```


\begin{table}[!htbp] \centering    
\caption{Summary of ANOVA Regression of Variable on ANES/OP Indicator, Differentiated by Treatment Group}    
\label{aov-sum}  
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }  
\\[-1.8ex]\hline
\hline \\[-1.8ex]  
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Df} & \multicolumn{1}{l}{Sum.Sq} & \multicolumn{1}{l}{Mean.Sq} & \multicolumn{1}{l}{F-value} & \multicolumn{1}{l}{p-value} \\  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{\textbf{Age}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & 52.830 & 52.830 & .170 & .680 \\  
\multicolumn{1}{l}{T1 Residuals} & 1,258 & 384,359.900 & 305.530 &  &  \\  
\multicolumn{1}{l}{T2} & 1 & 109.830 & 109.830 & .360 & .550 \\  
\multicolumn{1}{l}{T2 Residuals} & 1,258 & 388,724.900 & 309 &  &  \\  
\multicolumn{1}{l}{T3} & 1 & 861.720 & 861.720 & 2.810 & .090 \\  
\multicolumn{1}{l}{T3 Residuals} & 1,258 & 386,400.000 & 307.150 &  &  \\  
\multicolumn{1}{l}{T4} & 1 & 1.610 & 1.610 & .010 & .940 \\  
\multicolumn{1}{l}{T4 Residuals} & 1,258 & 387,498.400 & 308.030 &  &  \\  
\multicolumn{1}{l}{T5} & 1 & 106.900 & 106.900 & .350 & .550 \\  
\multicolumn{1}{l}{T5 Residuals} & 1,258 & 382,695.200 & 304.210 &  &  \\
 & & & & & \\
\multicolumn{1}{l}{\textbf{Feel Trump}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & 31.750 & 31.750 & .030 & .870 \\  
\multicolumn{1}{l}{T1 Residuals} & 1,258 & 1,465,958.000 & 1,165.310 &  &  \\  
\multicolumn{1}{l}{T2} & 1 & 246.230 & 246.230 & .200 & .660 \\  
\multicolumn{1}{l}{T2 Residuals} & 1,258 & 1,576,959.000 & 1,253.540 &  &  \\  
\multicolumn{1}{l}{T3} & 1 & 5,928.010 & 5,928.010 & 4.870 & .030 \\  
\multicolumn{1}{l}{T3 Residuals} & 1,258 & 1,530,847.000 & 1,216.890 &  &  \\  
\multicolumn{1}{l}{T4} & 1 & 2,500.500 & 2,500.500 & 2.040 & .150 \\  
\multicolumn{1}{l}{T4 Residuals} & 1,258 & 1,539,114.000 & 1,223.460 &  &  \\  
\multicolumn{1}{l}{T5} & 1 & 2,334.310 & 2,334.310 & 1.890 & .170 \\  
\multicolumn{1}{l}{T5 Residuals} & 1,258 & 1,550,046.000 & 1,232.150 &  &  \\  
\hline \\[-1.8ex]  
\end{tabular}  
\end{table} 



\begin{table}[!htbp] \centering    
\caption{Tukey's `Honest Significant Difference' Test of ANOVA Regression of Variable on ANES/OP Indicator, Differentiated by Treatment Group}    
\label{aov-tukey}  
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }  
\\[-1.8ex]\hline  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Diff.Means} & \multicolumn{1}{l}{CI Lower} & \multicolumn{1}{l}{CI Upper} & \multicolumn{1}{l}{Adj. p-value} \\  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{\textbf{Age}} & & & & \\  
\multicolumn{1}{l}{T1 OP1-AN1} & \multicolumn{1}{l}{.410} & \multicolumn{1}{l}{-1.523} & \multicolumn{1}{l}{2.342} & \multicolumn{1}{l}{.678} \\  
\multicolumn{1}{l}{T2 OP2-AN2} & \multicolumn{1}{l}{.590} & \multicolumn{1}{l}{-1.353} & \multicolumn{1}{l}{2.534} & \multicolumn{1}{l}{.551} \\  
\multicolumn{1}{l}{T3 OP3-AN3} & \multicolumn{1}{l}{-1.654} & \multicolumn{1}{l}{-3.591} & \multicolumn{1}{l}{.283} & \multicolumn{1}{l}{.094} \\  
\multicolumn{1}{l}{T4 OP4-AN4} & \multicolumn{1}{l}{.071} & \multicolumn{1}{l}{-1.869} & \multicolumn{1}{l}{2.011} & \multicolumn{1}{l}{.942} \\  
\multicolumn{1}{l}{T5 OP5-AN5} & \multicolumn{1}{l}{.583} & \multicolumn{1}{l}{-1.345} & \multicolumn{1}{l}{2.510} & \multicolumn{1}{l}{.553} \\  
 & & & & \\
\multicolumn{1}{l}{\textbf{Feel Trump}} & & & & \\  
\multicolumn{1}{l}{T1 OP1-AN1} & \multicolumn{1}{l}{-.317} & \multicolumn{1}{l}{-4.091} & \multicolumn{1}{l}{3.456} & \multicolumn{1}{l}{.869} \\  
\multicolumn{1}{l}{T2 OP2-AN2} & \multicolumn{1}{l}{-.884} & \multicolumn{1}{l}{-4.798} & \multicolumn{1}{l}{3.030} & \multicolumn{1}{l}{.658} \\  
\multicolumn{1}{l}{T3 OP3-AN3} & \multicolumn{1}{l}{-4.338} & \multicolumn{1}{l}{-8.194} & \multicolumn{1}{l}{-.482} & \multicolumn{1}{l}{.027} \\  
\multicolumn{1}{l}{T4 OP4-AN4} & \multicolumn{1}{l}{2.817} & \multicolumn{1}{l}{-1.049} & \multicolumn{1}{l}{6.684} & \multicolumn{1}{l}{.153} \\  
\multicolumn{1}{l}{T5 OP5-AN5} & \multicolumn{1}{l}{2.722} & \multicolumn{1}{l}{-1.158} & \multicolumn{1}{l}{6.602} & \multicolumn{1}{l}{.169} \\  
\hline \\[-1.8ex]  
\end{tabular}  
\end{table} 

\clearpage

\ssp

\footnotesize

\begin{longtable}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }  
\caption{Summary of GLM Regression of Variable on ANES/OP Indicator, Differentiated by Treatment Group}    
\label{glm-sum}  
\\[-1.8ex]\hline  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Estimate} & \multicolumn{1}{l}{Std.Error} & \multicolumn{1}{l}{z-value} & \multicolumn{1}{l}{p-value} \\  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{\textbf{Gender}} & & & & \\  
\multicolumn{1}{l}{T1 Intercept} & \multicolumn{1}{l}{.134} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{1.672} & \multicolumn{1}{l}{.095} \\  
\multicolumn{1}{l}{T1} & \multicolumn{1}{l}{-.006} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{-.056} & \multicolumn{1}{l}{.955} \\  
\multicolumn{1}{l}{T2 Intercept} & \multicolumn{1}{l}{.108} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{1.354} & \multicolumn{1}{l}{.176} \\  
\multicolumn{1}{l}{T2} & \multicolumn{1}{l}{-.032} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{-.282} & \multicolumn{1}{l}{.778} \\  
\multicolumn{1}{l}{T3 Intercept} & \multicolumn{1}{l}{.102} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{1.274} & \multicolumn{1}{l}{.203} \\  
\multicolumn{1}{l}{T3} & \multicolumn{1}{l}{.045} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{.395} & \multicolumn{1}{l}{.693} \\  
\multicolumn{1}{l}{T4 Intercept} & \multicolumn{1}{l}{.146} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{1.831} & \multicolumn{1}{l}{.067} \\  
\multicolumn{1}{l}{T4} & \multicolumn{1}{l}{-.019} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{-.169} & \multicolumn{1}{l}{.865} \\  
\multicolumn{1}{l}{T5 Intercept} & \multicolumn{1}{l}{.095} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{1.195} & \multicolumn{1}{l}{.232} \\  
\multicolumn{1}{l}{T5} & \multicolumn{1}{l}{.013} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{.910} \\  
 & & & & \\  
\multicolumn{1}{l}{\textbf{Race}} & & & & \\  
\multicolumn{1}{l}{T1 Intercept} & \multicolumn{1}{l}{-1.172} & \multicolumn{1}{l}{.094} & \multicolumn{1}{l}{-12.500} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T1} & \multicolumn{1}{l}{.069} & \multicolumn{1}{l}{.131} & \multicolumn{1}{l}{.526} & \multicolumn{1}{l}{.599} \\  
\multicolumn{1}{l}{T2 Intercept} & \multicolumn{1}{l}{-1.207} & \multicolumn{1}{l}{.095} & \multicolumn{1}{l}{-12.757} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T2} & \multicolumn{1}{l}{-.045} & \multicolumn{1}{l}{.135} & \multicolumn{1}{l}{-.337} & \multicolumn{1}{l}{.736} \\  
\multicolumn{1}{l}{T3 Intercept} & \multicolumn{1}{l}{-1.111} & \multicolumn{1}{l}{.092} & \multicolumn{1}{l}{-12.040} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T3} & \multicolumn{1}{l}{.050} & \multicolumn{1}{l}{.130} & \multicolumn{1}{l}{.389} & \multicolumn{1}{l}{.697} \\  
\multicolumn{1}{l}{T4 Intercept} & \multicolumn{1}{l}{-1.094} & \multicolumn{1}{l}{.092} & \multicolumn{1}{l}{-11.907} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T4} & \multicolumn{1}{l}{.017} & \multicolumn{1}{l}{.130} & \multicolumn{1}{l}{.130} & \multicolumn{1}{l}{.897} \\  
\multicolumn{1}{l}{T5 Intercept} & \multicolumn{1}{l}{-1.111} & \multicolumn{1}{l}{.092} & \multicolumn{1}{l}{-12.040} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T5} & \multicolumn{1}{l}{-.096} & \multicolumn{1}{l}{.132} & \multicolumn{1}{l}{-.727} & \multicolumn{1}{l}{.467} \\  
 & & & & \\  
\multicolumn{1}{l}{\textbf{Income}} & & & & \\  
\multicolumn{1}{l}{T1 Intercept} & \multicolumn{1}{l}{1.347} & \multicolumn{1}{l}{.098} & \multicolumn{1}{l}{13.683} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T1} & \multicolumn{1}{l}{.019} & \multicolumn{1}{l}{.140} & \multicolumn{1}{l}{.140} & \multicolumn{1}{l}{.889} \\  
\multicolumn{1}{l}{T2 Intercept} & \multicolumn{1}{l}{1.396} & \multicolumn{1}{l}{.100} & \multicolumn{1}{l}{13.976} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T2} & \multicolumn{1}{l}{-.125} & \multicolumn{1}{l}{.139} & \multicolumn{1}{l}{-.901} & \multicolumn{1}{l}{.368} \\  
\multicolumn{1}{l}{T3 Intercept} & \multicolumn{1}{l}{1.244} & \multicolumn{1}{l}{.096} & \multicolumn{1}{l}{13.010} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T3} & \multicolumn{1}{l}{.103} & \multicolumn{1}{l}{.137} & \multicolumn{1}{l}{.754} & \multicolumn{1}{l}{.451} \\  
\multicolumn{1}{l}{T4 Intercept} & \multicolumn{1}{l}{1.290} & \multicolumn{1}{l}{.097} & \multicolumn{1}{l}{13.320} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T4} & \multicolumn{1}{l}{.028} & \multicolumn{1}{l}{.138} & \multicolumn{1}{l}{.206} & \multicolumn{1}{l}{.837} \\  
\multicolumn{1}{l}{T5 Intercept} & \multicolumn{1}{l}{1.318} & \multicolumn{1}{l}{.098} & \multicolumn{1}{l}{13.503} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T5} & \multicolumn{1}{l}{-.028} & \multicolumn{1}{l}{.138} & \multicolumn{1}{l}{-.206} & \multicolumn{1}{l}{.837} \\  
 & & & & \\  
\multicolumn{1}{l}{\textbf{Occupation}} & & & & \\  
\multicolumn{1}{l}{T1 Intercept} & \multicolumn{1}{l}{-.602} & \multicolumn{1}{l}{.083} & \multicolumn{1}{l}{-7.221} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T1} & \multicolumn{1}{l}{-.000} & \multicolumn{1}{l}{.118} & \multicolumn{1}{l}{-.000} & \multicolumn{1}{l}{1.000} \\  
\multicolumn{1}{l}{T2 Intercept} & \multicolumn{1}{l}{-.616} & \multicolumn{1}{l}{.083} & \multicolumn{1}{l}{-7.373} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T2} & \multicolumn{1}{l}{.177} & \multicolumn{1}{l}{.117} & \multicolumn{1}{l}{1.515} & \multicolumn{1}{l}{.130} \\  
\multicolumn{1}{l}{T3 Intercept} & \multicolumn{1}{l}{-.405} & \multicolumn{1}{l}{.081} & \multicolumn{1}{l}{-4.986} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T3} & \multicolumn{1}{l}{-.224} & \multicolumn{1}{l}{.117} & \multicolumn{1}{l}{-1.920} & \multicolumn{1}{l}{.055} \\  
\multicolumn{1}{l}{T4 Intercept} & \multicolumn{1}{l}{-.506} & \multicolumn{1}{l}{.082} & \multicolumn{1}{l}{-6.149} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T4} & \multicolumn{1}{l}{.047} & \multicolumn{1}{l}{.116} & \multicolumn{1}{l}{.406} & \multicolumn{1}{l}{.685} \\  
\multicolumn{1}{l}{T5 Intercept} & \multicolumn{1}{l}{-.499} & \multicolumn{1}{l}{.082} & \multicolumn{1}{l}{-6.072} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T5} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{.116} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{1.000} \\  
 & & & & \\  
\multicolumn{1}{l}{\textbf{Party ID}} & & & & \\  
\multicolumn{1}{l}{T1 Intercept} & \multicolumn{1}{l}{.780} & \multicolumn{1}{l}{.086} & \multicolumn{1}{l}{9.090} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T1} & \multicolumn{1}{l}{-.281} & \multicolumn{1}{l}{.119} & \multicolumn{1}{l}{-2.366} & \multicolumn{1}{l}{.018} \\  
\multicolumn{1}{l}{T2 Intercept} & \multicolumn{1}{l}{.547} & \multicolumn{1}{l}{.083} & \multicolumn{1}{l}{6.611} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T2} & \multicolumn{1}{l}{.083} & \multicolumn{1}{l}{.118} & \multicolumn{1}{l}{.705} & \multicolumn{1}{l}{.481} \\  
\multicolumn{1}{l}{T3 Intercept} & \multicolumn{1}{l}{.672} & \multicolumn{1}{l}{.084} & \multicolumn{1}{l}{7.977} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T3} & \multicolumn{1}{l}{-.014} & \multicolumn{1}{l}{.119} & \multicolumn{1}{l}{-.119} & \multicolumn{1}{l}{.905} \\  
\multicolumn{1}{l}{T4 Intercept} & \multicolumn{1}{l}{.499} & \multicolumn{1}{l}{.082} & \multicolumn{1}{l}{6.072} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T4} & \multicolumn{1}{l}{.145} & \multicolumn{1}{l}{.117} & \multicolumn{1}{l}{1.231} & \multicolumn{1}{l}{.218} \\  
\multicolumn{1}{l}{T5 Intercept} & \multicolumn{1}{l}{.499} & \multicolumn{1}{l}{.082} & \multicolumn{1}{l}{6.072} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T5} & \multicolumn{1}{l}{.061} & \multicolumn{1}{l}{.117} & \multicolumn{1}{l}{.525} & \multicolumn{1}{l}{.600} \\  
 & & & & \\  
\multicolumn{1}{l}{\textbf{Pres. Approval}} & & & & \\  
\multicolumn{1}{l}{T1 Intercept} & \multicolumn{1}{l}{-.204} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{-2.545} & \multicolumn{1}{l}{.011} \\  
\multicolumn{1}{l}{T1} & \multicolumn{1}{l}{.070} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{.622} & \multicolumn{1}{l}{.534} \\  
\multicolumn{1}{l}{T2 Intercept} & \multicolumn{1}{l}{-.057} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{-.717} & \multicolumn{1}{l}{.473} \\  
\multicolumn{1}{l}{T2} & \multicolumn{1}{l}{-.038} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{-.338} & \multicolumn{1}{l}{.735} \\  
\multicolumn{1}{l}{T3 Intercept} & \multicolumn{1}{l}{-.006} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{-.080} & \multicolumn{1}{l}{.936} \\  
\multicolumn{1}{l}{T3} & \multicolumn{1}{l}{-.134} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{-1.184} & \multicolumn{1}{l}{.236} \\  
\multicolumn{1}{l}{T4 Intercept} & \multicolumn{1}{l}{-.089} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{-1.115} & \multicolumn{1}{l}{.265} \\  
\multicolumn{1}{l}{T4} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{.000} & \multicolumn{1}{l}{1.000} \\  
\multicolumn{1}{l}{T5 Intercept} & \multicolumn{1}{l}{-.178} & \multicolumn{1}{l}{.080} & \multicolumn{1}{l}{-2.228} & \multicolumn{1}{l}{.026} \\  
\multicolumn{1}{l}{T5} & \multicolumn{1}{l}{.102} & \multicolumn{1}{l}{.113} & \multicolumn{1}{l}{.903} & \multicolumn{1}{l}{.366} \\  
 & & & & \\  
\multicolumn{1}{l}{\textbf{Min. Wage}} & & & & \\  
\multicolumn{1}{l}{T1 Intercept} & \multicolumn{1}{l}{-.609} & \multicolumn{1}{l}{.083} & \multicolumn{1}{l}{-7.297} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T1} & \multicolumn{1}{l}{.055} & \multicolumn{1}{l}{.117} & \multicolumn{1}{l}{.470} & \multicolumn{1}{l}{.638} \\  
\multicolumn{1}{l}{T2 Intercept} & \multicolumn{1}{l}{-.492} & \multicolumn{1}{l}{.082} & \multicolumn{1}{l}{-5.995} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T2} & \multicolumn{1}{l}{-.041} & \multicolumn{1}{l}{.116} & \multicolumn{1}{l}{-.349} & \multicolumn{1}{l}{.727} \\  
\multicolumn{1}{l}{T3 Intercept} & \multicolumn{1}{l}{-.595} & \multicolumn{1}{l}{.083} & \multicolumn{1}{l}{-7.145} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T3} & \multicolumn{1}{l}{-.077} & \multicolumn{1}{l}{.118} & \multicolumn{1}{l}{-.651} & \multicolumn{1}{l}{.515} \\  
\multicolumn{1}{l}{T4 Intercept} & \multicolumn{1}{l}{-.486} & \multicolumn{1}{l}{.082} & \multicolumn{1}{l}{-5.918} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T4} & \multicolumn{1}{l}{-.075} & \multicolumn{1}{l}{.117} & \multicolumn{1}{l}{-.641} & \multicolumn{1}{l}{.522} \\  
\multicolumn{1}{l}{T5 Intercept} & \multicolumn{1}{l}{-.758} & \multicolumn{1}{l}{.085} & \multicolumn{1}{l}{-8.870} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T5} & \multicolumn{1}{l}{.143} & \multicolumn{1}{l}{.119} & \multicolumn{1}{l}{1.193} & \multicolumn{1}{l}{.233} \\  
 & & & & \\  
\multicolumn{1}{l}{\textbf{Country Track}} & & & & \\  
\multicolumn{1}{l}{T1 Intercept} & \multicolumn{1}{l}{.893} & \multicolumn{1}{l}{.088} & \multicolumn{1}{l}{10.176} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T1} & \multicolumn{1}{l}{-.008} & \multicolumn{1}{l}{.124} & \multicolumn{1}{l}{-.062} & \multicolumn{1}{l}{.951} \\  
\multicolumn{1}{l}{T2 Intercept} & \multicolumn{1}{l}{1.086} & \multicolumn{1}{l}{.092} & \multicolumn{1}{l}{11.840} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T2} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{.130} & \multicolumn{1}{l}{.065} & \multicolumn{1}{l}{.948} \\  
\multicolumn{1}{l}{T3 Intercept} & \multicolumn{1}{l}{1.103} & \multicolumn{1}{l}{.092} & \multicolumn{1}{l}{11.974} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T3} & \multicolumn{1}{l}{-.017} & \multicolumn{1}{l}{.130} & \multicolumn{1}{l}{-.130} & \multicolumn{1}{l}{.897} \\  
\multicolumn{1}{l}{T4 Intercept} & \multicolumn{1}{l}{1.012} & \multicolumn{1}{l}{.090} & \multicolumn{1}{l}{11.228} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T4} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{.128} & \multicolumn{1}{l}{.064} & \multicolumn{1}{l}{.949} \\  
\multicolumn{1}{l}{T5 Intercept} & \multicolumn{1}{l}{1.003} & \multicolumn{1}{l}{.090} & \multicolumn{1}{l}{11.159} & \multicolumn{1}{l}{.000} \\  
\multicolumn{1}{l}{T5} & \multicolumn{1}{l}{.008} & \multicolumn{1}{l}{.127} & \multicolumn{1}{l}{.064} & \multicolumn{1}{l}{.949} \\  
\hline \\[-1.8ex] 
\end{longtable} 

\dsp

\normalsize


\begin{table}[!htbp] \centering    
\caption{ANOVA Chisq Test of GLM Regression of Variable on ANES/OP Indicator, Differentiated by Treatment Group}    
\label{glm-anov}  
\resizebox{10.5cm}{!}{%
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }  
\\[-1.8ex]\hline  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Df} & \multicolumn{1}{l}{Deviance} & \multicolumn{1}{l}{Resid.Df} & \multicolumn{1}{l}{Residual Deviance} & \multicolumn{1}{l}{Pr.Chi} \\  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{\textbf{Gender}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.003} & 1,258 & 1,741.387 & \multicolumn{1}{l}{.955} \\  
\multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.080} & 1,258 & 1,743.981 & \multicolumn{1}{l}{.778} \\  
\multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.156} & 1,258 & 1,741.743 & \multicolumn{1}{l}{.693} \\  
\multicolumn{1}{l}{T4} & 1 & \multicolumn{1}{l}{.029} & 1,258 & 1,740.828 & \multicolumn{1}{l}{.865} \\  
\multicolumn{1}{l}{T5} & 1 & \multicolumn{1}{l}{.013} & 1,258 & 1,743.466 & \multicolumn{1}{l}{.910} \\  
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Race}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.276} & 1,258 & 1,396.688 & \multicolumn{1}{l}{.599} \\ 
\multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.113} & 1,258 & 1,347.160 & \multicolumn{1}{l}{.736} \\  
\multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.151} & 1,258 & 1,423.487 & \multicolumn{1}{l}{.697} \\  
\multicolumn{1}{l}{T4} & 1 & \multicolumn{1}{l}{.017} & 1,258 & 1,423.621 & \multicolumn{1}{l}{.897} \\  
\multicolumn{1}{l}{T5} & 1 & \multicolumn{1}{l}{.528} & 1,258 & 1,384.957 & \multicolumn{1}{l}{.467} \\  
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Income}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.019} & 1,258 & 1,277.453 & \multicolumn{1}{l}{.889} \\  
\multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.812} & 1,258 & 1,290.106 & \multicolumn{1}{l}{.367} \\  
\multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.569} & 1,258 & 1,311.366 & \multicolumn{1}{l}{.451} \\  
\multicolumn{1}{l}{T4} & 1 & \multicolumn{1}{l}{.043} & 1,258 & 1,306.695 & \multicolumn{1}{l}{.837} \\  
\multicolumn{1}{l}{T5} & 1 & \multicolumn{1}{l}{.043} & 1,258 & 1,306.695 & \multicolumn{1}{l}{.837} \\  
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Occupation}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.000} & 1,258 & 1,637.669 & \multicolumn{1}{l}{1.000} \\  
\multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{2.299} & 1,258 & 1,660.174 & \multicolumn{1}{l}{.129} \\  
\multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{3.695} & 1,258 & 1,661.905 & \multicolumn{1}{l}{.055} \\  
\multicolumn{1}{l}{T4} & 1 & \multicolumn{1}{l}{.165} & 1,258 & 1,675.415 & \multicolumn{1}{l}{.685} \\  
\multicolumn{1}{l}{T5} & 1 & \multicolumn{1}{l}{.000} & 1,258 & 1,670.674 & \multicolumn{1}{l}{1.000} \\  
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Party ID}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{5.617} & 1,258 & 1,619.670 & \multicolumn{1}{l}{.018} \\  
\multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.498} & 1,258 & 1,641.929 & \multicolumn{1}{l}{.480} \\  
\multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.014} & 1,258 & 1,614.864 & \multicolumn{1}{l}{.905} \\  
\multicolumn{1}{l}{T4} & 1 & \multicolumn{1}{l}{1.517} & 1,258 & 1,646.701 & \multicolumn{1}{l}{.218} \\  
\multicolumn{1}{l}{T5} & 1 & \multicolumn{1}{l}{.276} & 1,258 & 1,661.142 & \multicolumn{1}{l}{.600} \\  
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Pres. Approval}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.387} & 1,258 & 1,737.416 & \multicolumn{1}{l}{.534} \\  
\multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.114} & 1,258 & 1,744.787 & \multicolumn{1}{l}{.735} \\  
\multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{1.402} & 1,258 & 1,743.649 & \multicolumn{1}{l}{.236} \\  
\multicolumn{1}{l}{T4} & 1 & \multicolumn{1}{l}{.000} & 1,258 & 1,744.241 & \multicolumn{1}{l}{1.000} \\  
\multicolumn{1}{l}{T5} & 1 & \multicolumn{1}{l}{.816} & 1,258 & 1,740.832 & \multicolumn{1}{l}{.366} \\  
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Min. Wage}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.221} & 1,258 & 1,644.543 & \multicolumn{1}{l}{.638} \\  
\multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.122} & 1,258 & 1,666.506 & \multicolumn{1}{l}{.727} \\  
\multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.424} & 1,258 & 1,626.133 & \multicolumn{1}{l}{.515} \\  
\multicolumn{1}{l}{T4} & 1 & \multicolumn{1}{l}{.411} & 1,258 & 1,663.111 & \multicolumn{1}{l}{.521} \\  
\multicolumn{1}{l}{T5} & 1 & \multicolumn{1}{l}{1.426} & 1,258 & 1,605.348 & \multicolumn{1}{l}{.232} \\  
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Country Track}} & & & & & \\  
\multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.004} & 1,258 & 1,520.274 & \multicolumn{1}{l}{.951} \\  
\multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.004} & 1,258 & 1,421.458 & \multicolumn{1}{l}{.948} \\  
\multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.017} & 1,258 & 1,419.261 & \multicolumn{1}{l}{.897} \\  
\multicolumn{1}{l}{T4} & 1 & \multicolumn{1}{l}{.004} & 1,258 & 1,459.355 & \multicolumn{1}{l}{.949} \\  
\multicolumn{1}{l}{T5} & 1 & \multicolumn{1}{l}{.004} & 1,258 & 1,463.401 & \multicolumn{1}{l}{.949} \\  
\hline \\[-1.8ex]  
\end{tabular}}  
\end{table}

\clearpage



## Conclusion {#ordblock-conclusion}


