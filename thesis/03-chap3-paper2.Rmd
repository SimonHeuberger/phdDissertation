# QUALITY COMPARISON OF MAJOR MISSING DATA SOLUTIONS WITH A PROPOSED NEW METHOD FOR ORDINAL VARIABLES {#ordmiss}

## Introduction {#ordmiss-intro}

```{r include=FALSE}
# function to make all diff columns absolute values
abs.diff <- function(df){
  df$diff <- df$diff %>% abs
  return(df)
}

# function to drop leading zeroes in numeric columns in data frames (adapted
# from f_num()) and add a plus sign for positive values
dropZero.addPlus <- function (df, digits = 4){
  x <- dplyr::select_if(df, is.numeric)
  for(i in 1:ncol(x)){
    x[,i] <- round(as.numeric(x[,i]), digits)
    x[,i] <- sprintf(paste0("%.", digits, "f"), x[,i])
    x[,i] <- gsub("^0(?=\\.)|(?<=-)0", "", x[,i], perl = TRUE)
  }
  df[, colnames(x)] <- x
  
  for(i in 1:nrow(df)){
    if(grepl("-", df$diff[i]) == FALSE){
      df$diff[i] <- paste0("+", df$diff[i])
    }else{
      df$diff[i] <- paste0("-", df$diff[i])
    }
  }
  return(df)
}

drop.zero <- function (df, digits = 4, p, s, pad.char = NA, ...){
    library(dplyr)
    ldots <- list(...)
    if (length(ldots) > 0) {
        if (!is.null(ldots[["prefix"]]) & missing(p)) 
            p <- ldots[["prefix"]]
        if (!is.null(ldots[["suffix"]]) & missing(s)) 
            s <- ldots[["suffix"]]
    }
    x <- dplyr::select_if(df, is.numeric)
    for(i in 1:ncol(x)){
      x[,i] <- round(as.numeric(x[,i]), digits)
      x[,i] <- sprintf(paste0("%.", digits, "f"), x[,i])
      x[,i] <- gsub("^0(?=\\.)|(?<=-)0", "", x[,i], perl = TRUE)
    }
    df[, colnames(x)] <- x
    return(df)
}

```

```{r Levels Overall, include=FALSE}
levs <- c("amelia", "hot.deck", "hd.ord", "mice", "na.omit", "true")
col.names <- c("Method", "Variable", "ANES", "CCES", "Framing")
```


Missing data are ubiquitous in survey research [@allison_2002_missing;@raghunathan_2016_missing]. Respondents frequently refuse to answer questions, select "Don't Know" as a response option, or drop out during the response collection process [@honaker_2010_what]. This poses a big problem for researchers because data can typically not be analyzed with statistical software if they contain missing values [@little_2002_statistical;@molenberghs_2007_missing]. Scholars have developed several ways to treat missing data. These can be roughly categorized into deletion, single imputation, and multiple imputation. Single imputation concerns the replacement of missing values with substitute estimates such as the mean, regression coefficients, or values from randomly drawn 'similar' respondents in the data. Multiple imputation estimates missing values from conditional distributions and subsequently averages the results into a single parameter of inference [@rubin_1976_inference;@king_2001_analyzing;@fay_1996_alternative]. 

Multiple imputation has become the state of the art in missing data management [@andridge_2010_review; @graham_1999_performance; @schafer_2002_missing; @white_2011_multiple] since it accounts for and incorporates uncertainty around the estimated imputations through repeated draws. This is missing from single imputation techniques which treat the single estimated replacement value as a de-facto data entry on par with observed values. This leads to biased standard errors and confidence intervals. Crucially, uncertainty is not reflected in the imputed values [@kroh_2006_taking;@gill_2013_bayesian]. Similarly, listwise deletion has been shown to induce bias with political data [@bodner_2008_what;@collins_2001_comparison;@pigott_review_2001;@rees_1997_methods;@reilly_1993_data].

However, parametric multiple imputation as applied by the most popular imputation packages in `R` is not necessarily always the most suitable method for all types of variables. For discrete data, multiple hot deck imputation, a combination of the single imputation method hot decking (see section \ref{ordmiss-theory-impute-hd}) and multiple imputation, proves more precise as it avoids the common multiple imputation technique of imputing discrete data on a continuous scale [@gill_2012_have]. This practically turns discrete variables into continuous variables which changes their nature and can result in non-observable and biased imputation values with artificially smaller standard errors [@fuller_2005_deck; @kim_2004_finite; @kim_2004_fractional]. Multiple hot deck imputation on the other hand preserves the integrity of discrete data, does not change the size of standard errors, and produces more accurate imputations. It estimates affinity scores for each missing value to measure how similar a respondent with a missing value is to another respondent across all variables except the missing one. 

However, multiple hot deck imputation does not account for ordinal variables as its affinity score algorithm assumes even distances between categories in discrete data. This assumption does not apply for ordinal variables. I propose a method designed to impute discrete missing data specifically from ordinal variables. Because of the success of multiple hot deck imputation in its applicability to missing data with discrete variables with a small number of categories [@gill_2012_have], this method is based on multiple hot deck imputation and adapted to account for the specific circumstances of ordinal variables. Based on the ordered probit model approach described in section \ref{intro-op}, it applies a scaled solution with newly estimated numeric thresholds from an assumed underlying latent continuous variable to measure the distances between the categories and calculate affinity scores.

The following is an outline of the remainder of this chapter: Sections \ref{ordmiss-theory-mechanisms} to \ref{ordmiss-theory-singimpute} will discuss the theory behind missing data mechanisms and provide an overview of listwise deletion and the most common single imputation methods. Section \ref{ordmiss-theory-multimpute} explains the basics of multiple imputation and outlines major `R` packages implementing it. These include `Amelia`, `mice`, `hot.deck` (implementing multiple hot deck imputation), and my self-penned multiple hot deck imputation function for ordinal variables, `hd.ord`. Since simple imputation is widely condemned as a general imputation method, my focus lies on multiple imputation. Section \ref{ordmiss-data} describes the survey data these packages/function are tested on: A selection of the 2016 ANES, a subset of the 2016 CCES, and a randomized experiment on moral framing I ran on MTurk in 2017. Each dataset is complete and randomly amputed to insert missing data. Each of the four `R` packages/functions is applied to each dataset and tested for accuracy and speed for different types of missingness (MAR, MNAR) and variables (binary, ordinal, interval). Section \ref{ordmiss-results} shows the results and assesses the usefulness of my proposed new method. Finally, section \ref{ordmiss-conclusion} will provide concluding remarks.



## Theory {#ordmiss-theory}

### Missing Data Mechanisms {#ordmiss-theory-mechanisms}

Let there be $Y$, an $n \times v$ matrix with data on $n$ respondents for $v$ variables. Let there also be the response indicator $R$ as an $n \times v$ matrix with values of 0 or 1. Let their respective elements be denoted by $y_{ij}$ and $r_{ij}$, with $i = 1, ..., n$ and $j = 1, ..., v$. If $y_{ij}$ is observed, $r_{ij} = 1$. If $y_{ij}$ is missing, $r_{ij} = 0$. All elements where $r_{ij} = 0$ make up the missing data, $Y_{miss}$. All elements where $r_{ij} = 1$ make up the observed data, $Y_{obs}$. Together, $Y_{obs}$ and $Y_{miss}$ form the complete data $Y$. Missing data can then generally be described by $\text{prob}(R = 0 | Y_{obs}, Y_{miss}, \theta)$, i.e. the probability of missing data depends on the observed data, the missing data, and a vector of unknown parameters. Depending on the mechanism by which data is missing, this expression can be further simplified.

Data can be missing by three basic mechanisms: It can be missing completely at random (MCAR), missing at random (MAR), nor missing not at random (MNAR). It is not possible to test respective data on the mechanism underlying its missingness. The onus here is on the researcher to make substantive assumptions based on the nature of the data at hand -- in other words: Researchers need to make assumptions about how the data came to be missing in the first place.

The simplest and easiest case is missing completely at random (MCAR). Under the MCAR assumption, data is missing at the repeated flip of a coin. There is no process guiding the missingness; it is inserted truly at random. In statistical terms, this means both the unobserved and the missing data independently from each other form a random sample of the population and have the same underlying distribution. Missing data would not pose such a serious problem, if it routinely and ubiquitously occurred MCAR. Unfortunately, however, that is not the case, even when restricted just to survey data: Survey respondents are known to withhold sensitive data. This can stretch from the unwillingness to divulge information deemed to be personally private (income, sexual orientation) to the refusal to answer sensitive questions out of fear of political or social repercussions in the community (union membership, support for polarizing political candidates). These types of missing data occur systematically, i.e. answers have not been refused randomly across the whole range of income levels but are missing only for respondents with very high or very low income. Similarly, only answers criticizing the authorities are missing in surveys in non-democratic states, while the state-loyal responses are present. Notationally, the general missing data expression can be simplified under the MCAR assumption to $\text{prob}(R = 0 | \theta)$, i.e. the generic probability of missing data, independent of the data themselves and only dependent on $\theta$.

As a result, a MCAR assumption in political science surveys is rare and requires tremendous backing up. It is more commonly assumed among researchers that missing data are missing at random (MAR). MAR means missing data are related to the observed but not the unobserved data. In practical terms, this for instance means missing data on income can be related to observed data on education or occupation. Here, the missing data are not a random sample of the entire data. MAR transforms the general missing data expression to $\text{prob}(R = 0 | Y_{obs}, \theta)$, i.e. the chances of missing data depend on the observed data and $\theta$.

Finally, missing data can be missing not at random (MNAR)^[Data MNAR is also sometimes called 'non-ignorable' (see for instance @gill_2012_have and @allison_2002_missing).] This is the case when missing data are related to unknown and/or unobserved parameters. Continuing the example of missing data on income, under MNAR we do not observe data on education or occupation that can be used to fill the missing income data slot. In the case of data MNAR, the general missing data expression remains unchanged, $\text{prob}(R = 0 | Y_{obs}, Y_{miss}, \theta)$, i.e. the missingness of data depends on the observed and the missing data. 

As mentioned above, it is not possible to test whether missing data is MCAR, MAR, or MNAR. The assessment of the missing data mechanism underlying any respective data comes from researchers and their understanding of the data generating process. The statistical methods to address missing data can be broadly categorized into deletion, imputation, and multiple imputation. Sections \ref{ordmiss-theory-delete} to \ref{ordmiss-theory-multimpute} outline these below. Given the focus of this chapter, special attention lies on section \ref{ordmiss-theory-multimpute}.




### Listwise Deletion {#ordmiss-theory-delete}

One of the most common methods of handling missing data in quantitative political science is listwise deletion. This involves the removal of any incomplete observations, thereby reducing the sample size. The resulting sample is then ready for analysis. Whilst almost unbeatable in its simplicity and speed, this method often induces bias, depending on how much data are missing, how non-random the pattern of missingness is, and other aspects [@allison_2002_missing; @king_2001_analyzing; @little_2002_statistical]. 

In the case of data MCAR, listwise deletion is not biased, as it removes a random sample of the population [@king_2001_analyzing; @schafer_2002_missing]. When missing data are MAR, listwise deletion is likely to be biased, since the observed data is tilted towards respondents with characteristics that make them more likely to respond [@collins_2001_comparison; @graham_1997_analysis]. The potential for bias increases with data MNAR [@collins_2001_comparison; @diggle_1994_informative; @glynn_1993_multiple; @robins_1998_semiparametric].

While the bias inserted by listwise deletion in each individual data analysis may not necessarily be drastic, studies have shown that it can be so severe as to alter substantive conclusions [@brown_1994_efficacy; @graham_1996_maximizing; @honaker_2010_what; @wothke_2000_longitudinal]. Even if that were not or only rarely the case and most data were MCAR, reducing the sample size is generally never a recommended approach as, among other aspects, standard errors from regression models are inflated. As @king_2001_analyzing put it, the result of listwise deletion "is a loss of valuable information at best and severe selection bias at worst" (p. 49). In `R`, listwise deletion is done very quickly, for instance with the base function `na.omit()`. 




### Single Imputation {#ordmiss-theory-singimpute}

Single imputation means replacing missing data with substituted values, i.e. the structural opposite of deletion. Imputation requires some method of creating a predictive distribution, based on the observed data, from which value substitutions are picked. Single imputation, regardless of its exact nature, is not recommended to impute missing data since, similar to deletion, it biases standard errors and confidence intervals  [@honaker_2010_what]. Crucially, uncertainty is not reflected in the imputed values [@little_2002_statistical]. The following subsections are mere selections of the most common single imputation methods and make no claim of completeness. Since single imputation is widely condemned as a general imputation method and as my focus lies on multiple imputation, they are also brief.


#### Mean {#ordmiss-theory-impute-mean}

In mean imputation, the mean is used to substitute missing values within cells. Mean imputation, sometimes also called unconditional mean imputation, means replacing missing values with the mean of the observed values, so $Y_{miss} = \overline{Y_{obs}}$. While it does not change the mean of the sample, this method distorts the empirical distribution of $Y$, which in turn produces biased estimates of any non-linear quantities such as variances and covariances [@haitovsky_1968_missing]. It is also bound to be inaccurate in most cases, since few values generally fall exactly on the mean, and can be nonsensical for discrete variables [@efron_1994_missing].

Mean imputation can be done in many ways in `R`, for instance with the `impute()` function in the `Hmisc` package or by setting `method = "mean"` in the `mice()` function in `mice`. It is also very simple to execute it in base `R`, e.g. for a sample data frame `df` and the numeric column `x`: \begin{verbatim} df <- transform(df, x = ifelse(is.na(x), mean(x, na.rm=TRUE), x)) \end{verbatim}


#### Regression {#ordmiss-theory-impute-regress}

Imputation by regression, sometimes also called conditional mean imputation, imputes missing values conditional on observed values. Researchers predict observed variable values based on other variables, while the fitted values from the regression model are then used to impute variable values where they are missing. Assume an independent variable in a multiple regression model. Assume further that $x$ contains missing values, $x_{miss}$, and observed values, $x_{obs}$. We regress $x_{obs}$ on the other independent variables and use the estimated equation to generate predicted values for $x_{miss}$, $x_{pred}$. $x_{pred}$ then replace $x_{miss}$, completing the dataset. While more accurate than mean imputation, particularly for large samples with data MCAR, regression imputation nonetheless suffers from the same flaw that accompanies all single imputation approaches: Uncertainty is not reflected in the imputed values [@horton_2007_much].

Differing variations of imputation by regression exist in `R`, such as the `aregImpute()` function in the `Hmisc` package, which performs additive regression, and setting `method = "norm.predict"` in `mice()` to conduct linear regression. The `predict()` function in base `R` also applies linear regression imputation.


#### Hot Decking {#ordmiss-theory-impute-hd}

Hot decking imputation was developed in the 1970s and replaces missing values with values from similar respondents in the sample [@ernst_1978_weighting; @ford_1983_overview]. It is called 'hot decking' as a reference to taking draws from a deck of matching computer punch-cards. The deck was 'hot' since it was currently being processed, as opposed to pre-collected or 'cold' data [@andridge_2010_review; @little_2002_statistical]. In the most general version, researchers select all respondents that are 'similar' to a respondent with missing data and randomly draw one of those respondents (with replacement) to fill in the missing value. The respondent with the initially missing value is termed the \textit{recipient}, while the 'similar' respondent is called the \textit{donor}. Variations of the method include hot decking within adjustment cells, by nearest neighbor, and sequentially ordered by a covariate [@cox_1980_weighted; @david_1986_alternative; @kaiser_1983_effectiveness; @kalton_1981_efficient; @rockwell_1975_investigation].

Contrary to mean or regression imputation, hot decking imputation preserves the integrity of the data, i.e. only actually observed values are used to fill in missing slots [@bailar_1997_comparison]. In both other single imputation methods, it is possible and sometimes even likely that missing values are replaced by values not found amongst the observed values. Also contrary to regression imputation, hot decking does not require a fitted model and is thus less vulnerable to model misspecification. However, hot decking does necessitate the existence of at least some donors for a respondent at every variable value that is missing. With a lot of missing data and few 'similar' matches, the accuracy of hot decking greatly decreases [@young_2011_survey]. Hot decking works best for discrete data as continuous data are very unlikely to be matched or 'similar', though the definition of what might constitute a 'similar' respondent is somewhat subjective [@marker_2002_large-scale]. As is the case with all single imputation methods, uncertainty is not reflected in the imputed values. Selecting the initial sample of 'similar' respondents and the subsequent random sampling from that subsampling is treated as factual responses, which leads to smaller standard errors and confidence intervals than statistically valid [@little_2002_statistical].

To my knowledge, there is currently no `R` package that applies hot decking. Nonetheless, forms of hot decking are still in use by some government statistics agencies such as the National Center for Education Statistics (for parts of the Current Population Survey) or the U.S. Bureau of the Census [@census_2002_current; @education-statistics_2002_nces].




### Multiple Imputation {#ordmiss-theory-multimpute}

Multiple imputation was invented by Rubin in the 1970s to account for the absence of uncertainty in single imputation methods and allow more accurate standard error estimates. It fills missing values with a predictive model that includes observed data and prior knowledge [@honaker_2010_what]. Over the time of its development, it has become the dominant sophisticated strategy for handling missing data [@dempster_1977_maximum; @glynn_1993_multiple; @heitjan_1991_ignorability; @little_2002_statistical; @rubin_1976_inference; @rubin_1986_multiple; @rubin_1987_multiple; @rubin_1996_multiple]. Multiple imputation involves three general steps:

(1) Impute a dataset with missing values $m$ times. This results in $i$ complete datasets (with $i = 1, ..., m$)

(2) Analyze each of the $i$ complete datasets

(3) Combine the results from each of the $i$ analysis results into one collective result

\vspace{0.8cm}

\ssp
\begin{figure}[ht]
\centering
\begin{tikzpicture}
    \node [block3] (ds) {\scriptsize{Dataset with missing values}};
    \node [block2r, above right=1.2cm and 1.8cm of ds] (imp1) {\scriptsize{Imputed dataset 1}};
    \node [block2r, above right=-0.4cm and 1.8cm of ds] (imp2) {\scriptsize{Imputed dataset 2}};
    \node [block2r, below right=1.2cm and 1.8cm of ds] (impm) {\scriptsize{Imputed dataset $m$}};
	\coordinate[right=0cm of ds] (dsc);
	\coordinate[left=0cm of imp1] (imp1lc);	
	\coordinate[left=0cm of imp2] (imp2lc);	
	\coordinate[left=0cm of impm] (impmlc);	
	\path [line] (dsc) -- (imp1lc);
	\path [line] (dsc) -- (imp2lc);
	\path (imp2) --node[auto=false]{\Large{\vdots}} (impm);
	\path [line] (dsc) -- (impmlc);
    \node [block2r, right=1cm of imp1] (results1) {\scriptsize{Results 1}};
    \node [block2r, right=1cm of imp2] (results2) {\scriptsize{Results 2}};
    \node [block2r, right=1cm of impm] (resultsm) {\scriptsize{Results $m$}};
	\coordinate[right=0cm of imp1] (imp1rc);
	\coordinate[right=0cm of imp2] (imp2rc);
	\coordinate[right=0cm of impm] (impmrc);
	\coordinate[left=0cm of results1] (results1lc);	
	\coordinate[left=0cm of results2] (results2lc);	
	\coordinate[left=0cm of resultsm] (resultsmlc);	
	\path [line] (imp1rc) -- (results1lc);
	\path [line] (imp2rc) -- (results2lc);
	\path (results2) --node[auto=false]{\Large{\vdots}} (resultsm);
	\path [line] (impmrc) -- (resultsmlc);
    \node [block3, below right=1.4cm and 1.8cm of results1] (results) {\scriptsize{Combined result}};
	\coordinate[left=0cm of results] (resultsc);
	\coordinate[right=0cm of results1] (results1rc);	
	\coordinate[right=0cm of results2] (results2rc);	
	\coordinate[right=0cm of resultsm] (resultsmrc);	
	\path [line] (results1rc) -- (resultsc);
	\path [line] (results2rc) -- (resultsc);
	\path [line] (resultsmrc) -- (resultsc);
\end{tikzpicture}
\caption{Multiple Imputation Workflow} 
\label{mi-workflow}
\end{figure}

\dsp

Figure \ref{mi-workflow} provides a graphical overview of this workflow. Each missing value is imputed $m$ times from a conditional distribution using other present values for the respective value to create $i$ imputed complete datasets. The chosen statistical analysis $\tau$, for instance a regression model, is applied to each of these $i$ datasets, resulting in $\tau_{i}$, with $i = 1, ..., m$. Finally, averaging $\tau_{i}$ gives us the single estimate, $\overline{\tau}$. Together, this is expressed as: 

$$\overline{\tau} = \frac{1}{m}\sum\limits_{i=1}^{m} \tau_{i}$$

Following @rubin_1987_multiple, the total variance of $\overline{\tau}$, $Var_T$ consists of the mean variance of $\tau_i$ within each dataset $i$, $\overline{Var_W}$, and the sample variance of $\tau$ across all datasets, $Var_A$:

\begin{align*}
\overline{Var_W} = \frac{1}{m} \sum\limits_{i=1}^{m} SE(\tau_i)^2\\
Var_A = \sum\limits_{i=1}^{m} \frac{(\tau_{i} - \overline{\tau})^2}{m -1}\\
Var_T = \overline{Var_W} + Var_A
\end{align*}

Multiplied by a factor correcting for small numbers of $m$ (as $m < \infty$), $Var_T$ is adjusted to: 

$$Var_T = \overline{Var_W} + Var_A (1 + \frac{1}{m})$$

Each imputed complete dataset is identical to all other imputed complete datasets, with the exception of the imputed value. The imputed values for a missing value differ with each imputation of $M$ in order to reflect uncertainty levels. The 'multiple' part of the imputation is a crucial aspect here since each imputation run will produce slightly different parameter estimates. Imputing multiple times and then averaging the results creates variability which adjusts the standard errors upward [@kroh_2006_taking]. This deliberate random variation included in a deterministic multiple imputation run removes the overconfidence from single imputation, where the standard error estimates are too low [@schafer_2002_missing]. In a case where the utilized multiple imputation model predicts missing values well, variation across the imputed values is small. In other cases, variation may be larger, depending on the level of certainty we have about the missing value. Multiple imputation has been shown to produce consistent, asymptotically efficient and normal estimates for a variety of data MAR [@allison_2002_missing]. 

Choosing $m$, the number of imputations, is somewhat subjective. Originally, $m = 5$ was considered sufficient based on efficiency calculations [@rubin_1987_multiple] and is still the default in most software packages. More recent discussions stress the need for an increase of $m$ in order to estimate more nuanced standard errors. Various approaches continue to coexist, such as focusing on the parameter with the largest fraction of missing information [@kroh_2006_taking] or starting with $m = 5$ and gradually increasing it in subsequent runs [@raghunathan_2016_missing]. The most common current practice appears to be to set $m$ to the percentage of missing data, i.e. if 20 percent of data are missing, $m$ is set to 20 [@bodner_2008_what; @white_2011_multiple]. 

There are numerous ways to implement multiple imputation. Up until the late 1990s, this required considerable statistical knowledge and sophisticated methodological skills (see @honaker_2010_what for an overview). The use of multiple imputation was thus limited to a rather specialized audience of statisticians and methodologists. Since then, numerous `R` packages have emerged to facilitate user-friendliness. The by far most popular packages are `mice` and `Amelia`. Since its inception in 2001, `mice` has been cited 5,276 times on Google Scholar at the time of writing. `Amelia` was created in 2006 and has been cited 1,836 times. They are both considered among the very best implementations of multiple imputation [@horton_2007_much]. Any improvement in multiple imputation thus needs to be measured against them. `hot.deck`, the method by @gill_2012_have upon which my proposed method of multiple hot deck imputation with ordinal variables, `hd.ord` is based, follows this approach and demonstrates improved results when compared to `Amelia`. `hd.ord` extends this and also includes `mice` as a further benchmark of performance.

The following sections do not cover the full list of functions available in each package, as this would go far beyond the scope of this chapter and could literally fill books of its own, as evidenced by the 182-page package manual for `mice` [@buuren_2020_package]. Instead, I will focus on the packages' core underlying mechanisms and their major functions to perform imputation, which are named after their package namesakes: `mice`, `amelia`, `hot.deck`, and `hd.ord`.^[For the remainder of this chapter and to avoid confusion, all names will refer to the function unless explicitly stated otherwise.] I extend the focus on simplicity and user-friendliness further by running these major imputation functions with their default settings. Survey analysts usually do not possess the statistical expertise that enable them to dive deeply into distribution or chain properties. The vast majority can be assumed to use imputation functions with their default settings, particularly when applied to surveys and survey experiments that cover a nationally representative population sample. If a package only proved superior over others by setting specific and highly technical function arguments, this would defeat the purpose of making multiple imputation the missing data approach for the masses. I apply only two very minor exceptions to the default settings: The number of imputations is set to the percentage of missingness instead of the default 5, and the console printing options are turned off. Since `hot.deck` requires the setting of a method, this is set to `"best.cell"`. The same applies to `hd.ord`.





#### `mice`: Multivariate Imputation by Chained Equations {#ordmiss-theory-multimpute-mice}

`mice` is an `R` package released in 2001 [@buuren_2000_multiple]. It stands for Multiple Imputation by Chained Equations (MICE), which means imputing incomplete multivariate data by full conditional specification (FCS) [@buuren_2007_multiple; @buuren_2011_mice], a version of the imputation-posterior (IP) [@king_2001_analyzing]. The initial release of the `mice` package featured predictor selection, passive imputation, and automatic pooling. Subsequent releases included functionality for imputing multilevel data, post-processing imputed values, specialized pooling, stable imputation of categorical data, and model selection, among many others. Imputation by chained equations is extensively used across domains (see @buuren_2011_mice for a list of over 20 applied fields). Full conditional specification refers to imputation on a variable-by-variable basis, i.e. a set of conditional densities is used to impute data for each individual missing value. This approach does not require the specification of a multivariate distribution for the missing data, which separates it from competing methods like joint modeling [@schafer_1997_analysis]. 

Chained equations are based on the Gibbs sampler, a randomized Markov chain Monte Carlo algorithm to estimate a sequence of observations from a specified multivariate probability distribution [@gelman_2013_bayesian; @gill_2014_bayesian]. In essence, chained equations fill in missing values through an iterative repetition of univariate procedures that are chained together -- hence the name for the procedure. As the term univariate signifies, specification happens at the variable level, i.e. each chained equation specifies the imputation model separately for each column of the data. Following deliberations by @rubin_1987_multiple and @buuren_2011_mice, imputation by chained equations takes the missing data generating process into account and maintains data relations as well as the uncertainty about these relations. With these conditions satisfied, the imputation model results in statistically valid and factual imputations. This has been shown empirically under various circumstances, for instance for regression models [@giorgi_2008_performance; @horton_2001_multiple; @horton_2007_much], continuous data [@yu_2007_evaluation], missing predictor variables [@moons_2006_using], large surveys [@schunk_2008_markov], and addressing issues of convergence [@brand_1999_development; @buuren_2006_fully; @drechsler_2008_does].

Continuing the notation from section \ref{ordmiss-theory-mechanisms} and incorporating @buuren_2011_mice, let there be $Y$, an $n \times v$ matrix with data on $n$ respondents for $v$ variables, that is formed of missing, $Y_{miss}$, and observed data, $Y_{obs}$. As before, let there also be a vector of unknown parameters $\theta$. Now let $Y$ further be a random sample from the $z$-variate multivariate distribution, $Z(Y | \theta)$, with $\theta$ accounting for the multivariate distribution of $Y$. The proverbial pot of gold here is how to estimate the multivariate distribution of $\theta$. Under the chained equations model, we estimate a posterior distribution of $\theta$ by sampling repeatedly from conditional distributions, i.e.:

\begin{align*}
Z(Y_1 | Y_{-1}, \theta_1) \\
\vdots \\
Z(Y_z | Y_{-z}, \theta_z)
\end{align*}

Any iteration $n$ of chained equations is then a Gibbs sampler that sequentially draws

\begin{align*}
\theta_1^{*(n)} \sim Z(\theta_1 | Y_1^{obs}, Y_2^{(n-1)}, ..., Y_z^{(n-1)}) \\
Y_1^{*(n)} \sim Z(Y_1 | Y_1^{obs}, Y_2^{(n-1)}, ..., Y_z^{(n-1)}, \theta_1^{*(n)}) \\
\vdots \\
\theta_z^{*(n)} \sim Z(\theta_z | Y_z^{obs}, Y_1^{(n)}, ..., Y_{z-1}^{(n)}) \\
Y_z^{*(n)} \sim Z(Y_z | Y_z^{obs}, Y_1^{(n)}, ..., Y_z^{(n)}, \theta_z^{*(n)})
\end{align*}

with the chain starting from a random draw from observed marginal distributions and $Y_i^{(n)} = (Y_i^{obs}, Y_i^{*(n)}$ being the $i$th imputed variable at iteration $n$. Note that immediately preceding imputations, $Y_i^{*(n-1)}$, do not affect $Y_i^{*(n)}$ directly but only through connections with other variables. 

Figure \ref{mice-func} shows the package's main imputation function, `mice`, with all its arguments. As stated above, I will use `mice` with its default settings to ensure simplicity and user-friendliness.

\begin{figure}[hbt]
  \centering
  \includegraphics{figures/mice.png}
  \caption{The \texttt{mice} function}
  \label{mice-func}
\end{figure}

The majority of arguments are not of importance to general users. Arguments like `predictorMatrix`, which specifies the set of predictors to be used for each target column, and `blocks`, which provides the option to manually put variables into imputation blocks, require too much statistical knowledge to be of use to non-specialists. Other arguments do not affect the basic workings of the function. This applies for instance to `printFlag`, which sets the console printing preference, `seed`, which is used to offset the random number generator, and `data.init`, which specifies a data frame to be used to initialize imputations before the start of the iterative process. 

The only important arguments for general users are `data`, `m`, and `defaultMethod`. Only two of these require input from general users: `data` and `m`. `data` requires a data frame with missing values and `m` should be set to the percentage of missing data, as mentioned above. `defaultMethod` does not require user input but is crucial for insight into the default workings of `mice`. Its options `pmm`, `logreg`, `polyreg`, and `polr` refer to the default imputation methods that are implemented depending on the type of variable in question. `pmm` (predictive mean matching) is used for numeric data, `logreg` (logistic regression imputation) for binary and factor data with two levels, `polyreg` (polytomous regression imputation) for factor data with more than two unordered levels, and `polr` (proportional odds model) for factor data with more than 2 ordered levels. Note that `mice` thus distinguishes between ordered and unordered as well as the number of factor levels, but does not specifically incorporate ordinal variables, which feature ordered but unevenly spaced levels.


<!--
`data`
A data frame or a matrix containing the incomplete data. Missing values are coded as NA.
`m = 5`
Number of multiple imputations. The default is m=5.
`method = NULL`
Can be either a single string, or a vector of strings with length length(blocks), specifying the imputation method to be used for each column in data. If specified as a single string, the same method will be used for all blocks. The default imputation method (when no argument is specified) depends on the measurement level of the target column, as regulated by the defaultMethod argument. Columns that need not be imputed have the empty method "". See details.
`predictorMatrix`
A numeric matrix of length(blocks) rows and ncol(data) columns, containing 0/1 data specifying the set of predictors to be used for each target column. Each row corresponds to a variable block, i.e., a set of variables to be imputed. A value of 1 means that the column variable is used as a predictor for the target block (in the rows). By default, the predictorMatrix is a square matrix of ncol(data) rows and columns with all 1's, except for the diagonal. Note: For two-level imputation models (which have "2l" in their names) other codes (e.g, 2 or -2) are also allowed.
`where = NULL`
A data frame or matrix with logicals of the same dimensions as data indicating where in the data the imputations should be created. The default, where = is.na(data), specifies that the missing data should be imputed. The where argument may be used to overimpute observed data, or to skip imputations for selected missing values.
`blocks`
List of vectors with variable names per block. List elements may be named to identify blocks. Variables within a block are imputed by a multivariate imputation method (see method argument). By default each variable is placed into its own block, which is effectively fully conditional specification (FCS) by univariate models (variable-by-variable imputation). Only variables whose names appear in blocks are imputed. The relevant columns in the where matrix are set to FALSE of variables that are not block members. A variable may appear in multiple blocks. In that case, it is effectively re-imputed each time that it is visited.
`visitSequence = NULL`
A vector of block names of arbitrary length, specifying the sequence of blocks that are imputed during one iteration of the Gibbs sampler. A block is a collection of variables. All variables that are members of the same block are imputed when the block is visited. A variable that is a member of multiple blocks is re-imputed within the same iteration. The default visitSequence = "roman" visits the blocks (left to right) in the order in which they appear in blocks. One may also use one of the following keywords: "arabic" (right to left), "monotone" (ordered low to high proportion of missing data) and "revmonotone" (reverse of monotone).
`formulas`
A named list of formula's, or expressions that can be converted into formula's by as.formula. List elements correspond to blocks. The block to which the list element applies is identified by its name, so list names must correspond to block names. The formulas argument is an alternative to the predictorMatrix argument that allows for more flexibility in specifying imputation models, e.g., for specifying interaction terms.
`blots = NULL`
A named list of alist's that can be used to pass down arguments to lower level imputation function. The entries of element blots[[blockname]] are passed down to the function called for block blockname.
`post = NULL`
A vector of strings with length ncol(data) specifying expressions as strings. Each string is parsed and executed within the sampler() function to post-process imputed values during the iterations. The default is a vector of empty strings, indicating no post-processing.
`defaultMethod = c("pmm", "logreg", "polyreg", "polr")`
A vector of length 4 containing the default imputation methods for 1) numeric data, 2) factor data with 2 levels, 3) factor data with > 2 unordered levels, and 4) factor data with > 2 ordered levels. By default, the method uses pmm, predictive mean matching (numeric data) logreg, logistic regression imputation (binary data, factor with 2 levels) polyreg, polytomous regression imputation for unordered categorical data (factor > 2 levels) polr, proportional odds model for (ordered, > 2 levels).
`maxit = 5`
A scalar giving the number of iterations. The default is 5.
`printFlag = TRUE`
If TRUE, mice will print history on console. Use print=FALSE for silent computation.
`seed = NA`
An integer that is used as argument by the set.seed() for offsetting the random number generator. Default is to leave the random number generator alone.
`data.init = NULL`
A data frame of the same size and type as data, without missing data, used to initialize imputations before the start of the iterative process. The default NULL implies that starting imputation are created by a simple random draw from the data. Note that specification of data.init will start all m Gibbs sampling streams from the same imputation.
-->





#### `Amelia`: A Program for Missing Data {#ordmiss-theory-multimpute-amelia}

`Amelia` is an `R` package originally released in 1998 [@honaker_1998_amelia]. A second version, `Amelia II`, was released in 2010 [@honaker_2012_amelia]. Contrary to `mice`, which is based on IP, both versions of `Amelia` are based on the expectation-maximization (EM) algorithm [@dempster_1977_maximum; @gelman_2013_bayesian; @jackman_2000_estimation; @mclachlan_1997_algorithm; @tanner_1996_tools]. EM functions to a large part like IP, with the crucial exception that deterministic calculations of posterior means replace random draws from the entire posterior. This translates into running regressions to estimate the regression coefficient􏰒, imputing a missing value with a predicted value, re-estimating the regression coefficient, and repeating the process until convergence [@king_2001_analyzing]. While the iterations and parameters thus represent an entire density in IP, they are single maximum posterior values in EM. This makes EM comparatively much faster in finding the maximum of the likelihood function. On its own, however, EM is unsuitable for multiple imputation as it does not provide the rest of the distribution. `Amelia` circumvents this issue with expectation-maximization importance sampling (EMi) [@gelfand_1990_sampling-based; @rubin_1987_multiple; @wei_1990_monte], which combines EM with the iterative simulation approach of importance sampling. This proved unsuitable for large datasets, however, as it led to high running times and system crashes. `Amelia II` addresses this by mixing the inference methods EM and bootstrapping [@efron_1994_missing; @lahlrl_2003_impact; @rubin_1994_missing; @shao_1996_bootstrap], allowing the imputation of more variables for more observations more quickly.

`Amelia II` is based on the assumption that the complete data ($Y_{obs}$ and $Y_{miss}$) are multivariate normal (MVN): $Y \sim N_v(\mu, \sum)$, with mean vector $\mu$ and covariance matrix $\sum$. The MVN model has been proven to work for a variety of variable types [@ezzati-rice_1995_simulation; @graham_1999_performance; @rubin_1986_multiple; @schafer_1997_analysis]. Continuing the notation from section \ref{ordmiss-theory-mechanisms} and incorporating @honaker_2010_what, let there be a vector of unknown parameters $\theta$, with $\theta = (\mu, \sum)$. Let there further be our missingness matrix$R$ and the likelihood of $Y_{obs}$, $\text{prob}(Y_{obs}, R | \theta)$. `Amelia II` is explicitly set up for the MAR assumption of missing data, $\text{prob}(R = 0 | Y_{obs}, \theta)$. Under this assumption, the likelihood can be transformed as

\begin{align*}
\text{prob}(Y_{obs}, R | \theta) = \text{prob}(R | Y_{obs}) \text{prob}(Y_{obs} | \theta).
\end{align*}

Since the missing mechanism is MAR, we are only interested in the inference on complete data parameters, thus the likelihood becomes

\begin{align*}
L(\theta | Y_{obs}) \propto \text{prob}(Y_{obs} | \theta)
\end{align*}

which further translates into

\begin{align*}
\text{prob}(Y_{obs} | \theta) = \int \text{prob}(Y | \theta) y Y_{miss}
\end{align*}

under the law of iterated expectations. This results in the posterior

\begin{align*}
\text{prob}(\theta | Y_{obs}) \propto \text{prob}(Y_{obs} | \theta) = \int \text{prob}(Y | \theta) y Y_{miss}.
\end{align*}

Taking draws from this posterior is computationally intensive since the contents of $\mu$ and $\sum$ increase exponentially as the number of variables increases -- this is the perennial crux of multiple imputation, particularly for large datasets with many variables. `Amelia II` solves this through a combination of EM and bootstrapping. This process bootstraps the data to simulate estimation uncertainty for each posterior draw, runs the EM algorithm to find the mode of the posterior bootstrapped data, and then imputes by drawing from $Y_{miss}$ conditional on $Y_{obs}$ and the respective draws of $\theta$. The latter is a linear regression with parameters that can be estimated from $\theta$. This bootstrapped EM approach is said to be faster than IP as Markov chains do not need be assessed for convergence and an improvement over EMi since the variance matrix of $\mu$ and $\sum$ do not need to be calculated, allowing the algorithm to handle larger datasets.

Figure \ref{amelia-func} shows the package's main imputation function, `amelia`, with all its arguments. As stated above, I will use `amelia` with its default settings to ensure simplicity and user-friendliness.

\begin{figure}[hbt]
  \centering
  \includegraphics{figures/amelia.png}
  \caption{The \texttt{amelia} function}
  \label{amelia-func}
\end{figure}

As with `mice`, the majority of arguments are not of importance to general users. Specifications such as `splinetime`, which allows the control of cubic smoothing splines of time, and `lags`, which indicates columns in the data that should have their lags included in the imputation model, will only be used in very particular situations by a small minority of users. Other arguments likewise are not crucial to the basic workings of the function, such as `p2s`	to control console printing and `parallel` to identify any type of parallel operation to be used. 

The only arguments that require user input are `x` and `m`. `x` requires data with missing values that can be in a variety of formats, while `m`, similar to `mice`, should be adjusted to reflect the percentage of missingness in the data. Three other arguments are important since they arguably comprise the core of `amelia`'s underlying imputation mechanism: `tolerance`, `autopri`, and `boot.type`. `tolerance`	sets the convergence threshold for the EM algorithm. `autopri`	allows the EM chain to increase the empirical prior if the path strays into an non-positive definite covariance matrix. `boot.type` offers the option to turn off the non-parametric bootstrap that is applied by default. 

`amelia` incorporates ordinal variables to some extent. General research treats independent ordinal variables as continuous variables. `amelia` supports this and treats ordinal variables as continuous variables as a default. This means missing ordinal variables are imputed on a continuous scale, rather than preserved as the factual levels present in the observed data. However, the `ords` argument allows users to 'disable' continuous ordinal imputation. In this case, ordinal variables are still imputed on a continuous scale, but these imputations are then scaled and used as the probability of success in a binomial distribution. The draw from this binomial distribution is then transformed into one of the ordinal levels present in the observed data by rounding. Note that none of these `amelia` features address or reflect the spacing between the ordinal variable categories. 



<!--
`x`
either a matrix, data.frame, a object of class "amelia", or an object of class "molist". The first two will call the default S3 method. The third a convenient way to perform more imputations with the same parameters. The fourth will impute based on the settings from moPrep and any additional arguments.
`m`	
the number of imputed datasets to create.
`p2s`	
an integer value taking either 0 for no screen output, 1 for normal screen printing of iteration numbers, and 2 for detailed screen output. See "Details" for specifics on output when p2s=2.
`frontend`	
a logical value used internally for the GUI.
`idvars`	
a vector of column numbers or column names that indicates identification variables. These will be dropped from the analysis but copied into the imputed datasets.
`ts`	
column number or variable name indicating the variable identifying time in time series data.
`cs`	
column number or variable name indicating the cross section variable.
`polytime`	
integer between 0 and 3 indicating what power of polynomial should be included in the imputation model to account for the effects of time. A setting of 0 would indicate constant levels, 1 would indicate linear time effects, 2 would indicate squared effects, and 3 would indicate cubic time effects.
`intercs`	
a logical variable indicating if the time effects of polytime should vary across the cross-section.
`lags`
a vector of numbers or names indicating columns in the data that should have their lags included in the imputation model.
`leads`	
a vector of numbers or names indicating columns in the data that should have their leads (future values) included in the imputation model.
`startvals`	
starting values, 0 for the parameter matrix from listwise deletion, 1 for an identity matrix.
`tolerance`	
the convergence threshold for the EM algorithm.
`logs`
a vector of column numbers or column names that refer to variables that require log-linear transformation.
`sqrts`	
a vector of numbers or names indicating columns in the data that should be transformed by a sqaure root function. Data in this column cannot be less than zero.
`lgstc`	
a vector of numbers or names indicating columns in the data that should be transformed by a logistic function for proportional data. Data in this column must be between 0 and 1.
`noms`
a vector of numbers or names indicating columns in the data that are nominal variables.
`ords`
a vector of numbers or names indicating columns in the data that should be treated as ordinal variables.
`incheck`	
a logical indicating whether or not the inputs to the function should be checked before running amelia. This should only be set to FALSE if you are extremely confident that your settings are non-problematic and you are trying to save computational time.
`collect`	
a logical value indicating whether or not the garbage collection frequency should be increased during the imputation model. Only set this to TRUE if you are experiencing memory issues as it can significantly slow down the imputation process.
`arglist`	
an object of class "ameliaArgs" from a previous run of Amelia. Including this object will use the arguments from that run.
`empri`	
number indicating level of the empirical (or ridge) prior. This prior shrinks the covariances of the data, but keeps the means and variances the same for problems of high missingness, small N's or large correlations among the variables. Should be kept small, perhaps 0.5 to 1 percent of the rows of the data; a reasonable upper bound is around 10 percent of the rows of the data.
`priors`	
a four or five column matrix containing the priors for either individual missing observations or variable-wide missing values. See "Details" for more information.
`autopri`	
allows the EM chain to increase the empirical prior if the path strays into an nonpositive definite covariance matrix, up to a maximum empirical prior of the value of this argument times $n$, the number of observations. Must be between 0 and 1, and at zero this turns off this feature.
`emburn`	
a numeric vector of length 2, where emburn[1] is a the minimum EM chain length and emburn[2] is the maximum EM chain length. These are ignored if they are less than 1.
`bounds`	
a three column matrix to hold logical bounds on the imputations. Each row of the matrix should be of the form c(column.number, lower.bound,upper.bound) See Details below.
`max.resample`	
an integer that specifies how many times Amelia should redraw the imputed values when trying to meet the logical constraints of bounds. After this value, imputed values are set to the bounds.
`overimp`	
a two-column matrix describing which cells are to be overimputed. Each row of the matrix should be a c(row, column) pair. Each of these cells will be treated as missing and replaced with draws from the imputation model.
`boot.type`	
choice of bootstrap, currently restricted to either "ordinary" for the usual non-parametric bootstrap and "none" for no bootstrap.
`parallel`	
the type of parallel operation to be used (if any). If missing, the default is taken from the option "amelia.parallel" (and if that is not set, "no").
`ncpus`	
integer: the number of processes to be used in parallel operation: typically one would choose the number of available CPUs.
`cl`
an optional parallel or snow cluster for use if parallel = "snow". If not supplied, a cluster on the local machine is created for the duration of the amelia call.
-->







#### `hot.deck`: Multiple Hot Deck Imputation {#ordmiss-theory-multimpute-hdnorm}

`hot.deck` is an `R` package released in 2012 [@gill_2012_have]. It combines a variation of non-parametric hot decking (see \ref{ordmiss-theory-singimpute}) with multiple imputation and aims to fill gaps where parametric multiple imputation, i.e. the approach used in `mice` and `amelia`, falls short [@fuller_2005_deck; @kim_2004_finite; @kim_2004_fractional; @reilly_1993_data]. Like hot decking, `hot.deck` uses draws of actual observable values (\textit{donors}) to fill missing values (\textit{recipients}. In order to account for uncertainty around the drawn values, `hot.deck` iterates these draws over $m$ imputations and pools the results. 

The main proposed advantage of `hot.deck` lies in its applicability to missing data with discrete variables with a small number of categories. Approaches like the one used in `amelia`, for instance, by default impute discrete data on a continuous scale. This changes the nature of discrete variables and practically turns them into continuous variables. This can result in non-observable, biased, and sometimes even nonsensical imputation values with artificially smaller standard errors. The proposed `amelia` solution of rounding continuous imputations is problematic as well: Let imputation 1 of a binary variable between 0 and 1 be 0.4. Let further imputation 2 of the binary variable be 0.6. With rounding, these imputations become 0 and 1, when they are in fact 0.4 and 0.6. The rounding problem is further exacerbated for ordinal variables, where the spacing between the discrete variable categories is unknown, since it arbitrarily reduces or lengthens distances between the categories. Rounding thus by definition introduces at least some level of bias. This is not the case in `hot.deck` as it preserves the integrity of discrete data, does not change the size of standard errors, and produces more accurate imputations. `hot.deck` also does not require assumptions of a MVN distribution that is required by `amelia`.

Following @gill_2012_have, `hot.deck` estimates affinity scores, $\alpha$, for each missing value to measure how similar a respondent with a missing value, the recipient $c$, is to another respondent, the potential donor $o$, across all variables except the missing one. Each score is bounded by 0 and 1. The total set of affinity scores is denoted by $\alpha_{co}$. For each respondent, let there be vector $(p, v)$, with $p$ being the dependent variable and $v$ a vector of discrete explanatory variables of length $k$. If recipient $c$ has $q_c$ missing values in $v_c$, then the potential donor vector, $v_o$ has between 0 and $k-q_c$ exact matches with $c$. Let $w_{co}$ be the number of variables where $c$ and $o$ have non-identical values. This leaves $k-q_c -w_{co}$ as the number of variables where they have identical values. Scaled by the highest number of possible matches $(k-q_c)$, this value forms the affinity score

\begin{align*}
\alpha_{co} = \frac{k-q_c-w_{co}}{k-q_c}
\end{align*}

for each missing value recipient $c$. When the number of identical matches decreases, so does $\alpha_{co}$. While this might work well for binary variables, it poses a problem for discrete variables with many levels, as the probability to find identical matches decreases. To account for this, `hot.deck` treats potential donors $o$ for the $h$th variable in $v_{o[h]}$ that are 'close' differently than potential donors $o$ that are further away. 'Close' is defined as $v_{o[h]}$ and $v_{c[h]}$ being in the same concentric standard deviation from $\overline{h}$, the mean of variable $h$. Values outside of this range are penalized while values within this range are counted as matches. All donors with the highest affinity scores, i.e. all matches, form the best imputation cell $B$. Since all values of $v_{c[h]}$ in $B$ are part of the same distribution of independent and identically distributed (iid) random variables, which satisfies the MCAR requirement, we can use random draws from $B$ to impute the missing value. As with the other multiple imputation approaches, this process is then repeated $m$ times for each missing value to account for imputation uncertainty.

Figure \ref{hot.deck-func} shows the package's main imputation function, `hot.deck`, with all its arguments. As before, I will use `hot.deck` with its default settings. 

\begin{figure}[hbt]
  \centering
  \includegraphics{figures/hot.deck.png}
  \caption{The \texttt{hot.deck} function}
  \label{hot.deck-func}
\end{figure}

Like `mice` and `amelia`, `hot.deck` only features two arguments that require user input, `data` and `m`. Similar to the other functions, `data` requires a data frame or matrix with missing values, while `m` should once more be set to the percentage of missingness. Specialized arguments such as `optimStep` and `optimStop`, which can be tweaked to optimize standard deviation cutoff parameters, as well as `weightedAffinity`,  which indicates whether a correlation-weighted affinity score should be used, do not apply to general users.

`method` and `cutoff` form the core of `hot.deck`. The default setting of `best.cell` in the `method` argument implements multiple hot deck imputation. The alternative, `p.draw`, on the other hand, merely conducts random probabilistic draws. `cutoff` allows users to specify which variables the algorithm should treat as discrete. By default, any variable up to and including ten unique values is considered discrete. This thus includes the majority of political science survey measures, with the sensible exceptions of variables like age or widely spread assessments of income levels.

Overall, `hot.deck` is a specialized function to improve the application of multiple imputation for discrete data and has been shown to do so for highly granular discrete data [@gill_2012_have]. Moreover, political science survey research relies on highly discrete measures. What is missing from `hot.deck`, however, is the incorporation of ordinal variables as a special form of discrete data. I thus identify this gap as the ideal leverage point to improve the use of ordinal variables in the imputation of missing data. To do so, I adapt `hot.deck` to form `hd.ord`, a function specifically designed to utilize the ordered but unevenly spaced information contained in ordinal variables.






<!--
`data`
A data frame or matrix with missing values to be imputed using multiple hot deck imputation.
`m`	
Number of imputed datasets required.
`method`	
Method used to draw donors based on affinity either “best.cell” (the default) or “p.draw” for probabilistic draw
`cutoff`	
A numeric scalar such that any variable with fewer than cutoff unique non-missing values will be considered discrete and necessarily imputed with hot deck imputation.
`sdCutoff`	
Number of standard deviations between observations such that observations fewer than sdCutoff standard deviations away from each other are considered sufficiently close to be a match, otherwise they are considered too far away to be a match.
`optimizeSD`	
Logical indicating whether the sdCutoff parameter should be optimized such that the smallest possible value is chosen that produces no thin cells from which to draw donors. Thin cells are those where the number of donors is less than m.
`optimStep`	
The size of the steps in the optimization if optimizeSD is TRUE.
`optimStop`	
The value at which optimization should stop if it has not already found a value that produces no thin cells. If this value is reached and thin cells still exist, a warning will be returned, though the routine will continue using optimStop as sdCutoff.
`weightedAffinity`	
Logical indicating whether a correlation-weighted affinity score should be used.
`impContinuous`
Character string indicating how continuous missing data should be imputed. Valid options are “HD” (the default) in which case hot-deck imputation will be used, or “mice” in which case multiple imputation by chained equations will be used.
`IDvars`
A character vector of variable names not to be used in the imputation, but to be included in the final imputed datasets.
-->





#### `hd.ord`: Multiple Hot Deck Imputation with Ordinal Variables {#ordmiss-theory-multimpute-hdord}

`hd.ord` is a self-penned `R` function designed specifically to implement multiple hot deck imputation with ordinal variables. It an extension of `hot.deck` and fully utilizes the unevenly spaced yet ordered information contained in ordinal variables. As described in section \ref{intro-ordinal}, ordinal variables matter in political science surveys because a key variable in such surveys is ordinal: Education. The importance of the spacing between education values is best shown with a simplified example shown in Table \ref{ordmiss-ordspace}.

\begin{table}[ht]
  \centering
  \begin{tabular}{lccccc}
  \bottomrule 
  \midrule
  Respondent & Age & Party ID & Education & Income & Gender\\
  \hline
  A & 25 & Republican & High School Graduate & \$30-40,000 & Male \\
  B & 40 & NA & Some High School &  \$20-30,000 & Female\\
  C & 30 & Democrat & Bachelor's Degree &  \$50-60,000 & Female\\
  \bottomrule 
  \end{tabular}
  \caption{Illustrative Data}.
  \label{ordmiss-ordspace}
\end{table}


Respondent B shows missing data for party ID. To impute a fill-in value, we look at how close respondents A and C are to B in terms of age, education, income, and gender. C is closer to B in terms of age and they share the same gender. A is closer to B on education and income. `hot.deck` measures these distances and estimates affinity scores for respondents A and C. The affinity scores measure how close A and C are to B on all variables except the missing one, i.e. party ID. B then receives the party ID fill-in value from whichever respondent has the higher score. The algorithm building the affinity score is based on evenly spaced sequential numeric values, e.g. 1, 2, 3 etc. to represent the distances between the variable categories. This is the case for age, income, and gender, but not for education, since education is an ordinal variable. Applying `hot.deck` to such a numeric representation would misrepresent the data.

Instead, `hd.ord` applies `polr` from the `MASS` package to any specified number of ordinal variables in the data to estimate the underlying latent continuous variable. This estimates cutoff thresholds between the ordinal categories and bins data cases according to the linear predictors. The binned cases determine which variable categories make sense, given the underlying latent continuous variable. This can result in a reduction of education categories if the categories are too finely thinned out. `hd.ord` uses these newly estimated categories. Rather than assigning evenly spaced sequential numeric values to them, the function estimates the mid-cutpoints between each category, based on the `polr` results. We then replace the ordinal variable categories with the newly estimated numeric mid-cutpoints in the data. Finally, these values are scaled and used in the assessment of distance to calculate affinity scores.

```{r Illustrative Data Code, include=FALSE}

load("functions/OPMord.Rdata")
load("functions/OPMcut.Rdata")
ill.data <- readRDS("data/anes/anes_1000.rds")

ill.data <- ill.data[,!names(ill.data) %in% c("Liberal", "Conservative", "Single")]
dv <- "Educ"
all.evs <- colnames(ill.data)[-which(colnames(ill.data) == dv)]                         
add.nas.columns <- c("Dem", "Male", "Interest", "Inc", "Age")                        
no.nas <- ill.data[,!names(ill.data) %in% add.nas.columns]                                  
yes.nas <- ill.data[,names(ill.data) %in% add.nas.columns]
prop <- .2

set.seed(123)
data.amp <- cbind(no.nas, ampute(yes.nas, prop = prop, mech = "MAR")$amp)
OPMord.full <- OPMord(data.amp, dv = dv, evs = all.evs)
OPMcut.dat <- OPMcut(data = OPMord.full$data.full.nas, dv = dv, OPMordOut = OPMord.full)

OPMord.full$int.df$Intercepts <- OPMord.full$int.df$Intercepts %>% as.character 
OPMord.full$int.df$Intercepts <- c("Less Than High School|Some High School", "Some High School|High School Graduate", "High School Graduate|Some College", "Some College|Bachelor's Degree", "Bachelor's Degree|Master's Degree")
ill.data.int.df <- OPMord.full$int.df[,1:2] %>% drop.zero(digits = 3)
ill.data.int.df.tab <- ill.data.int.df
colnames(ill.data.int.df.tab) <- c("Intercepts", "Thresholds")

ill.data.cutp <- data.frame(cbind(c("Less Than High School", "Some High School", "High School Graduate", "Some College", "Bachelor's Degree", "Master's Degree"), OPMcut.dat$Educ %>% unique %>% round(., digits = 3) %>% sort))
ill.data.cutp$X2 <- ill.data.cutp$X2 %>% as.character %>% as.numeric
ill.data.cutp <- ill.data.cutp %>% drop.zero(digits = 3)
colnames(ill.data.cutp) <- c("origed", "midc")
ill.data.cutp.tab <- ill.data.cutp
colnames(ill.data.cutp.tab) <- c("Original Education Categories", "Mid-Cutpoints")

```




Table \ref{ordmiss-ill-res} displays illustrative results from running `polr` on survey data, with column "Thresholds" showing the estimated cutoff thresholds between the education categories. Table \ref{ordmiss-ill-mid} in turn shows the estimated mid-cutpoints for each of the education categories. The mid-cutpoint values for the categories in Table \ref{ordmiss-ill-mid} fall between the adjacent values in Table \ref{ordmiss-ill-res}, i.e. the mid-cutpoint of `r ill.data.cutp$midc[ill.data.cutp$origed == "Some High School"]` for Some High School lies between the respective thresholds of `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"]` and `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"]`. To estimate the beginning cutpoint for the first category (Less Than High School), we halve the difference between the first and second threshold and subtract this value from the first threshold: `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"]` $-$ (`r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"]` $-$ `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric`) / 2 =  `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric - ((ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"] %>% as.numeric - ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric) / 2) %>% round(digits = 3)`. The same process is applied the ending cutpoint for the last category (Master's Degree). The mid-cutpoint values are then scaled and used for the calculation of the affinity scores.

\clearpage


```{r Illustrative Data Table 1, results='asis', echo=FALSE}

stargazer(ill.data.int.df.tab, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Illustrative Data polr Results",
          label = "ordmiss-ill-res")
```

```{r Illustrative Data Table 2, results='asis', echo=FALSE}

stargazer(ill.data.cutp.tab, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Illustrative Data Value Replacements",
          label = "ordmiss-ill-mid")

```




Figure \ref{hd.ord-func} shows my self-penned imputation function, `hd.ord`, with all its arguments. As before, I will use `hd.ord` with its default settings. Since `hd.ord` is an adaptation of `hot.deck`, the two functions are identical except for the `ord` argument, which allows users to specify the ordinal variables for `polr` treatment.

\begin{figure}[hbt]
  \centering
  \includegraphics{figures/hd.ord.png}
  \caption{The \texttt{hd.ord} function}
  \label{hd.ord-func}
\end{figure}




<!--
`data`
A data frame or matrix with missing values to be imputed using multiple hot deck imputation.
`ord`
An ordinal variable to be treated with polr()
`m`	
Number of imputed datasets required.
`method`	
Method used to draw donors based on affinity either “best.cell” (the default) or “p.draw” for probabilistic draw
`cutoff`	
A numeric scalar such that any variable with fewer than cutoff unique non-missing values will be considered discrete and necessarily imputed with hot deck imputation.
`sdCutoff`	
Number of standard deviations between observations such that observations fewer than sdCutoff standard deviations away from each other are considered sufficiently close to be a match, otherwise they are considered too far away to be a match.
`optimizeSD`	
Logical indicating whether the sdCutoff parameter should be optimized such that the smallest possible value is chosen that produces no thin cells from which to draw donors. Thin cells are those where the number of donors is less than m.
`optimStep`	
The size of the steps in the optimization if optimizeSD is TRUE.
`optimStop`	
The value at which optimization should stop if it has not already found a value that produces no thin cells. If this value is reached and thin cells still exist, a warning will be returned, though the routine will continue using optimStop as sdCutoff.
`weightedAffinity`	
Logical indicating whether a correlation-weighted affinity score should be used.
`impContinuous`
Character string indicating how continuous missing data should be imputed. Valid options are “HD” (the default) in which case hot-deck imputation will be used, or “mice” in which case multiple imputation by chained equations will be used.
`IDvars`
A character vector of variable names not to be used in the imputation, but to be included in the final imputed datasets.
-->






## Data {#ordmiss-data}

To test the performance of several imputation methods, we need to work with complete data, as only complete data allow us to obtain the true values needed as a benchmark for comparison. I choose three different sets of survey data: An experiment on moral framing I ran on MTurk in 2017 (original data), data from the 2016 ANES, and data from the 2016 CCES. Data for all selected variables in all three datasets is complete. In order to test the accuracy of several imputation methods, I delete data from these complete data sets with the `ampute()` function from the `mice` package [@buuren_2020_package]. `ampute()` allows the removal of data MCAR, MAR, and MNAR. Particularly the availability of the latter offers unique opportunities: Establishing whether real-life missing data is MNAR a difficult feat. Data that are artificially MNAR, however, circumvent this problem and allow us to test the accuracy of imputation methods for data MNAR as well. `ampute()` has been shown to accurately remove data MCAR, MAR, and MNAR [@schouten_2018_generating].

Each dataset is imputed with five different functions: `amelia`, `mice`, `hot.deck`, `hd.ord`, and `na.omit` (for listwise deletion). As outlined in section \ref{ordmiss-theory-multimpute}, all functions are used with their default settings with only two exceptions: The number of imputations is set to the percentage of missingness instead of the default 5, and the console printing options are turned off.

I test each function for imputation accuracy and speed for binary, ordinal, and interval variables in all three datasets. Each dataset contains two ordinal (`Education`, `Interest`), two interval (`Age`, `Income`) and numerous binary variables. In order to enable factually accurate comparison and unless specified otherwise, each dataset contains 1,000 observations and six levels of the ordinal variable `Education`. 1,000 observations represent a common size for survey experiment data, and the `polr` analysis from section \ref{ordblock-data} estimates five or six levels to best represent `Education` in a US context. Imputation of each dataset was carried out with each of the five imputation methods for 1,000 iterations. With the exception of Table \ref{mar.5var.old.frame}, 20 percent NA were randomly amputed in each iteration for each dataset. 

Following @collins_2001_comparison and @honaker_2010_what, as many relevant predictor variables as possible were used to impute each of the datasets. For the ANES data, up to 14 predictor variables were used: `Ind` (Independent), `Moderate`, `Black`, `Hisp` (Hispanic), `Asian`, `Empl` (Employed), `Stud` (Student), `Religious`, `InternetHome`, `OwnHome`, `Rally` (have you attended a political rally), `Donate` (have you donated to a political candidate), `Married`, and `Separated`. For the CCES data, up to 17 predictor variables were used: `Rep` (Republican), `Moderate`, `Liberal`, `Black`, `Hisp`, `Asian`, `Empl`, `Unempl` (Unemployed), `Stud`, `Gay`, `Bisexual`, `StudLoans` (do you have student loans), `InternetHome`, `NotReligious`, `RentHome`, `Separated`, and `Single`. For the Framing data, up to 13 predictor variables were used: `Ind`, `Conservative`, `Liberal`, `Black`, `Hisp`, `White`, `Asian`, `Unempl`, `Ret` (Retired), `Stud`, `Official` (have you written to a political official), `Media` (how much do you follow public affairs in the media), and `Participation` (how many political activities have you participated in). Highly collinear variables were excluded with a cutoff of 0.6.

The variable mean serves as the baseline of comparison for the performance of each imputation method. Since each dataset is complete, we know the true variable mean of all variables. The closer a method comes to the true mean, the better its performance. Sections \ref{ordmiss-results-mar} to \ref{ordmiss-results-increaseNA} show the results in terms of performance accuracy. First, I impute all datasets MAR (section \ref{ordmiss-results-mar}) and MNAR (section \ref{ordmiss-results-mnar}) for five and 12 amputed variables. The five amputed variables are consistent across all three datasets: `Democrat` (binary), `Male` (binary), `Interest` (ordinal, scaled from 1 to 4), `Income` (interval), and `Age` (interval). The 12 amputed variables contain additional variables that differ between each dataset due to availability in the data. I do not impute these datasets MCAR for obvious reasons: Imputation is not necessary for data MCAR because simple deletion leads to unbiased and therefore valid results. In section \ref{ordmiss-results-increaseOrd}, I increase the number of ordinal variables to be treated by `polr` for `hd.ord` by including `Interest`. Imputations are also conducted MAR and MNAR for four and 11 amputed variables. The variables are the same as in sections \ref{ordmiss-results-mar} and \ref{ordmiss-results-mnar}, but without including `Interest`. Finally, I increase the amount of missing data to 50 and 80 percent (section \ref{ordmiss-results-increaseNA}).

Section \ref{ordmiss-results-speed} shows the results in terms of performance speed by the number of imputed variables (Table \ref{runtimes5var12var}) and the percentage of missingness (Table \ref{run.all.perc}). All running times were achieved on a Code Ocean AWS EC2 instance with 16 cores and 120 GB of memory.







## Results {#ordmiss-results}

<!--
I didn't find any scenario where hd.ord actually did significantly and consistently better than amelia or mice. It comes close on occasion for some binary variables and MNAR, but not often and consistently enough. I told Jeff this, and he asked me to write the chapter along my findings -- that the assumption behind polr didn't pan out, at least not for missing data. Everything below is built around that argument.

If it should come up, maybe because a committee member is unhappy, who knows, these are half-hearted ideas I currently have that I might possibly still investigate if I had to:
-- Something with correlation. When I made the coding error that resulted in sampling from 85 observations, amelia was awful and hd.ord much better. Jeff also uses very high correlation in his hot.deck paper (I believe it was 0.8). Jeff also used data on modernization theory for 135 countries between 1950 and 1990, though. That data is very different from surveys and doesn't suffer from any external validity problems. I'm not sure this can be applied to surveys. I frame my stuff around surveys, and what good are artificial data with super high correlations that don't occur in the 'wild' in actual surveys?
-- My second idea is related to the first, I suspect: If hd.ord was close or good in my analyses so far, it was for framing. Why? I would guess that it has to do with correlations and external validity. My sample is markedly different from ANES or CCES when you look at the true variable means. The correlations are probably also very different. That would mean that hd.ord works well for non-externally valid samples -- and what good would that be, really?
-->



<!-- ---- MAIN TEXT ---- -->

<!-- MAR  -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- 12 variables: Dem, Male, Interest, Inc, Age, Black, Empl + 5 selected -->

<!-- MNAR -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- 12 variables: Dem, Male, Interest, Inc, Age, Black, Empl + 5 selected -->

<!-- INCREASED ORDINAL VARIABLES (DVs: educ, interest): -->
<!-- MAR -->
<!-- 4 variables: Dem, Male, Inc, Age -->
<!-- 11 variables: Dem, Male, Inc, Age, Black, Empl + 5 selected -->

<!-- MNAR -->
<!-- 4 variables: Dem, Male, Inc, Age -->
<!-- 11 variables: Dem, Male, Inc, Age, Black, Empl + 5 selected -->

<!-- INCREASED MISSINGNESS (MAR, 20, 50, 80 percent): -->
<!-- Framing, 1000 iterations, 5 variables: Dem, Male, Interest, Inc, Age -->

<!-- SPEED: -->
<!-- MAR 5 variables, MAR 12 variables -->
<!-- MAR 20, 50, 80 percent framing -->



<!-- ---- APPENDIX ---- -->

<!-- ALL OBSERVATIONS (section \ref{app-ordmiss-allObs}): -->
<!-- ANES all obs. 1000 iterations, CCES all obs. 10 iterations (maxed out RAM) -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR (Table \ref{mar.5var.all}) and MNAR (\ref{mnar.5var.all}) -->


<!-- INCREASED MISSINGNESS (section \ref{app-ordmiss-increaseNA}): -->
<!-- CCES 10,000 iterations -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->
<!-- 20, 50, 80 percent -->


<!-- SPEED (section \ref{app-ordmiss-speed}): -->
<!-- CCES 10,000 iterations -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->
<!-- 20, 50, 80 percent -->

<!-- ANES all obs. 1000 iterations, CCES all obs. 10 iterations (maxed out RAM) -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->


<!--
Problem with CCES 1,000 n 10,000 its MAR 5 Var 20, 50, 80 percent (which I had planned for the appendix):
The 20 percent worked fine on CO, but 50 percent failed because RAM was maxed out. 80 percent will then probably fail as well

Problem with CCES 5 Var all obs. (MAR + MNAR) (which I had planned for the appendix):
No more than 10 iterations are possible before RAM is maxed out, on Jeff and CO
-->




### MAR {#ordmiss-results-mar}

```{r MAR 5 Variables, include=FALSE}

mar.5var.anes <- read.csv("data/anes/mar/results/anes.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% dropZero.addPlus
mar.5var.cces <- read.csv("data/cces/mar/results/cces.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% dropZero.addPlus
mar.5var.frame <- read.csv("data/framing/mar/results/framing.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% dropZero.addPlus

mar.5var.anes$diff[mar.5var.anes$method == "true"] <- mar.5var.anes$value[mar.5var.anes$method == "true"]
mar.5var.cces$diff[mar.5var.cces$method == "true"] <- mar.5var.cces$value[mar.5var.cces$method == "true"]
mar.5var.frame$diff[mar.5var.frame$method == "true"] <- mar.5var.frame$value[mar.5var.frame$method == "true"]

levels(mar.5var.anes$method) <- levels(mar.5var.cces$method) <- levels(mar.5var.frame$method) <-
  levs

mar.5var <- cbind(mar.5var.anes[, c(1,2,4)], mar.5var.cces[,4], mar.5var.frame[,4])
colnames(mar.5var) <- col.names

# to make the in-text citations shorter
mar.5.anes <- mar.5var$ANES
mar.5.cces <- mar.5var$CCES
mar.5.frame <- mar.5var$Framing
mar.5.meth <- mar.5var$Method
mar.5.var <- mar.5var$Variable

stargazer(mar.5var, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. MAR, 5 Variables with NA",
          label = "mar.5var")

```

<!--
MAR 5 Var
Explain vars: Dem and Male binary, Interest ordinal (scale 1-4), Inc and Age interval
	Binary
		hd.ord on par or worse than hot.deck for all three ds for both vars
		mice best overall across all ds and vars
		mice and amelia very close (.0001 difference)
		More difference for ANES (.0003 mice vs .0011 hd.ord Dem, .0001 mice vs. .0013 hd.ord Male) and Male CCES (.0001 amelia vs. .0014 hd.ord)
		Fewer difference for Dem CCES (.0001 amelia vs. .0004 hd.ord)
		hot.deck performs on par with mice and amelia for framing for both vars
		hd.ord further off for framing as well (.0008 vs. .0000 mice Dem; .0005 vs. .0000 amelia Male)
	Ordinal
		hd.ord worst for all three ds, with considerable distance to hot.deck (.0191 vs. .0130 ANES; .0196 vs. .0125 CCES; .0248 vs. .0213 framing)
		mice and amelia by far best across all ds
		Much larger performance difference between methods for ordinal than for binary: mice is not more than .0003 (Dem ANES) away from true value for all ds. hd.ord's max difference is .0248 (framing)
		Equal performance of mice and amelia, with the edge to mice because of CCES (.0000 vs. 0.0003)
	interval
		hd.ord worst for all three ds for both vars
		Again considerable distance to hot.deck
		mice best for Inc
		amelia best for Age
		Even larger performance difference between methods: mice is no more than .0014 (framing) away from true value across ds for Inc. hd.ord's max difference is .1278 (ANES)
		Same for amelia: amelia's max difference is .0039 (framing). hd.ord's is .4597 (ANES)
-->

This section shows the imputation results for the MAR missing data mechanism. MAR amputation was achieved by setting the `mech` argument in the `ampute` function to `MAR`. Table \ref{mar.5var} shows the results of imputing all datasets MAR for five amputed variables. For the two binary variables, `Dem` and `Male`, `hd.ord` performs on par or worse than `hot.deck` for all datasets, while `mice` and `amelia` perform best. `hd.ord` is relatively close for CCES `Dem` (`r mar.5.cces[mar.5.meth == "amelia" & mar.5.var == "Dem"]` `amelia` vs. `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Dem"]` `hd.ord`) but further away for ANES ( `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Dem"]` `mice` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Dem"]` `hd.ord` `Dem`; `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Male"]` `mice` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` `hd.ord` `Male`) and CCES `Male` (`r mar.5.cces[mar.5.meth == "amelia" & mar.5.var == "Male"]` `amelia` vs. `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` `hd.ord`).

\begin{table}[!htbp] \centering 
  \caption{Accuracy of Multiple Imputation Methods. MAR, 5 Variables with NA} 
  \label{mar.5var} 
\begin{threeparttable}
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.3420} & \multicolumn{1}{c}{.3770} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0010} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0011} & \multicolumn{1}{c}{--.0004} & \multicolumn{1}{c}{+.0008} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0004} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0290} & \multicolumn{1}{c}{--.0229} & \multicolumn{1}{c}{--.0340} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.4890} & \multicolumn{1}{c}{.4830} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0013} & \multicolumn{1}{c}{--.0011} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0013} & \multicolumn{1}{c}{--.0014} & \multicolumn{1}{c}{+.0005} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0392} & \multicolumn{1}{c}{--.0414} & \multicolumn{1}{c}{--.0256} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{2.9340} & \multicolumn{1}{c}{3.3290} & \multicolumn{1}{c}{3.2170} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0130} & \multicolumn{1}{c}{--.0125} & \multicolumn{1}{c}{--.0213} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0191} & \multicolumn{1}{c}{--.0196} & \multicolumn{1}{c}{--.0248} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0003} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0705} & \multicolumn{1}{c}{--.0724} & \multicolumn{1}{c}{--.0714} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{16.6140} & \multicolumn{1}{c}{6.4810} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1068} & \multicolumn{1}{c}{--.0259} & \multicolumn{1}{c}{--.0119} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1278} & \multicolumn{1}{c}{--.0407} & \multicolumn{1}{c}{--.0192} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{+.0008} & \multicolumn{1}{c}{--.0004} & \multicolumn{1}{c}{+.0003} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{+.0014} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.5631} & \multicolumn{1}{c}{--.2468} & \multicolumn{1}{c}{--.1367} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{50.0410} & \multicolumn{1}{c}{52.8230} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.3888} & \multicolumn{1}{c}{--.2616} & \multicolumn{1}{c}{--.3650} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.4597} & \multicolumn{1}{c}{--.3895} & \multicolumn{1}{c}{--.3923} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{+.0007} & \multicolumn{1}{c}{--.0033} & \multicolumn{1}{c}{--.0039} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{+.0017} & \multicolumn{1}{c}{--.0073} & \multicolumn{1}{c}{--.0049} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--1.1875} & \multicolumn{1}{c}{--1.2361} & \multicolumn{1}{c}{--1.0641} \\
\hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 

For the ordinal variable, `Interest`, `hd.ord` performs worst for all datasets, with considerable distance to `hot.deck` (`r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` vs. `r mar.5.anes[mar.5.meth == "hot.deck" & mar.5.var == "Interest"]` ANES; `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` vs. `r mar.5.cces[mar.5.meth == "hot.deck" & mar.5.var == "Interest"]` CCES; `r mar.5.frame[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` vs. `r mar.5.frame[mar.5.meth == "hot.deck" & mar.5.var == "Interest"]` Framing). `mice` and `amelia` perform by far best across all datasets. The performance differences between the methods are far larger for the ordinal than for the binary variables: `mice` is not more than `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Interest"]` (ANES) away from the true value across all datasets, while the maximum difference for `hd.ord` amounts to `r mar.5.frame[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` (Framing).

For the interval variables, `Inc` and `Age`, `hd.ord` also performs worst for all datasets. The distance to `hot.deck` is once more considerable. `mice` performs best for `Inc` and `amelia` shows the best results for `Age`. The performance differences between the methods are even larger here: For `Inc`, `mice` is not more than `r mar.5.frame[mar.5.meth == "mice" & mar.5.var == "Inc"]` (Framing) away from the true value across all datasets, but the maximum difference for `hd.ord` is `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Inc"]` (ANES). Similarly, `amelia`'s largest deviation from the true value for `Age` is `r mar.5.frame[mar.5.meth == "amelia" & mar.5.var == "Age"]` (Framing) as opposed to `hd.ord`'s `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Age"]` (ANES).



```{r MAR 12 Variables, include=FALSE}

mar.12var.anes <- read.csv("data/anes/mar/results/anes.mar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mar.12var.cces <- read.csv("data/cces/mar/results/cces.mar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mar.12var.frame<- read.csv("data/framing/mar/results/framing.mar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus

mar.12var.anes$diff[mar.12var.anes$method == "true"] <- mar.12var.anes$value[mar.12var.anes$method == "true"]
mar.12var.cces$diff[mar.12var.cces$method == "true"] <- mar.12var.cces$value[mar.12var.cces$method == "true"]
mar.12var.frame$diff[mar.12var.frame$method == "true"] <- mar.12var.frame$value[mar.12var.frame$method == "true"]

levels(mar.12var.anes$method) <- levels(mar.12var.cces$method) <- levels(mar.12var.frame$method) <-
  levs

un.meth <- mar.12var.anes$method %>% unique
un.meth.len <- un.meth %>% length
un.vars <- c(mar.12var.anes$variable %>% as.character,
                 mar.12var.cces$variable %>% as.character, 
                 mar.12var.frame$variable %>% as.character) %>%
  unique
un.vars.len <- un.vars %>% length
un.anes.len <- mar.12var.anes$variable %>% unique %>% length
un.cces.var <- mar.12var.cces$variable %>% unique %>% as.character
un.cces.len <- un.cces.var %>% length
un.frame.var <- mar.12var.frame$variable %>% unique %>% as.character
un.frame.len <- un.frame.var %>% length
Variable <- rep(un.vars, each = un.meth.len)
Method <- rep(un.meth, un.vars.len) %>% as.character
meth.len <- Method %>% length

mar.12var <- cbind(Method, Variable) %>% as.data.frame

mar.12var$mar.anes.col <- c(mar.12var.anes$diff, 
                            rep("---", 
                                un.meth.len * (un.vars.len - un.anes.len)))
mar.12var$mar.cces.col <- rep("---", meth.len) %>% as.character
mar.12var$mar.frame.col <- rep("---", meth.len) %>% as.character

for(i in 1:(un.cces.len)){
  mar.12var$mar.cces.col[mar.12var$Variable == un.cces.var[i]] <- mar.12var.cces$diff[mar.12var.cces$variable == un.cces.var[i]]
  mar.12var$mar.frame.col[mar.12var$Variable == un.frame.var[i]] <- mar.12var.frame$diff[mar.12var.frame$variable == un.frame.var[i]]
}

colnames(mar.12var) <- col.names

# to make the in-text citations shorter
mar.12.anes <- mar.12var$ANES
mar.12.cces <- mar.12var$CCES
mar.12.frame <- mar.12var$Framing
mar.12.meth <- mar.12var$Method
mar.12.var <- mar.12var$Variable

stargazer(mar.12var, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. MAR, 12 Variables with NA",
          label = "mar.12var")

```

<!--
MAR 12 Var
Explain vars: First 5 vars the same as for MAR 5 Var. Black, Empl, Religious, Married, OwnHome, Rally, Donate, Gay, StudLoans, Hisp, Official, Stud binary. Media and Participation ordinal (1-5). 
	Binary
		Similar picture to MAR 5 Var, though overall hd.ord arguably closer. Max difference now .0006 (Empl ANES and framing)
		amelia and mice best again, often actually zero difference to true value
	Ordinal
		Same picture as before: hd.ord is worst across all ds for all vars (including Media and Participation framing)
		mice and amelia again by far best
		hd.ord less worse than for MAR 5 Var in terms of performance differences. hd.ord's max diff for Interest is now .0106 (framing) (compared to .0248 (framing) for 5 Var). Possibly explained by thinner spread of NAs, so fewer NAs per variable. Media and Participation framing results confirm those for Interest across all ds
	interval
		Same picture as before: hd.ord is worst across all ds
		Difference to hot.deck still there but less pronounced than for MAR 5 Var
		mice overall better than amelia for Inc, though .0007 vs. .0013 ANES for amelia
		mice also overall better than amelia for Age, though .0015 vs. .0050 CCES for amelia
		As for ordinal, hd.ord less worse than for MAR 5 Var in terms of performance differences but consistent
-->

Table \ref{mar.12var} shows the results of imputing all datasets MAR for 12 amputed variables. The first five listed variables are the same as for the MAR analysis for five amputed variables in Table \ref{mar.5var}. The remaining variables were chosen based on availability in each dataset. As much as possible, the same variables were selected across all datasets. `Black`, `Empl`, `Religious`, `Married`, `OwnHome`, `Rally`, `Donate`, `Gay`, `StudLoans`, `Hisp`, `Official`, and `Stud` are binary variables. `Black` indicates whether a respondent is of African-American origin, `Empl` whether she is currently employed, `Religious` whether she follows a religious belief, `Married` whether she is currently married, `OwnHome` whether she owns her home, `Rally` whether she has attended a political rally, `Donate` whether she has donated to a political candidate, `Gay` whether she identifies as homosexual, `StudLoans` whether she currently has student loans, `Hisp` whether she is of Hispanic origin, `Official` whether she has contacted her political representative, and `Stud` whether she currently is a student. `Media` is an ordinal variable and indicates how much she follows public affairs in the media (scaled from 1 to 5). `Participation` is a interval variable and shows the accumulative count of political activities she has participated in (scaled from 0 to 4).

\ssp

\footnotesize

\begin{longtable}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
  \caption{Accuracy of Multiple Imputation Methods. MAR, 12 Variables with NA} 
  \label{mar.12var} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.3420} & \multicolumn{1}{c}{.3770} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0004} & \multicolumn{1}{c}{+.0004} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0191} & \multicolumn{1}{c}{--.0172} & \multicolumn{1}{c}{--.0280} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.4890} & \multicolumn{1}{c}{.4830} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0004} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0256} & \multicolumn{1}{c}{--.0364} & \multicolumn{1}{c}{--.0154} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{2.9340} & \multicolumn{1}{c}{3.3290} & \multicolumn{1}{c}{3.2170} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0053} & \multicolumn{1}{c}{--.0041} & \multicolumn{1}{c}{--.0095} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0077} & \multicolumn{1}{c}{--.0067} & \multicolumn{1}{c}{--.0106} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0002} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0620} & \multicolumn{1}{c}{--.0515} & \multicolumn{1}{c}{--.0721} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{16.6140} & \multicolumn{1}{c}{6.4810} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0470} & \multicolumn{1}{c}{--.0130} & \multicolumn{1}{c}{--.0060} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0591} & \multicolumn{1}{c}{--.0212} & \multicolumn{1}{c}{--.0089} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0007} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0013} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.6303} & \multicolumn{1}{c}{--.2860} & \multicolumn{1}{c}{--.0960} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{50.0410} & \multicolumn{1}{c}{52.8230} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.1391} & \multicolumn{1}{c}{--.0883} & \multicolumn{1}{c}{--.1494} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.1835} & \multicolumn{1}{c}{--.1435} & \multicolumn{1}{c}{--.1592} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{+.0056} & \multicolumn{1}{c}{--.0015} & \multicolumn{1}{c}{--.0041} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{+.0048} & \multicolumn{1}{c}{--.0050} & \multicolumn{1}{c}{--.0029} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.8638} & \multicolumn{1}{c}{--.5974} & \multicolumn{1}{c}{--.6434} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{.0790} & \multicolumn{1}{c}{.0950} & \multicolumn{1}{c}{.0690} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0003} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0092} & \multicolumn{1}{c}{--.0090} & \multicolumn{1}{c}{--.0110} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{.6610} & \multicolumn{1}{c}{.4370} & \multicolumn{1}{c}{.7430} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{+.0006} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0007} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{+.0006} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0006} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0087} & \multicolumn{1}{c}{--.0301} & \multicolumn{1}{c}{--.0157} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{.6460} & \multicolumn{1}{c}{.6420} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0006} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0166} & \multicolumn{1}{c}{--.0234} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{.5290} & \multicolumn{1}{c}{.6310} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0384} & \multicolumn{1}{c}{--.0326} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{.6820} & \multicolumn{1}{c}{.7010} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0334} & \multicolumn{1}{c}{--.0304} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{.0830} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0191} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{.1390} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0320} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0420} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0112} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.1910} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0117} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0550} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0072} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.3560} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0003} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0385} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0440} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0012} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{1.7240} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0049} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0060} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0984} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.9540} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0021} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0027} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0003} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.1003} \\ 
\hline \\[-1.8ex] 
\end{longtable} 

\dsp

\normalsize

The results are consistent with those presented in Table \ref{mar.5var}. For the binary variables, `amelia` and `mice` again perform best with results actually matching the true variable values, though `hd.ord` arguably shows closer results than in Table \ref{mar.5var} with a maximum difference to a true value of `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Empl"]` (ANES and Framing `Empl`). For the ordinal variable, `hd.ord` continues to perform worst across all datasets. The same applies to Framing `Media`. `mice` and `amelia` again show the best results. Note, however, that `hd.ord` is less worse in terms of performance differences when compared to the MAR analysis of five imputed variables. The maximum difference to the true `Interest` value is `r mar.12.frame[mar.12.meth == "hd.ord" & mar.12.var == "Interest"]` (Framing) for 12 variables but `r mar.5.frame[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` (Framing) for five variables. This is possibly explained by a thinner spread of missing values across a higher number of variables, resulting in a lower number of NAs in each amputed variable. 

The results for the interval variables follow the same pattern. `hd.ord` displays the worst results for all datasets. The difference to `hot.deck` is still present though less pronounced than in the MAR analysis of five imputed variables. `mice` overall performs better than `amelia` for `Inc` and `Age`, with the exceptions of ANES `Inc` (`r mar.12.anes[mar.12.meth == "amelia" & mar.12.var == "Inc"]` vs. `r mar.12.anes[mar.12.meth == "mice" & mar.12.var == "Inc"]`) and CCES `Age` (`r mar.12.cces[mar.12.meth == "amelia" & mar.12.var == "Age"]` vs. `r mar.12.cces[mar.12.meth == "mice" & mar.12.var == "Age"]`). The results for Framing `Participation` confirm those for `Inc` and `Age`.




### MNAR {#ordmiss-results-mnar}

```{r MNAR 5 Variables, include=FALSE}

mnar.5var.anes <- read.csv("data/anes/mnar/results/anes.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mnar.5var.cces <- read.csv("data/cces/mnar/results/cces.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mnar.5var.frame <- read.csv("data/framing/mnar/results/framing.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus

mnar.5var.anes$diff[mnar.5var.anes$method == "true"] <- mnar.5var.anes$value[mnar.5var.anes$method == "true"]
mnar.5var.cces$diff[mnar.5var.cces$method == "true"] <- mnar.5var.cces$value[mnar.5var.cces$method == "true"]
mnar.5var.frame$diff[mnar.5var.frame$method == "true"] <- mnar.5var.frame$value[mnar.5var.frame$method == "true"]

levels(mnar.5var.anes$method) <- levels(mnar.5var.cces$method) <- levels(mnar.5var.frame$method) <-
  levs

mnar.5var <- cbind(mnar.5var.anes[,c(1,2,4)], mnar.5var.cces[,4], mnar.5var.frame[,4])
colnames(mnar.5var) <- col.names

# to make the in-text citations shorter
mnar.5.anes <- mnar.5var$ANES
mnar.5.cces <- mnar.5var$CCES
mnar.5.frame <- mnar.5var$Framing
mnar.5.meth <- mnar.5var$Method
mnar.5.var <- mnar.5var$Variable

stargazer(mnar.5var,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. MNAR, 5 Variables with NA",
          label = "mnar.5var")


```

<!--
MNAR 5 VAR
Vars the same as for MAR 5 Var
	Binary
		Overall difference to true value much higher for all methods for all ds for all vars. Around .0100 for Dem ANES and CCES, around .004 for Dem framing. Around .0125 for Male across all ds. For comparison: Around .0005 for Dem and Male for all ds for MAR 5 Var.
		hd.ord is closer to amelia and mice than for MAR, sometimes more (.0133 hd.ord vs. .0132 amelia Male ANES), sometimes less (.0120 hd.ord vs. .0099 mice Dem ANES). hd.ord actually best of all methods for framing for both vars, but overall amelia and mice perform better
		Interesting: na.omit as good as amelia and mice for Male framing and rather close for the other vars and ds too
	Ordinal
		Consistent picture of overall much higher difference to true value. In terms of method performance, same picture as for MAR: hd.ord worst across all ds. 
		amelia and mice best by far, virtually identical, though with much higher differences than for MAR
		Interesting: na.omit close to hd.ord for framing. 
	interval
		Much higher differences to true value, as consistent for all MNAR results. Same picture in terms of method performance, similar to Ordinal
		Interesting: na.omit better than hd.ord for Age ANES. na.omit also better than hd.ord and hot.deck for Age framing
-->

This section shows the imputation results for the MNAR missing data mechanism. MNAR amputation was achieved by setting the `mech` argument in the `ampute` function to `MNAR`. All MAR and MNAR analyses are otherwise identical. Table \ref{mnar.5var} shows the results of imputing all datasets MNAR for five amputed variables. It is immediately noticeable that the differences between the methods' imputation results and the true values is much higher for all methods for all variables for all datasets. The results for `Dem`, for instance, hover around .0100 for ANES/CCES and around .004 for Framing, while the `Male` numbers center around .0125 across all datasets. In the corresponding MAR analysis, however, the results for `Dem` and `Male` revealed around .0005 for all datasets. 

\begin{table}[!htbp] \centering 
  \caption{Accuracy of Multiple Imputation Methods. MNAR, 5 Variables with NA} 
  \label{mnar.5var} 
\begin{threeparttable}
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.3420} & \multicolumn{1}{c}{.3770} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0114} & \multicolumn{1}{c}{--.0099} & \multicolumn{1}{c}{--.0038} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0120} & \multicolumn{1}{c}{--.0105} & \multicolumn{1}{c}{--.0033} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0106} & \multicolumn{1}{c}{--.0102} & \multicolumn{1}{c}{--.0046} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0099} & \multicolumn{1}{c}{--.0101} & \multicolumn{1}{c}{--.0036} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0176} & \multicolumn{1}{c}{--.0140} & \multicolumn{1}{c}{--.0185} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.4890} & \multicolumn{1}{c}{.4830} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0136} & \multicolumn{1}{c}{--.0116} & \multicolumn{1}{c}{--.0127} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0133} & \multicolumn{1}{c}{--.0124} & \multicolumn{1}{c}{--.0125} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0132} & \multicolumn{1}{c}{--.0121} & \multicolumn{1}{c}{--.0133} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0132} & \multicolumn{1}{c}{--.0120} & \multicolumn{1}{c}{--.0135} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0214} & \multicolumn{1}{c}{--.0219} & \multicolumn{1}{c}{--.0133} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{2.9340} & \multicolumn{1}{c}{3.3290} & \multicolumn{1}{c}{3.2170} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0288} & \multicolumn{1}{c}{--.0246} & \multicolumn{1}{c}{--.0333} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0335} & \multicolumn{1}{c}{--.0296} & \multicolumn{1}{c}{--.0369} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0167} & \multicolumn{1}{c}{--.0146} & \multicolumn{1}{c}{--.0133} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0167} & \multicolumn{1}{c}{--.0146} & \multicolumn{1}{c}{--.0135} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0379} & \multicolumn{1}{c}{--.0372} & \multicolumn{1}{c}{--.0379} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{16.6140} & \multicolumn{1}{c}{6.4810} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.2299} & \multicolumn{1}{c}{--.0928} & \multicolumn{1}{c}{--.0591} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.2554} & \multicolumn{1}{c}{--.1038} & \multicolumn{1}{c}{--.0648} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1225} & \multicolumn{1}{c}{--.0578} & \multicolumn{1}{c}{--.0463} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1229} & \multicolumn{1}{c}{--.0566} & \multicolumn{1}{c}{--.0445} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.2770} & \multicolumn{1}{c}{--.1334} & \multicolumn{1}{c}{--.0740} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{50.0410} & \multicolumn{1}{c}{52.8230} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.6319} & \multicolumn{1}{c}{--.4596} & \multicolumn{1}{c}{--.7147} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.7415} & \multicolumn{1}{c}{--.5929} & \multicolumn{1}{c}{--.7477} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.2450} & \multicolumn{1}{c}{--.2266} & \multicolumn{1}{c}{--.2875} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.2369} & \multicolumn{1}{c}{--.2160} & \multicolumn{1}{c}{--.2888} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.6427} & \multicolumn{1}{c}{--.6392} & \multicolumn{1}{c}{--.5853} \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table}


For the binary variables, `hd.ord` performs more closely on par with `amelia` and `mice` than in the corresponding MAR analysis above, sometimes more (`r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]` `hd.ord` vs. `r mnar.5.anes[mnar.5.meth == "amelia" & mnar.5.var == "Male"]` `amelia` ANES `Male`) and sometimes less so (`r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Dem"]` `hd.ord` vs. `r mnar.5.anes[mnar.5.meth == "mice" & mnar.5.var == "Dem"]` `mice` ANES `Dem`). In addition, `hd.ord` actually represents the best method for both binary variables in the Framing data. Note the interesting results for `na.omit` here: It performs as well as `amelia` and `mice` for Framing `Male`.

The results for the ordinal variables confirm those of the MAR analysis: `hd.ord` represents the worst method across all datasets. `amelia` and `mice` show by far the best results and are virtually identical with each other, though the differences to the true values are much higher than in the MAR analysis -- as is the case for the entire MNAR analysis. `na.omit` shows results that are close to `hd.ord`'s for Framing.

The results for the interval variables paint the same picture as the ordinal ones. `hd.ord` shows the worst performance. Note that `na.omit` now actually outperforms `hot.deck` for Framing `Age` and `hd.ord` for Framing and ANES `Age`.


```{r MNAR 12 Variables, include=FALSE}

mnar.12var.anes <- read.csv("data/anes/mnar/results/anes.mnar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mnar.12var.cces <- read.csv("data/cces/mnar/results/cces.mnar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mnar.12var.frame <- read.csv("data/framing/mnar/results/framing.mnar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus

mnar.12var.anes$diff[mnar.12var.anes$method == "true"] <- mnar.12var.anes$value[mnar.12var.anes$method == "true"]
mnar.12var.cces$diff[mnar.12var.cces$method == "true"] <- mnar.12var.cces$value[mnar.12var.cces$method == "true"]
mnar.12var.frame$diff[mnar.12var.frame$method == "true"] <- mnar.12var.frame$value[mnar.12var.frame$method == "true"]

levels(mnar.12var.anes$method) <- levels(mnar.12var.cces$method) <- levels(mnar.12var.frame$method) <-
  levs

# there are several in-between code steps that are identical to MAR 12 Variables
# above, so I didn't include them here

mnar.12var <- cbind(Method, Variable) %>% as.data.frame

mnar.12var$mnar.anes.col <- c(mnar.12var.anes$diff,
                              rep("---",
                                  un.meth.len * (un.vars.len - un.anes.len)))
mnar.12var$mnar.cces.col <- rep("---", meth.len) %>% as.character
mnar.12var$mnar.frame.col <- rep("---", meth.len) %>% as.character

for(i in 1:(un.cces.len)){
  mnar.12var$mnar.cces.col[mnar.12var$Variable == un.cces.var[i]] <- mnar.12var.cces$diff[mnar.12var.cces$variable == un.cces.var[i]]
  mnar.12var$mnar.frame.col[mnar.12var$Variable == un.frame.var[i]] <- mnar.12var.frame$diff[mnar.12var.frame$variable == un.frame.var[i]]
}

colnames(mnar.12var) <- col.names

# to make the in-text citations shorter
mnar.12.anes <- mnar.12var$ANES
mnar.12.cces <- mnar.12var$CCES
mnar.12.frame <- mnar.12var$Framing
mnar.12.meth <- mnar.12var$Method
mnar.12.var <- mnar.12var$Variable

stargazer(mnar.12var,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. MNAR, 12 Variables with NA",
          label = "mnar.12var")


```

<!--
MNAR 12 VAR
Vars the same as for MNAR 5 Var
	Binary
		Similar picture to MNAR 5 Var. amelia and mice perform better, but often not by much. Occasionally, hd.ord eclipses them (.0013 hd.ord vs. .0014 mice Dem framing; 0.0053 hd.ord vs. .0055 mice and amelia Male ANES).
		na.omit again often performs close to the other methods 
	Ordinal
		Exactly the same as for MNAR 5 Var. hd.ord worst across all ds. mice and amelia far better and virtually identical (including Media and Participation framing).
		na.omit further off than for MNAR 5 Var
	interval
		Same method performance as for MNAR 5 Var.
		na.omit better than hot.deck and hd.ord for Age framing (.2239 vs. .2811 and .2989) and Age CCES (.1367 vs. .1732 and .2251)
-->

Table \ref{mnar.12var} shows the results of imputing all datasets MNAR for 12 amputed variables. The results are consistent with those obtained for five amputed variables MNAR. `amelia` and `mice` perform better than `hd.ord` for the binary variables overall, but often not by much. Occasionally, `hd.ord` eclipses them (`r mnar.12.frame[mnar.12.meth == "hd.ord" & mnar.12.var == "Dem"]` vs. `r mnar.12.frame[mnar.12.meth == "mice" & mnar.12.var == "Dem"]` `mice` Framing `Dem`; `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Male"]` vs. `r mnar.12.anes[mnar.12.meth == "mice" & mnar.12.var == "Male"]` `mice` and `amelia` ANES `Male`). `na.omit` once more performs close to the other methods.

\ssp

\footnotesize

\begin{longtable}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
  \caption{Accuracy of Multiple Imputation Methods. MNAR, 12 Variables with NA} 
  \label{mnar.12var} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.3420} & \multicolumn{1}{c}{.3770} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{--.0046} & \multicolumn{1}{c}{--.0015} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{--.0013} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0043} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{--.0018} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{--.0044} & \multicolumn{1}{c}{--.0014} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0092} & \multicolumn{1}{c}{--.0081} & \multicolumn{1}{c}{--.0106} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.4890} & \multicolumn{1}{c}{.4830} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0055} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{--.0052} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0053} & \multicolumn{1}{c}{--.0051} & \multicolumn{1}{c}{--.0051} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0055} & \multicolumn{1}{c}{--.0050} & \multicolumn{1}{c}{--.0054} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0055} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{--.0055} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0093} & \multicolumn{1}{c}{--.0119} & \multicolumn{1}{c}{--.0060} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{2.9340} & \multicolumn{1}{c}{3.3290} & \multicolumn{1}{c}{3.2170} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0113} & \multicolumn{1}{c}{--.0090} & \multicolumn{1}{c}{--.0139} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0134} & \multicolumn{1}{c}{--.0113} & \multicolumn{1}{c}{--.0150} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0068} & \multicolumn{1}{c}{--.0061} & \multicolumn{1}{c}{--.0053} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0068} & \multicolumn{1}{c}{--.0061} & \multicolumn{1}{c}{--.0054} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0236} & \multicolumn{1}{c}{--.0161} & \multicolumn{1}{c}{--.0252} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{16.6140} & \multicolumn{1}{c}{6.4810} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0899} & \multicolumn{1}{c}{--.0350} & \multicolumn{1}{c}{--.0239} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1046} & \multicolumn{1}{c}{--.0421} & \multicolumn{1}{c}{--.0262} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0495} & \multicolumn{1}{c}{--.0223} & \multicolumn{1}{c}{--.0184} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0503} & \multicolumn{1}{c}{--.0218} & \multicolumn{1}{c}{--.0177} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.2088} & \multicolumn{1}{c}{--.0970} & \multicolumn{1}{c}{--.0333} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{50.0410} & \multicolumn{1}{c}{52.8230} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.2571} & \multicolumn{1}{c}{--.1732} & \multicolumn{1}{c}{--.2811} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.3081} & \multicolumn{1}{c}{--.2251} & \multicolumn{1}{c}{--.2989} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.1100} & \multicolumn{1}{c}{--.1014} & \multicolumn{1}{c}{--.1159} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.1047} & \multicolumn{1}{c}{--.0986} & \multicolumn{1}{c}{--.1159} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.3397} & \multicolumn{1}{c}{--.1367} & \multicolumn{1}{c}{--.2239} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{.0790} & \multicolumn{1}{c}{.0950} & \multicolumn{1}{c}{.0690} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0035} & \multicolumn{1}{c}{--.0038} & \multicolumn{1}{c}{--.0028} \\
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0037} & \multicolumn{1}{c}{--.0038} & \multicolumn{1}{c}{--.0029} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0037} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{--.0011} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0034} & \multicolumn{1}{c}{--.0038} & \multicolumn{1}{c}{--.0010} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{--.0052} & \multicolumn{1}{c}{--.0045} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{.6610} & \multicolumn{1}{c}{.4370} & \multicolumn{1}{c}{.7430} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0034} & \multicolumn{1}{c}{--.0053} & \multicolumn{1}{c}{--.0022} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0033} & \multicolumn{1}{c}{--.0053} & \multicolumn{1}{c}{--.0022} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0031} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{--.0024} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0031} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{--.0025} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0014} & \multicolumn{1}{c}{--.0111} & \multicolumn{1}{c}{--.0037} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{.6460} & \multicolumn{1}{c}{.6420} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{--.0039} & \multicolumn{1}{c}{---} \\
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0043} & \multicolumn{1}{c}{--.0038} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{--.0073} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{.5290} & \multicolumn{1}{c}{.6310} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0041} & \multicolumn{1}{c}{--.0038} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{--.0037} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0042} & \multicolumn{1}{c}{--.0037} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0042} & \multicolumn{1}{c}{--.0037} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0122} & \multicolumn{1}{c}{--.0096} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{.6820} & \multicolumn{1}{c}{.7010} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0030} & \multicolumn{1}{c}{--.0030} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0028} & \multicolumn{1}{c}{--.0027} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0027} & \multicolumn{1}{c}{--.0030} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0027} & \multicolumn{1}{c}{--.0030} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0111} & \multicolumn{1}{c}{--.0090} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{.0830} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0043} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0042} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0039} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0077} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{.1390} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0051} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0054} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0048} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0122} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0420} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0024} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0025} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0026} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0024} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0035} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.1910} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0058} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0058} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0051} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0050} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0070} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0550} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0027} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0027} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0011} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0010} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0033} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.3560} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0054} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0054} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0049} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0048} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0142} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0440} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0026} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0025} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0023} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0020} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0017} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{1.7240} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0142} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0152} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0101} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0101} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0363} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.9540} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0115} \\
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0115} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0099} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0094} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0358} \\ 
\hline \\[-1.8ex] 
\end{longtable} 

\dsp

\normalsize

For the ordinal variable, `hd.ord` shows the worst performance across all datasets. `mice` and `amelia` perform far better and display virtually identical result. This also applies to Framing `Media`. `na.omit` does not perform as well as in the corresponding MAR analysis. `mice` and `amelia`'s superior performance is also visible in the results for the ordinal variables, including Framing `Participation`. `na.omit` performs better than `hot.deck` and `hd.ord` for Framing `Age` (`r mnar.12.frame[mnar.12.meth == "na.omit" & mnar.12.var == "Age"]` vs. `r mnar.12.frame[mnar.12.meth == "hot.deck" & mnar.12.var == "Age"]` and `r mnar.12.frame[mnar.12.meth == "hd.ord" & mnar.12.var == "Age"]`), and CCES `Age` (`r mnar.12.cces[mnar.12.meth == "na.omit" & mnar.12.var == "Age"]` vs. `r mnar.12.cces[mnar.12.meth == "hot.deck" & mnar.12.var == "Age"]` and `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Age"]`). 




### Increased Number of Ordinal Variables {#ordmiss-results-increaseOrd}

```{r MULT MAR 4 Variables, include=FALSE}

mult.mar.4var.anes <- read.csv("data/anes/mar/results/anes.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mult.mar.4var.cces <- read.csv("data/cces/mar/results/cces.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mult.mar.4var.frame <- read.csv("data/framing/mar/results/framing.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus

mult.mar.4var.anes$diff[mult.mar.4var.anes$method == "true"] <- mult.mar.4var.anes$value[mult.mar.4var.anes$method == "true"]
mult.mar.4var.cces$diff[mult.mar.4var.cces$method == "true"] <- mult.mar.4var.cces$value[mult.mar.4var.cces$method == "true"]
mult.mar.4var.frame$diff[mult.mar.4var.frame$method == "true"] <- mult.mar.4var.frame$value[mult.mar.4var.frame$method == "true"]

levels(mult.mar.4var.anes$method) <- levels(mult.mar.4var.cces$method) <- levels(mult.mar.4var.frame$method) <-
  levs

mult.mar.4var <- cbind(mult.mar.4var.anes[, c(1,2,4)], mult.mar.4var.cces[,4], mult.mar.4var.frame[,4])
colnames(mult.mar.4var) <- col.names

# to make the in-text citations shorter
mult.mar.4.anes <- mult.mar.4var$ANES
mult.mar.4.cces <- mult.mar.4var$CCES
mult.mar.4.frame <- mult.mar.4var$Framing
mult.mar.4.meth <- mult.mar.4var$Method
mult.mar.4.var <- mult.mar.4var$Variable

stargazer(mult.mar.4var, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. Two Ordinal Variables (Education, Interest), MAR, 4 Variables with NA",
          label = "mult.mar.4var")

```

<!--
Increased Ordinal
Treats Education and Interest with polr
Also split into MAR/MNAR and 4/11 Variables
MAR 4 Var
Vars the same as for MAR and MNAR 5 Var, just without Interest (so no ordinal vars here)
	Binary
		hd.ord worst for Dem across all ds. hd.ord better than hot.deck for Male across all ds.
		amelia and mice best, virtually identical, often zero
		Results for hd.ord get slightly worse when compared to MAR 5 Var:
		MAR 5 Var hd.ord: Dem .0011, .0004, .0008. Male .0013, .0014, .0005
		MAR 4 Var hd.ord: Dem .0018, .0005, .0012. Male .0015, .0018, .0011		
	interval
		hd.ord worst for all ds for all vars
		amelia and mice far best. mice does better for Inc and Age CCES, amelia does better for Inc and Age framing. Both equally good for Inc and Age ANES
		Consistent with the binary changes from one to two ordinal variables, the results for hd.ord get slightly worse when compared to MAR 5 Var:
		MAR 5 Var hd.ord: Inc .1278, .0407, .0192. Age .4597, .3895, .3923
		MAR 4 Var hd.ord: Inc .1523, .0516, .0225. Age .5431, .4664, .4583
-->

This section shows the imputation results where I increase the number of ordinal variables to be treated by `polr` for `hd.ord`. Specifically, I include `Interest` in the `polr` treatment. The intuition behind this is a strengthening of the underlying latent continuous variable assumption. The results so far do not show superior performance by `hd.ord`. However, this might be due to a lack of 'influence' so far. Perhaps one ordinal variable treated with `polr` is not enough to manifest itself in improved results. By including another ordinal variable in the treatment, this 'influence' is strengthened and the `polr` assumption is put to another test. As in sections \ref{ordmiss-results-mar} and \ref{ordmiss-results-mnar}, imputations are conducted MAR and MNAR. Because `Interest` is moved to the `polr` treatment, the number of imputed variables is reduced to four and 11, respectively, to ensure accurate comparison. This means the amputed variables do not include an ordinal variable any more, since no suitable replacement could be found in the ANES and CCES data. The remaining variables are the same as before.

Table \ref{mult.mar.4var} shows the results of imputing all datasets with two `polr`-treated variables MAR for four amputed variables. `hd.ord` displays the worst results for `Dem` across all datasets and beats only `hot.deck` for `Male`. `amelia` and `mice` perform best and show virtually identically results that often match the true variable means. Comparison with the MAR analysis of five imputed variables reveals that `hd.ord` consistently performs slightly worse here: `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Dem"]`, `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Dem"]`, `r mult.mar.4.frame[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Dem"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Dem"]`, `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Dem"]`, `r mar.5.frame[mar.5.meth == "hd.ord" & mar.5.var == "Dem"]` for `Dem` and `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Male"]`, `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Male"]`, `r mult.mar.4.frame[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Male"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Male"]`, `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Male"]`, `r mar.5.frame[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` for `Male`.

\begin{table}[!htbp] \centering 
  \caption{Accuracy of Multiple Imputation Methods. Two Ordinal Variables (Education, Interest), MAR, 4 Variables with NA} 
  \label{mult.mar.4var} 
\begin{threeparttable}
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.3420} & \multicolumn{1}{c}{.3770} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0008} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{+.0005} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0018} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{+.0012} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0333} & \multicolumn{1}{c}{--.0294} & \multicolumn{1}{c}{--.0357} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.4890} & \multicolumn{1}{c}{.4830} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0022} & \multicolumn{1}{c}{--.0019} & \multicolumn{1}{c}{+.0015} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0015} & \multicolumn{1}{c}{--.0018} & \multicolumn{1}{c}{+.0011} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0002} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0396} & \multicolumn{1}{c}{--.0407} & \multicolumn{1}{c}{--.0297} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{16.6140} & \multicolumn{1}{c}{6.4810} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0830} & \multicolumn{1}{c}{--.0246} & \multicolumn{1}{c}{--.0093} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1523} & \multicolumn{1}{c}{--.0516} & \multicolumn{1}{c}{--.0225} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{+.0010} & \multicolumn{1}{c}{--.0006} & \multicolumn{1}{c}{+.0009} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0008} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{+.0020} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.5771} & \multicolumn{1}{c}{--.2564} & \multicolumn{1}{c}{--.1400} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{50.0410} & \multicolumn{1}{c}{52.8230} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.2889} & \multicolumn{1}{c}{--.2350} & \multicolumn{1}{c}{--.3546} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.5431} & \multicolumn{1}{c}{--.4664} & \multicolumn{1}{c}{--.4583} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{+.0018} & \multicolumn{1}{c}{+.0085} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{+.0024} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{--.0022} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--1.1521} & \multicolumn{1}{c}{--1.1228} & \multicolumn{1}{c}{--.9688} \\  
\hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table}

`hd.ord` also performs worst for all datasets across both interval variables. `amelia` and `mice` again perform best. `mice` does better for CCES, while `amelia` does better for Framing. Both perform equally well for ANES. As for the binary variables, the results for `hd.ord` consistently get slightly worse in the switch from one to two ordinal variables in `polr`-treatment: `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Inc"]`, `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Inc"]`, `r mult.mar.4.frame[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Inc"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Inc"]`, `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Inc"]`, `r mar.5.frame[mar.5.meth == "hd.ord" & mar.5.var == "Inc"]` for `Inc` and `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Age"]`, `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Age"]`, `r mult.mar.4.frame[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Age"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Age"]`, `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Age"]`, `r mar.5.frame[mar.5.meth == "hd.ord" & mar.5.var == "Age"]` for `Age`.



```{r MULT MAR 11 Variables, include=FALSE}

mult.mar.11var.anes <- read.csv("data/anes/mar/results/anes.mar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mult.mar.11var.cces <- read.csv("data/cces/mar/results/cces.mar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mult.mar.11var.frame <- read.csv("data/framing/mar/results/framing.mar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus

mult.mar.11var.anes$diff[mult.mar.11var.anes$method == "true"] <- mult.mar.11var.anes$value[mult.mar.11var.anes$method == "true"]
mult.mar.11var.cces$diff[mult.mar.11var.cces$method == "true"] <- mult.mar.11var.cces$value[mult.mar.11var.cces$method == "true"]
mult.mar.11var.frame$diff[mult.mar.11var.frame$method == "true"] <- mult.mar.11var.frame$value[mult.mar.11var.frame$method == "true"]

levels(mult.mar.11var.anes$method) <- levels(mult.mar.11var.cces$method) <- levels(mult.mar.11var.frame$method) <-
  levs


mult.un.meth <- mult.mar.11var.anes$method %>% unique
mult.un.meth.len <- mult.un.meth %>% length
mult.un.vars <- c(mult.mar.11var.anes$variable %>% as.character,
                  mult.mar.11var.cces$variable %>% as.character,
                  mult.mar.11var.frame$variable %>% as.character) %>%
  unique
mult.un.vars.len <- mult.un.vars %>% length
mult.un.anes.len <- mult.mar.11var.anes$variable %>% unique %>% length
mult.un.cces.var <- mult.mar.11var.cces$variable %>% unique %>% as.character
mult.un.cces.len <- mult.un.cces.var %>% length
mult.un.frame.var <- mult.mar.11var.frame$variable %>% unique %>% as.character
mult.un.frame.len <- mult.un.frame.var %>% length

mult.variable <- rep(mult.un.vars, each = mult.un.meth.len)
mult.method <- rep(mult.un.meth, mult.un.vars.len) %>% as.character
mult.meth.len <- mult.method %>% length

mult.mar.11var <- cbind(mult.method, mult.variable) %>% as.data.frame

mult.mar.11var$mult.anes.col <- c(mult.mar.11var.anes$diff,
                                  rep("---",
                                      mult.un.meth.len * (mult.un.vars.len - mult.un.anes.len)))
mult.mar.11var$mult.cces.col <- rep("---", mult.meth.len) %>% as.character
mult.mar.11var$mult.frame.col <- rep("---", mult.meth.len) %>% as.character

for(i in 1:(mult.un.cces.len)){
  mult.mar.11var$mult.cces.col[mult.mar.11var$mult.variable == mult.un.cces.var[i]] <- mult.mar.11var.cces$diff[mult.mar.11var.cces$variable == mult.un.cces.var[i]]
  mult.mar.11var$mult.frame.col[mult.mar.11var$mult.variable == mult.un.frame.var[i]] <- mult.mar.11var.frame$diff[mult.mar.11var.frame$variable == mult.un.frame.var[i]]
}

colnames(mult.mar.11var) <- col.names

# to make the in-text citations shorter
mult.mar.11.anes <- mult.mar.11var$ANES
mult.mar.11.cces <- mult.mar.11var$CCES
mult.mar.11.frame <- mult.mar.11var$Framing
mult.mar.11.meth <- mult.mar.11var$Method
mult.mar.11.var <- mult.mar.11var$Variable

stargazer(mult.mar.11var, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. Two Ordinal Variables (Education, Interest), MAR, 11 Variables with NA",
          label = "mult.mar.11var")


```

<!--
MAR 11 Var
Vars the same as for MAR and MNAR 12 Var, just without Interest (so no ordinal vars here)
	Binary
		Similar picture to MAR 4 Var, though overall hd.ord arguably closer now
		amelia and mice best again, often actually zero difference to true value
		Consistent with MAR 4 Var/5 Var development, hd.ord performs slightly worse with two ordinal variables when compared to MAR 12 Var:
		MAR 12 Var hd.ord: Dem .0005, .0004, .0004. Male .0001, .0003, .0000
		MAR 11 Var hd.ord: Dem .0005, .0005, .0005. Male .0001, .0004, .0002
	Ordinal framing
		hd.ord worst for Media and Participation, confirming previous results
		mice and amelia again by far best
		hd.ord gets slightly better when compared to MAR 12 Var:
		MAR 12 Var hd.ord: Media .0060. Participation .0027
		MAR 11 Var hd.ord: Media .0059, Participation .0026
	interval
		Same picture as before: hd.ord is worst across all ds
		Difference to hot.deck still there but less pronounced than for MAR 4 Var
		amelia is better for age, mice is better for Inc (except Inc ANES, .0027 vs. .0018)
		hd.ord consistently gets slightly worse when compared to MAR 12 Var:		
		MAR 12 Var hd.ord: Inc .0591, .0212, .0089. Age .1835, .1435, .1592 
		MAR 11 Var hd.ord: Inc .0657, .0241, .0095. Age .1951, .1532, .1640
-->

Table \ref{mult.mar.11var} shows the results of imputing all datasets with two `polr`-treated variables MAR for 11 amputed variables. A similar picture for Table \ref{mult.mar.4var} emerges for the binary variables, with `amelia` and `mice` once more displaying the best results, though `hd.ord` appears to perform somewhat closer here. Consistent with the deterioration of `hd.ord` results from Table \ref{mar.5var} to Table \ref{mult.mar.4var}, `hd.ord` again consistently performs slightly worse when compared to the MAR analysis with 12 amputed variables and only `Education` treated by `polr`: `r mult.mar.11.anes[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Dem"]`, `r mult.mar.11.cces[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Dem"]`, `r mult.mar.11.frame[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Dem"]` vs. `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Dem"]`, `r mar.12.cces[mar.12.meth == "hd.ord" & mar.12.var == "Dem"]`, `r mar.12.frame[mar.12.meth == "hd.ord" & mar.12.var == "Dem"]` for `Dem` and `r mult.mar.11.anes[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Male"]`, `r mult.mar.11.cces[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Male"]`, `r mult.mar.11.frame[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Male"]` vs. `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Male"]`, `r mar.12.cces[mar.12.meth == "hd.ord" & mar.12.var == "Male"]`, `r mar.12.frame[mar.12.meth == "hd.ord" & mar.12.var == "Male"]` for `Male`.

\ssp

\footnotesize

\begin{longtable}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
  \caption{Accuracy of Multiple Imputation Methods. Two Ordinal Variables (Education, Interest), MAR, 11 Variables with NA} 
  \label{mult.mar.11var} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.3420} & \multicolumn{1}{c}{.3770} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{--.0004} & \multicolumn{1}{c}{+.0002} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{+.0005} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0200} & \multicolumn{1}{c}{--.0213} & \multicolumn{1}{c}{--.0294} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.4890} & \multicolumn{1}{c}{.4830} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0004} & \multicolumn{1}{c}{+.0002} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0254} & \multicolumn{1}{c}{--.0350} & \multicolumn{1}{c}{--.0174} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{16.6140} & \multicolumn{1}{c}{6.4810} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0346} & \multicolumn{1}{c}{--.0114} & \multicolumn{1}{c}{--.0054} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0657} & \multicolumn{1}{c}{--.0241} & \multicolumn{1}{c}{--.0095} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0018} & \multicolumn{1}{c}{--.0007} & \multicolumn{1}{c}{--.0004} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0027} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.6432} & \multicolumn{1}{c}{--.2888} & \multicolumn{1}{c}{--.0983} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{50.0410} & \multicolumn{1}{c}{52.8230} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.0887} & \multicolumn{1}{c}{--.0704} & \multicolumn{1}{c}{--.1379} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.1951} & \multicolumn{1}{c}{--.1532} & \multicolumn{1}{c}{--.1640} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{+.0010} & \multicolumn{1}{c}{--.0021} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.0016} & \multicolumn{1}{c}{--.0055} & \multicolumn{1}{c}{+.0022} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.8025} & \multicolumn{1}{c}{--.4330} & \multicolumn{1}{c}{--.5290} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{.0790} & \multicolumn{1}{c}{.0950} & \multicolumn{1}{c}{.0690} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0103} & \multicolumn{1}{c}{--.0122} & \multicolumn{1}{c}{--.0134} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{.6610} & \multicolumn{1}{c}{.4370} & \multicolumn{1}{c}{.7430} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{+.0007} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{+.0008} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{+.0008} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0007} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0109} & \multicolumn{1}{c}{--.0328} & \multicolumn{1}{c}{--.0159} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{.6460} & \multicolumn{1}{c}{.6420} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0174} & \multicolumn{1}{c}{--.0241} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{.5290} & \multicolumn{1}{c}{.6310} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0390} & \multicolumn{1}{c}{--.0324} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{.6820} & \multicolumn{1}{c}{.7010} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0005} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0341} & \multicolumn{1}{c}{--.0295} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{.0830} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{+.0002} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0186} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{.1390} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0007} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0310} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0420} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0113} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.1910} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0146} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0550} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0091} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.3560} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0006} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0004} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0373} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0440} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0001} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0026} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{1.7240} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0047} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0059} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0002} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0002} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0921} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.9540} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0019} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0026} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0002} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{+.0000} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0963} \\
\hline \\[-1.8ex] 
\end{longtable} 

\dsp

\normalsize

The same can be observed for the interval variables, with `hd.ord` again claiming last place across all datasets. The difference to `hot.deck` is still there but less pronounced than in Table \ref{mult.mar.4var}. `amelia` does best for `Age` while `mice` performs better for `Inc`, with the exception of the ANES (`r mult.mar.11.anes[mult.mar.11.meth == "mice" & mult.mar.11.var == "Inc"]` `mice` vs. `r mult.mar.11.anes[mult.mar.11.meth == "amelia" & mult.mar.11.var == "Inc"]` `amelia`). As for the binary variables, `hd.ord` also consistently performs slightly worse when compared to the MAR analysis with 12 amputed variables and only `Education` treated by `polr`: `r mult.mar.11.anes[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Inc"]`, `r mult.mar.11.cces[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Inc"]`, `r mult.mar.11.frame[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Inc"]` vs. `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Inc"]`, `r mar.12.cces[mar.12.meth == "hd.ord" & mar.12.var == "Inc"]`, `r mar.12.frame[mar.12.meth == "hd.ord" & mar.12.var == "Inc"]` for `Inc` and `r mult.mar.11.anes[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Age"]`, `r mult.mar.11.cces[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Age"]`, `r mult.mar.11.frame[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Age"]` vs. `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Age"]`, `r mar.12.cces[mar.12.meth == "hd.ord" & mar.12.var == "Age"]`, `r mar.12.frame[mar.12.meth == "hd.ord" & mar.12.var == "Age"]` for `Age`.



```{r MULT MNAR 4 Variables, include=FALSE}

mult.mnar.4var.anes <- read.csv("data/anes/mnar/results/anes.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mult.mnar.4var.cces <- read.csv("data/cces/mnar/results/cces.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mult.mnar.4var.frame <- read.csv("data/framing/mnar/results/framing.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus

mult.mnar.4var.anes$diff[mult.mnar.4var.anes$method == "true"] <- mult.mnar.4var.anes$value[mult.mnar.4var.anes$method == "true"]
mult.mnar.4var.cces$diff[mult.mnar.4var.cces$method == "true"] <- mult.mnar.4var.cces$value[mult.mnar.4var.cces$method == "true"]
mult.mnar.4var.frame$diff[mult.mnar.4var.frame$method == "true"] <- mult.mnar.4var.frame$value[mult.mnar.4var.frame$method == "true"]

levels(mult.mnar.4var.anes$method) <- levels(mult.mnar.4var.cces$method) <- levels(mult.mnar.4var.frame$method) <-
  levs

mult.mnar.4var <- cbind(mult.mnar.4var.anes[, c(1,2,4)], mult.mnar.4var.cces[,4], mult.mnar.4var.frame[,4])
colnames(mult.mnar.4var) <- col.names

# to make the in-text citations shorter
mult.mnar.4.anes <- mult.mnar.4var$ANES
mult.mnar.4.cces <- mult.mnar.4var$CCES
mult.mnar.4.frame <- mult.mnar.4var$Framing
mult.mnar.4.meth <- mult.mnar.4var$Method
mult.mnar.4.var <- mult.mnar.4var$Variable

stargazer(mult.mnar.4var, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. Two Ordinal Variables (Education, Interest), MNAR, 4 Variables with NA",
          label = "mult.mnar.4var")


```

<!--
MNAR 4 Var
Vars the same as for MAR 4 Var
	Binary
		hd.ord equally close to amelia and mice as for MNAR 5 Var, sometimes more (.0131 hd.ord vs. .0130 mice Dem CCES), sometimes less (.0155 hd.ord vs. .0127 mice Dem ANES). hd.ord actually best of all methods for framing for both vars, but overall amelia and mice perform better
		na.omit not as close as for MNAR 5 Var
		hd.ord consistently gets slightly worse when compared to MNAR 5 Var:		
		MNAR 5 Var hd.ord: Dem .0120, .0105, .0033. Male .0133, .0124, .0125
		MNAR 4 Var hd.ord: Dem .0155, .0131, .0042. Male .0172, .0160, .0157
	interval
		Consistent with previous analyses. 
		na.omit better than hd.ord for Age for all three ds and for Inc ANES
		hd.ord consistently gets slightly worse when compared to MNAR 5 Var:		
		MNAR 5 Var hd.ord: Inc .2554, .1038, .0648. Age .7415, .5929, .7477
		MNAR 4 Var hd.ord: Inc .3174, .1303, .0812. Age .8997, .7259, .9349
-->

Table \ref{mult.mnar.4var} shows the results of imputing all datasets with two `polr`-treated variables MNAR for four amputed variables. For the binary variables, `hd.ord` performs on the same level as `amelia` and `mice` when compared to the MNAR analysis of five imputed variables with only `Education` treated by `polr`; sometimes more (`r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Dem"]` `hd.ord` vs. `r mult.mnar.4.cces[mult.mnar.4.meth == "mice" & mult.mnar.4.var == "Dem"]` `mice` CCES `Dem`), sometimes less so (`r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Dem"]` `hd.ord` vs. `r mult.mnar.4.anes[mult.mnar.4.meth == "mice" & mult.mnar.4.var == "Dem"]` `mice` ANES `Dem`). `hd.ord` actually shows the best results of all methods for Framing for both variables. `na.omit` does not perform as well as it does in Table \ref{mnar.5var}. `hd.ord` again consistently performs slightly worse with the two-ordinal-variable-`polr`-treatment: `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Dem"]`,`r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Dem"]`, `r mult.mnar.4.frame[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Dem"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Dem"]`, `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Dem"]`, `r mnar.5.frame[mnar.5.meth == "hd.ord" & mnar.5.var == "Dem"]` for `Dem` and `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Male"]`, `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Male"]`, `r mult.mnar.4.frame[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Male"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]`, `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]`, `r mnar.5.frame[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]` for `Male`.

\begin{table}[!htbp] \centering 
  \caption{Accuracy of Multiple Imputation Methods. Two Ordinal Variables (Education, Interest), MNAR, 4 Variables with NA} 
  \label{mult.mnar.4var} 
\begin{threeparttable}
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.3420} & \multicolumn{1}{c}{.3770} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0142} & \multicolumn{1}{c}{--.0133} & \multicolumn{1}{c}{--.0043} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0155} & \multicolumn{1}{c}{--.0131} & \multicolumn{1}{c}{--.0042} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0136} & \multicolumn{1}{c}{--.0131} & \multicolumn{1}{c}{--.0059} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0127} & \multicolumn{1}{c}{--.0130} & \multicolumn{1}{c}{--.0046} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0211} & \multicolumn{1}{c}{--.0185} & \multicolumn{1}{c}{--.0207} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.4890} & \multicolumn{1}{c}{.4830} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0180} & \multicolumn{1}{c}{--.0162} & \multicolumn{1}{c}{--.0167} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0172} & \multicolumn{1}{c}{--.0160} & \multicolumn{1}{c}{--.0157} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0170} & \multicolumn{1}{c}{--.0154} & \multicolumn{1}{c}{--.0170} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0170} & \multicolumn{1}{c}{--.0153} & \multicolumn{1}{c}{--.0172} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0233} & \multicolumn{1}{c}{--.0241} & \multicolumn{1}{c}{--.0169} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{16.6140} & \multicolumn{1}{c}{6.4810} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.2481} & \multicolumn{1}{c}{--.1034} & \multicolumn{1}{c}{--.0728} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.3174} & \multicolumn{1}{c}{--.1303} & \multicolumn{1}{c}{--.0812} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1555} & \multicolumn{1}{c}{--.0741} & \multicolumn{1}{c}{--.0589} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1568} & \multicolumn{1}{c}{--.0730} & \multicolumn{1}{c}{--.0565} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.3114} & \multicolumn{1}{c}{--.1513} & \multicolumn{1}{c}{--.0852} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{50.0410} & \multicolumn{1}{c}{52.8230} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.6020} & \multicolumn{1}{c}{--.4844} & \multicolumn{1}{c}{--.8613} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.8997} & \multicolumn{1}{c}{--.7259} & \multicolumn{1}{c}{--.9349} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.3103} & \multicolumn{1}{c}{--.2831} & \multicolumn{1}{c}{--.3702} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.2994} & \multicolumn{1}{c}{--.2702} & \multicolumn{1}{c}{--.3705} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.6726} & \multicolumn{1}{c}{--.6482} & \multicolumn{1}{c}{--.6213} \\
\hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table}

The results for the interval variables are consistent with the previous analyses. In addition, note that `na.omit` performs better than `hd.ord` for `Age` across all three datasets and for ANES `Inc`. Once more, `hd.ord` consistently performs slightly worse with more than two ordinal variables: `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Inc"]`, `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Inc"]`, `r mult.mnar.4.frame[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Inc"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Inc"]`, `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Inc"]`, `r mnar.5.frame[mnar.5.meth == "hd.ord" & mnar.5.var == "Inc"]` for `Inc` and `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Age"]`, `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Age"]`, `r mult.mnar.4.frame[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Age"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Age"]`, `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Age"]`, `r mnar.5.frame[mnar.5.meth == "hd.ord" & mnar.5.var == "Age"]` for `Age`.


```{r MULT MNAR 11 Variables, include=FALSE}

mult.mnar.11var.anes <- read.csv("data/anes/mnar/results/anes.mnar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mult.mnar.11var.cces <- read.csv("data/cces/mnar/results/cces.mnar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus
mult.mnar.11var.frame <- read.csv("data/framing/mnar/results/framing.mnar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% dropZero.addPlus

mult.mnar.11var.anes$diff[mult.mnar.11var.anes$method == "true"] <- mult.mnar.11var.anes$value[mult.mnar.11var.anes$method == "true"]
mult.mnar.11var.cces$diff[mult.mnar.11var.cces$method == "true"] <- mult.mnar.11var.cces$value[mult.mnar.11var.cces$method == "true"]
mult.mnar.11var.frame$diff[mult.mnar.11var.frame$method == "true"] <- mult.mnar.11var.frame$value[mult.mnar.11var.frame$method == "true"]

levels(mult.mnar.11var.anes$method) <- levels(mult.mnar.11var.cces$method) <- levels(mult.mnar.11var.frame$method) <-
  levs


mult.un.meth <- mult.mnar.11var.anes$method %>% unique
mult.un.meth.len <- mult.un.meth %>% length
mult.un.vars <- c(mult.mnar.11var.anes$variable %>% as.character,
                  mult.mnar.11var.cces$variable %>% as.character,
                  mult.mnar.11var.frame$variable %>% as.character) %>%
  unique
mult.un.vars.len <- mult.un.vars %>% length
mult.un.anes.len <- mult.mnar.11var.anes$variable %>% unique %>% length
mult.un.cces.var <- mult.mnar.11var.cces$variable %>% unique %>% as.character
mult.un.cces.len <- mult.un.cces.var %>% length
mult.un.frame.var <- mult.mnar.11var.frame$variable %>% unique %>% as.character
mult.un.frame.len <- mult.un.frame.var %>% length

mult.variable <- rep(mult.un.vars, each = mult.un.meth.len)
mult.method <- rep(mult.un.meth, mult.un.vars.len) %>% as.character
mult.meth.len <- mult.method %>% length

mult.mnar.11var <- cbind(mult.method, mult.variable) %>% as.data.frame

mult.mnar.11var$mult.anes.col <- c(mult.mnar.11var.anes$diff,
                                  rep("---",
                                      mult.un.meth.len * (mult.un.vars.len - mult.un.anes.len)))
mult.mnar.11var$mult.cces.col <- rep("---", mult.meth.len) %>% as.character
mult.mnar.11var$mult.frame.col <- rep("---", mult.meth.len) %>% as.character

for(i in 1:(mult.un.cces.len)){
  mult.mnar.11var$mult.cces.col[mult.mnar.11var$mult.variable == mult.un.cces.var[i]] <- mult.mnar.11var.cces$diff[mult.mnar.11var.cces$variable == mult.un.cces.var[i]]
  mult.mnar.11var$mult.frame.col[mult.mnar.11var$mult.variable == mult.un.frame.var[i]] <- mult.mnar.11var.frame$diff[mult.mnar.11var.frame$variable == mult.un.frame.var[i]]
}

colnames(mult.mnar.11var) <- col.names

# to make the in-text citations shorter
mult.mnar.11.anes <- mult.mnar.11var$ANES
mult.mnar.11.cces <- mult.mnar.11var$CCES
mult.mnar.11.frame <- mult.mnar.11var$Framing
mult.mnar.11.meth <- mult.mnar.11var$Method
mult.mnar.11.var <- mult.mnar.11var$Variable

stargazer(mult.mnar.11var, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. Two Ordinal Variables (Education, Interest), MNAR, 11 Variables with NA",
          label = "mult.mnar.11var")

```

<!--
MNAR 11 Var
Vars the same as for MAR 11 Var
	Binary
		amelia and mice perform better, but often not by much. Occasionally, hd.ord eclipses them (.0057 hd.ord vs. .0060 mice Male ANES; .0057 hd.ord vs. .0059 amelia Male framing)
		hd.ord consistently gets slightly worse when compared to MNAR 12 Var:		
		MNAR 12 Var hd.ord: Dem .0049, .0049, .0013. Male .0053, .0051,. 0051
		MNAR 11 Var hd.ord: Dem .0053, .0053, .0024. Male .0057, .0056, .0057
	Ordinal framing
		hd.ord worst for Media but better than hot.deck for Participation
		mice and amelia performs best by a significant margin
		na.omit as far off as for MNAR 12 Var
		hd.ord consistently gets slightly worse when compared to MNAR 12 Var:		
		MNAR 12 Var hd.ord: Media .0152, Participation .0115
		MNAR 11 Var hd.ord: Media .0164. Participation .0130
	interval
		Consistent with previous analyses
		na.omit better than hd.ord for Age for all ds. na.omit also best overall across all methods for Age CCES
		hd.ord consistently gets slightly worse when compared to MNAR 12 Var:		
		MNAR 12 Var hd.ord: Inc .1046, .0421, .0262. Age .3081, .2251, .2989
		MNAR 11 Var hd.ord: Inc .1142, .0473, .0291. Age .3329, .2424, .3271
-->

Table \ref{mult.mnar.11var} shows the results of imputing all datasets with two `polr`-treated variables MNAR for 11 amputed variables. For the binary variables, `amelia` and `mice` perform better, but often not by much. Occasionally, `hd.ord` eclipses them (`r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Male"]` `hd.ord` vs. `r mult.mnar.11.anes[mult.mnar.11.meth == "mice" & mult.mnar.11.var == "Male"]` `mice` ANES `Male`; `r mult.mnar.11.frame[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Male"]` `hd.ord` vs. `r mult.mnar.11.frame[mult.mnar.11.meth == "amelia" & mult.mnar.11.var == "Male"]` `amelia` Framing `Male`). As was the case in the comparison between the MNAR analysis with four amputed variables and the MNAR analysis with five amputed variables, `hd.ord` consistently demonstrates slightly worse results in the switch from 12 (Table \ref{mnar.12var}) to 11 and one to two `polr`-treated variables: `r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Dem"]`, `r mult.mnar.11.cces[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Dem"]`, `r mult.mnar.11.frame[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Dem"]` vs. `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Dem"]`, `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Dem"]`, `r mnar.12.frame[mnar.12.meth == "hd.ord" & mnar.12.var == "Dem"]` for `Dem` and `r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Male"]`, `r mult.mnar.11.cces[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Male"]`, `r mult.mnar.11.frame[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Male"]` vs. `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Male"]`, `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Male"]`, `r mnar.12.frame[mnar.12.meth == "hd.ord" & mnar.12.var == "Male"]` for `Male`.

\ssp

\footnotesize

\begin{longtable}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
  \caption{Accuracy of Multiple Imputation Methods. Two Ordinal Variables (Education, Interest), MNAR, 11 Variables with NA} 
  \label{mult.mnar.11var} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.3420} & \multicolumn{1}{c}{.3770} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0050} & \multicolumn{1}{c}{--.0053} & \multicolumn{1}{c}{--.0022} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0053} & \multicolumn{1}{c}{--.0053} & \multicolumn{1}{c}{--.0024} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0047} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{--.0024} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0044} & \multicolumn{1}{c}{--.0048} & \multicolumn{1}{c}{--.0019} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0097} & \multicolumn{1}{c}{--.0097} & \multicolumn{1}{c}{--.0109} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.4890} & \multicolumn{1}{c}{.4830} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0061} & \multicolumn{1}{c}{--.0058} & \multicolumn{1}{c}{--.0061} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0057} & \multicolumn{1}{c}{--.0056} & \multicolumn{1}{c}{--.0057} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0060} & \multicolumn{1}{c}{--.0054} & \multicolumn{1}{c}{--.0059} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0060} & \multicolumn{1}{c}{--.0054} & \multicolumn{1}{c}{--.0060} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0088} & \multicolumn{1}{c}{--.0116} & \multicolumn{1}{c}{--.0068} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{16.6140} & \multicolumn{1}{c}{6.4810} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0841} & \multicolumn{1}{c}{--.0349} & \multicolumn{1}{c}{--.0258} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1142} & \multicolumn{1}{c}{--.0473} & \multicolumn{1}{c}{--.0291} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0540} & \multicolumn{1}{c}{--.0250} & \multicolumn{1}{c}{--.0206} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0549} & \multicolumn{1}{c}{--.0249} & \multicolumn{1}{c}{--.0199} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.2112} & \multicolumn{1}{c}{--.0982} & \multicolumn{1}{c}{--.0342} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{50.0410} & \multicolumn{1}{c}{52.8230} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.2124} & \multicolumn{1}{c}{--.1607} & \multicolumn{1}{c}{--.2819} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.3329} & \multicolumn{1}{c}{--.2424} & \multicolumn{1}{c}{--.3271} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.1194} & \multicolumn{1}{c}{--.1135} & \multicolumn{1}{c}{--.1257} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.1141} & \multicolumn{1}{c}{--.1102} & \multicolumn{1}{c}{--.1244} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.2978} & \multicolumn{1}{c}{--.0974} & \multicolumn{1}{c}{--.2074} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{.0790} & \multicolumn{1}{c}{.0950} & \multicolumn{1}{c}{.0690} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0036} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{--.0024} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{--.0042} & \multicolumn{1}{c}{--.0031} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0041} & \multicolumn{1}{c}{--.0044} & \multicolumn{1}{c}{--.0012} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0037} & \multicolumn{1}{c}{--.0041} & \multicolumn{1}{c}{--.0011} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Black} & \multicolumn{1}{c}{--.0050} & \multicolumn{1}{c}{--.0066} & \multicolumn{1}{c}{--.0054} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{.6610} & \multicolumn{1}{c}{.4370} & \multicolumn{1}{c}{.7430} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0042} & \multicolumn{1}{c}{--.0057} & \multicolumn{1}{c}{--.0027} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0036} & \multicolumn{1}{c}{--.0058} & \multicolumn{1}{c}{--.0024} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0034} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{--.0025} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0033} & \multicolumn{1}{c}{--.0044} & \multicolumn{1}{c}{--.0026} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Empl} & \multicolumn{1}{c}{--.0022} & \multicolumn{1}{c}{--.0122} & \multicolumn{1}{c}{--.0033} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{.6460} & \multicolumn{1}{c}{.6420} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{--.0044} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0047} & \multicolumn{1}{c}{--.0042} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0043} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0043} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Religious} & \multicolumn{1}{c}{--.0055} & \multicolumn{1}{c}{--.0077} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{.5290} & \multicolumn{1}{c}{.6310} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0047} & \multicolumn{1}{c}{--.0043} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0042} & \multicolumn{1}{c}{--.0040} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0046} & \multicolumn{1}{c}{--.0039} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{--.0039} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Married} & \multicolumn{1}{c}{--.0120} & \multicolumn{1}{c}{--.0095} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{.6820} & \multicolumn{1}{c}{.7010} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0037} & \multicolumn{1}{c}{--.0035} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0030} & \multicolumn{1}{c}{--.0030} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0031} & \multicolumn{1}{c}{--.0033} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0030} & \multicolumn{1}{c}{--.0033} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{OwnHome} & \multicolumn{1}{c}{--.0112} & \multicolumn{1}{c}{--.0088} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{.0830} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0047} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0047} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0045} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0044} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Rally} & \multicolumn{1}{c}{--.0080} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{.1390} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0057} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0060} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0054} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0053} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Donate} & \multicolumn{1}{c}{--.0122} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0420} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0027} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0026} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0028} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0027} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Gay} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0037} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.1910} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0067} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0064} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0057} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0056} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{StudLoans} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0082} & \multicolumn{1}{c}{---} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0550} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0027} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0028} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0012} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0012} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Hisp} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0037} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.3560} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0067} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0061} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0055} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0054} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Official} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0139} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.0440} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0026} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0025} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0025} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0021} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Stud} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0021} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{1.7240} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0156} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0164} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0110} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0110} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0345} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{.9540} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0138} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0130} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0111} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0105} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Participation} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{--.0348} \\ 
\hline \\[-1.8ex] 
\end{longtable} 

\dsp

\normalsize

Finally, the results for the interval variables confirm previous results, with `amelia` and `mice` demonstrating the best performance. In addition, note that `na.omit` delivers better results than `hd.ord` for `Age` for all datasets and represents the best method for CCES `Age`. Similar to the MNAR results for four variables, the performance of `hd.ord` in the MNAR analysis of 11 variables deteriorates in the switch from one to two ordinal variables: `r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Inc"]`, `r mult.mnar.11.cces[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Inc"]`, `r mult.mnar.11.frame[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Inc"]` vs. `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Inc"]`, `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Inc"]`, `r mnar.12.frame[mnar.12.meth == "hd.ord" & mnar.12.var == "Inc"]` for `Inc` and `r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Age"]`, `r mult.mnar.11.cces[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Age"]`, `r mult.mnar.11.frame[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Age"]` vs. `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Age"]`, `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Age"]`, `r mnar.12.frame[mnar.12.meth == "hd.ord" & mnar.12.var == "Age"]` for `Age`.




### Increased Percentage of Missingness {#ordmiss-results-increaseNA}

```{r Increasing Missingness Percentage, include=FALSE}

mar.5var.frame.20 <- mar.5var.frame
mar.5var.frame.50 <- read.csv("data/framing/mar/results/framing.mar.results.5var.1000n.1000it.50perc.csv") %>% .[,-1] %>% dropZero.addPlus
mar.5var.frame.80 <- read.csv("data/framing/mar/results/framing.mar.results.5var.1000n.1000it.80perc.csv") %>% .[,-1] %>% dropZero.addPlus

mar.5var.frame.20$diff[mar.5var.frame.20$method == "true"] <- 
  mar.5var.frame.50$diff[mar.5var.frame.50$method == "true"] <- 
  mar.5var.frame.80$diff[mar.5var.frame.80$method == "true"] <- 
  mar.5var.frame.20$value[mar.5var.frame.20$method == "true"]

levels(mar.5var.frame.20$method) <- 
  levels(mar.5var.frame.50$method) <- 
  levels(mar.5var.frame.80$method) <- 
  levs

mar.5var.old.frame <- cbind(mar.5var.frame.20[,c(1,2,4)], mar.5var.frame.50[,4], mar.5var.frame.80[,4])
colnames(mar.5var.old.frame) <- c("Method", "Variable", "20% NA", "50% NA", "80% NA")

# to make the in-text citations shorter
frame.NA20 <- mar.5var.old.frame$`20% NA`
frame.NA50 <- mar.5var.old.frame$`50% NA`
frame.NA80 <- mar.5var.old.frame$`80% NA`
frame.80.meth <- mar.5var.old.frame$Method
frame.80.var <- mar.5var.old.frame$Variable

stargazer(mar.5var.old.frame, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods for Increasing Percentages of Missing Data (Framing Experiment Data)",
          label = "mar.5var.old.frame")

```

<!--
Increased Missingness
Framing MAR 5 Var
	Binary
		hd.ord worst for all percentages for all vars
		hot.deck outperforms amelia and mice for Dem and Male CCES
		amelia and mice virtually identical
	Ordinal
		hd.ord worst for all percentages for all vars
		amelia and mice far best and virtually identical
	interval
		hd.ord worst for all percentages for all vars
		amelia and mice far best
		amelia outperforms mice for all percentages for all vars	
-->

This brief section shows the imputation results when the percentage of missingness is increased. I conduct this here MAR for five variables in the Framing data. The results are shown in Table \ref{mar.5var.old.frame}. As was to be expected at this point, `hd.ord` displays the worst performance for all percentages for all binary variables. 

\begin{table}[!ht] \centering 
  \caption{Accuracy of Multiple Imputation Methods for Increasing Percentages of Missing Data (Framing Experiment Data)} 
  \label{mar.5var.old.frame} 
\begin{threeparttable}
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{20\% NA} & \multicolumn{1}{c}{50\% NA} & \multicolumn{1}{c}{80\% NA} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{.4660} & \multicolumn{1}{c}{.4660} & \multicolumn{1}{c}{.4660} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0016} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0008} & \multicolumn{1}{c}{+.0018} & \multicolumn{1}{c}{+.0037} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{+.0004} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{+.0004} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & \multicolumn{1}{c}{--.0340} & \multicolumn{1}{c}{--.0841} & \multicolumn{1}{c}{--.1393} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{.5260} & \multicolumn{1}{c}{.5260} & \multicolumn{1}{c}{.5260} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0004} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0005} & \multicolumn{1}{c}{+.0009} & \multicolumn{1}{c}{+.0014} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{+.0000} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0003} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0001} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{--.0006} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{--.0256} & \multicolumn{1}{c}{--.0632} & \multicolumn{1}{c}{--.1051} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{3.2170} & \multicolumn{1}{c}{3.2170} & \multicolumn{1}{c}{3.2170} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0213} & \multicolumn{1}{c}{--.0604} & \multicolumn{1}{c}{--.0908} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0248} & \multicolumn{1}{c}{--.0679} & \multicolumn{1}{c}{--.1007} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0002} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{+.0003} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0003} & \multicolumn{1}{c}{--.0004} & \multicolumn{1}{c}{+.0001} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Interest} & \multicolumn{1}{c}{--.0714} & \multicolumn{1}{c}{--.1833} & \multicolumn{1}{c}{--.3219} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{3.0890} & \multicolumn{1}{c}{3.0890} & \multicolumn{1}{c}{3.0890} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0119} & \multicolumn{1}{c}{--.0371} & \multicolumn{1}{c}{--.0594} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.0192} & \multicolumn{1}{c}{--.0550} & \multicolumn{1}{c}{--.0805} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{+.0003} & \multicolumn{1}{c}{+.0001} & \multicolumn{1}{c}{+.0009} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{+.0014} & \multicolumn{1}{c}{+.0028} & \multicolumn{1}{c}{+.0039} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Inc} & \multicolumn{1}{c}{--.1367} & \multicolumn{1}{c}{--.3076} & \multicolumn{1}{c}{--.4850} \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{37.9120} & \multicolumn{1}{c}{37.9120} & \multicolumn{1}{c}{37.9120} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.3650} & \multicolumn{1}{c}{--1.0406} & \multicolumn{1}{c}{--1.5789} \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.3923} & \multicolumn{1}{c}{--1.1319} & \multicolumn{1}{c}{--1.6962} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.0039} & \multicolumn{1}{c}{+.0043} & \multicolumn{1}{c}{--.0005} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--.0049} & \multicolumn{1}{c}{+.0047} & \multicolumn{1}{c}{+.0034} \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{--1.0641} & \multicolumn{1}{c}{--2.3051} & \multicolumn{1}{c}{--3.4885} \\  
\hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table}

`amelia` and `mice` show virtually identical results. Note that `hot.deck` appears to outperform `amelia` and `mice` for CCES `Dem` and `Male`. `hd.ord` also represents the worst method for all percentages for all ordinal and interval variables. `amelia` and `mice` show virtually identical results and far outperform the other methods for the ordinal variables. Note that `amelia` also consistently outperforms `mice` for all percentages for all interval variables.




### Speed {#ordmiss-results-speed}

```{r Runtimes 5/12 Variables MAR, include=FALSE}

run.5var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.5var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.5var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.12var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.12var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.12var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.5var.12var <- data.frame(matrix(NA, 4, 7))
colnames(run.5var.12var) <- c("Method", "ANES5", "CCES5", "Framing5", "ANES12", "CCES12", "Framing12")
run.5var.12var$Method <- run.5var.anes$V2

runs <- list(run.5var.anes, run.5var.cces, run.5var.frame,
             run.12var.anes, run.12var.cces, run.12var.frame)

for(x in 1:length(runs)){
  run.5var.12var[1:4, x+1] <- runs[[x]][,2]
}

run.meth <- run.5var.12var$Method

# so I can use how much slower amelia is than hd.ord
var.hd.am.div <- sapply(2:7, 
                    function(x)
                      run.5var.12var[run.meth == "amelia", x] / 
                      run.5var.12var[run.meth == "hd.ord", x]) 

# so I can use how much slower mice is than hd.ord
var.hd.mi.div <- sapply(2:7, 
                    function(x)
                      run.5var.12var[run.meth == "mice", x] / 
                      run.5var.12var[run.meth == "hd.ord", x]) 

stargazer(run.5var.12var, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes) by Number of Imputed Variables",
          label = "runtimes5var12var")

```

<!--
Speed
Analyzed by number of imputed variables for all ds (MAR)
	hd.ord and hot.deck much faster and virtually identical
	amelia consistently several times slower for all ds for both numbers of imputed vars
	mice much, much worse than all other methods across the board
Analyzed by percentage of missingness for framing data (MAR 5 Var)
	amelia and mice get closer as the percentage of missingess increases
-->

This section shows the running times for all methods. Specifically, I outline the speed differences by the number of imputed variables for all datasets (Table \ref{runtimes5var12var}) and by the percentage of missingness for the Framing data (Table \ref{run.all.perc}). Both analyses are conducted MAR, with the latter only for five imputed variables. All running times are given in minutes and apply to all 1,000 amputation/imputation iterations combined.

We can see in Table \ref{runtimes5var12var} that `hd.ord` and `hot.deck` show virtually identical running times for all datasets for both numbers of imputed variables. This is to be expected as both methods are very similar in terms of their code build-up. More importantly, however, we observe that both methods are much faster than `amelia` and `mice`: `amelia` is at least `r var.hd.am.div %>% min %>% round(., digits = 1)` times slower than `hd.ord` (for the Framing data for both numbers of imputed variables). The highest difference amounts to `r var.hd.am.div %>% max %>% round(., digits = 1)` (for the CCES data with 12 imputed variables). `mice`, however, shows by far the highest running times. At its worst, `mice` is `r var.hd.mi.div %>% max %>% round(., digits = 1)` times slower than `hd.ord` (for the CCES data with 12 imputed variables). At its best, this deficit still amounts `r var.hd.mi.div %>% min %>% round(., digits = 1)` times slower (for the Framing data with five imputed variables). 

\begin{table}[!htbp] \centering 
  \caption{Runtimes of Multiple Imputation Methods (in Minutes) by Number of Imputed Variables} 
  \label{runtimes5var12var} 
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{5 Variables with NA} & \multicolumn{3}{c}{12 Variables with NA}\\ 
\cline{2-7}  \\[-1.8ex]
 & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{Framing} \\ 
\cline{2-4} 
\cline{5-7}  \\[-1.8ex]
\multicolumn{1}{c}{hd.ord} & 2.632 & 2.778 & 2.559 & 2.614 & 2.679 & 2.555 \\ 
\multicolumn{1}{c}{hot.deck} & 2.628 & 2.786 & 2.567 & 2.640 & 2.690 & 2.582 \\ 
\multicolumn{1}{c}{amelia} & 9.052 & 10.611 & 8.079 & 9.038 & 10.345 & 8.109 \\ 
\multicolumn{1}{c}{mice} & 50.445 & 57.205 & 48.436 & 104.143 & 113.390 & 103.953 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}


```{r Runtimes Increased Missingness MAR Code, include=FALSE}

run.20 <- run.5var.frame
run.50 <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.5var.1000n.1000it.50perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.80 <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.5var.1000n.1000it.80perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.all.perc <- cbind(run.20$V1, run.50$V1, run.80$V1) %>% as.data.frame %>% round(., digits = 2)
run.all.perc$V4 <- run.20$V2 %>% as.character
run.all.perc <- run.all.perc[,c(4, 1:3)]
colnames(run.all.perc) <- c("Method", "20% NA", "50% NA", "80% NA")

perc.hd.am.div <- run.all.perc[3,2:4] %>% as.numeric / run.all.perc[1,2:4] %>% as.numeric
perc.hd.mi.div <- run.all.perc[4,2:4] %>% as.numeric / run.all.perc[1,2:4] %>% as.numeric

```


Table \ref{run.all.perc} shows that `hd.ord` and `hot.deck` remain the fastest methods across all percentages of missingness, but the gap to `amelia` and `mice` narrows as the missingness increases. `amelia` improves from `r perc.hd.am.div %>% max %>% round(., digits = 1)` times slower than `hd.ord` for 20 percent NA to `r perc.hd.am.div %>% min %>% round(., digits = 1)` times slower for 80 percent NA. Similarly, `mice` speeds up from `r perc.hd.mi.div %>% max %>% round(., digits = 1)` to `r perc.hd.mi.div %>% min %>% round(., digits = 1)` times slower. 


```{r Runtimes Increased Missingness MAR Table, results='asis', echo=FALSE}

stargazer(run.all.perc, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes) by Percentage of Missingness (Framing Experiment Data)",
          label = "run.all.perc")

```




## Conclusion {#ordmiss-conclusion}


Make sure to include in the MNAR discussion that MNAR is next to impossible to prove/definitively assert 'in the wild' 

--> mice.pdf: 6.2. Sensitivity analysis under MNAR


Amelia is very finicky when it comes to collinearity


`optimizeSD` in `hot.deck`: Logical indicating whether the sdCutoff parameter should be optimized such that the smallest possible value is chosen that produces no thin cells from which to draw donors. Thin cells are those where the number of donors is less than m --> potential explanation why `hot.deck` isn't better than the others could be something to do with the warnings saying there aren't enough donors


<!--
What doesn't work

-- Increasing percentage of NAs == worse `hd.ord` performance
-- Ordinal and interval variables == worse `hd.ord` performance
-- High number of observations == reduced number of iterations (crashes and/or RAM maxing out)
-- High number of observations == makes `amelia` faster relative to `hd.ord`
-- 17 ANES education levels == increases needed number of iterations
-- 17 ANES education levels == causes `amelia` to stop on CO and Jeff
-- 10,000 iterations == results with 1,000 iterations are just as good
-- `ampute` with `bycases=FALSE` and `cont=FALSE` == worse `hd.ord` performance, good `na.omit` performance
-- `own.NA` == `na.omit` performs best (currently in appendix)
-- `own.NA.rows`== pretty much everything is zero, incl. `na.omit`; `mice` is awful (currently in appendix)
-- Running `hd.ord` with `method = p.draw` == worse `hd.ord` performance
-- Running `hd.ord` with `method = p.draw` == only works with only binary vars in the data
-- Increasing `sdCutoff` == only does something with only binary vars in the data
-- Only binary vars in data == no gain in `hd.ord` performance



What works

-- Results for binary variables == equal performance of `hd.ord`, `mice`, `amelia`
-- Increasing number of variables with NAs (all, not just binary) == better `hd.ord` performance
-- 1000 observations in datasets == increases `amelia` running time
-- MNAR == MAR in terms of `hd.ord` performance
-- Multiple ordinal variables == same performance as with one ordinal variable
-->
