# INTRODUCTION {#intro}

News outlets, political campaigns, and survey institutes spend millions of dollars every year in the attempt to uncover what people think. The exact nature of public opinion is highly sought after information. It is not hard to see why: Public opinion is central to the functioning of democracy. It consists of the desires and wants of people. Their collective views and opinions determine who gets elected, what policies are enacted, and how society is structured. We need to know what people want, how they think, how they feel, what they support, and what they oppose because these wants and desires directly inform policy making and political decisions. We do so by conducting surveys. Surveys are a central part of public opinion. Without surveys, we would not know what people think. Survey collect data from a population or a sample population to gain information and insights into political issues and are ubiquitous in today's media environment. Research institutes like the Pew Research Center conduct dozens of surveys on numerous topics every year. In the run-up to the 2020 presidential election, the number of opinion polls conducted in swing states easily reached triple digits.

In political science, surveys are often used to analyze political attitudes. To establish causal effects, researchers use survey experiments. Like surveys, survey experiments collect background information on people's demographics. Unlike surveys, however, they also include attempts to uncover treatment effects on public opinion by testing one or several hypotheses. Similar to a laboratory setting, this involves exposing respondents to differing conditions in order to determine the effect of the intervention in question. In medical research, a treatment may take the form of a pill. In a survey context, a treatment can take the form of particular types of questions, different question wordings, or exposure to a particular type of content, among many others. Survey experiments are one of the most powerful methodological tools in political science. By combining experimental design that provides clear causal inference with the flexibility of the survey context as a site for attitude research, survey experiments can be used in almost any field to study almost any question. 

Surveys and survey experiments are only as good as the analytical techniques we as researchers use, though. This applies particularly to how we phrase and measure the questions we ask respondents. If a question is worded confusingly, for instance with a double negative, we cannot gain much insight from it. If we provide unsuitable response options, for example in multiple choice questions, we might obtain inaccurate information. Take for instance a potential question inquiring after support for the legalization of marijuana. Say this question only offered the response choices "Yes, I support legalization" and "No, I don't support legalization". Are these good choices? Most likely not. Restricting the range of possible answers to a binary choice denies respondents the chance to voice more complex opinions. Some people might support legalization a little bit. Others might be highly enthusiastic about it. Yet others again might rigidly oppose it, while a fourth group is ambivalent about the subject. How we word questions and how we measure responses matters. This does not end at questionnaire design. How we use the collected information for subsequent analysis is just as, if not more, important. The statistical techniques and methods we use to analyze survey and survey experiment data crucially influence the results we obtain. This applies particularly to different types of variables. While some variables are comparatively easy to wrangle, such as numerical ones, others provide a greater challenge. For one particular type, namely ordinal variables, I consider some of our current analytical techniques to be insufficient.

Ordinal variables are part of the larger framework of categorical variables. Categorical variables represent types of data which are commonly divided into three groups: nominal, interval, and ordinal. Nominal variables are categorical variables with two or more categories that are not intrinsically ordered. Examples include gender (Female, Male, Transgender etc.), race (African-American, White, Hispanic etc.), and party ID (Democrat, Republican, Independent) where the categories cannot be ordered sensibly into highest or lowest. Interval variables are ordered categorical variables with evenly spaced values. Examples include income (\$20,000, \$40,000, \$60,000, \$80,000 etc.), where the distance between \$20,000 and \$40,000 is the same as the distance between \$60,000 and \$80,000. Ordinal variables are ordered categorical variables where the spacing between values is not the same. Examples include education (Elementary School, Some High School, High School Graduate etc.) where the distance between "Elementary School" and "Some High School" is likely different than the distance between "Some High School" and "High School Graduate". Each subsequent category has quantitatively more education than the previous one, but the exact measure of the distances between the categories is not known.  

Current practice often does not take ordinal variable information into account. One such technique involves turning ordinal categories into categorical indicators. Researchers here create a new binary variable for an education category, for instance "High School Graduate". While it removes the issue of uneven distances, this approach ignores the ordered nature contained in ordinal variables. Education categories are treated like nominal variables, i.e. we would wrongly assume that the ordering of education categories is arbitrary. Ordinal variables are also often made numerical for analytic purposes. This is problematic because of their unevenly spaced categories. If we turn the education categories "Elementary School", "Some High School", and "High School Graduate" into the numerical values 1, 2, and 3, we wrongly assume that the distances between the education categories correspond to these evenly spaced values. Do the numbers 1 to 3 really represent the distances between these categories? Perhaps the true spacing between some of the categories is so narrow they should not even be separate categories at all. We cannot answer this by making an arbitrary assumption that may not be justified by the data. Important information might be lost for one of the major predictors of political opinion, which could lead to distortion [@obrien_1981_using]. To truly use the ordinal nature of a variable, we need to use its inherent unevenly spaced order to make a more underlying description of the data possible [@agresti_2010_analysis]. To fill this gap, I propose an ordered probit approach that estimates an ordinal variable's underlying latent continuous structure. I use this approach to develop two methods: The first method improves the use of ordinal variables in blocking in survey experiments. The second method provides a new way to treat missing survey data with ordinal variables. I subsequently apply both methods in an online survey experiment on political framing.

Chapter \ref{ordblock} outlines the blocking method. Survey experiments depend on balance to enable causal estimates. In order to identify potential causal effects, all treatment groups in a survey experiment need to be balanced on the potential outcomes. We get there by making all treatment groups look the same in terms of covariates. While this can be achieved with randomization, this typically leads to problems for small samples. Blocking, i.e. arranging participants in groups that are equal in terms of participants' covariates and using random allocation within these groups, can alleviate such worries. I use an ordered probit approach to estimate an assumed underlying latent continuous structure underneath ordinal variables, which results in a new set of data-driven variable categories that can then be used for blocking. This avoids arbitrary numerical conversion and fully utilizes the ordinal information provided in the unevenly spaced variable categories. I test this method by blocking external survey data, conducting variance tests, and running a placebo regression. \textcolor{red}{Findings are overall positive, with most results supporting the notion that the re-estimation of ordinal variable categories with an ordered probit approach might matter}.

Chapter \ref{ordmiss} describes a new method to treat missing survey data with ordinal variables. Missing data are ubiquitous in survey research [@allison_2002_missing;@raghunathan_2016_missing]. This poses a big problem for researchers because data can typically not be analyzed with statistical software if they contain missing values [@little_2002_statistical;@molenberghs_2007_missing]. Scholars have developed several ways to treat missing data, among them multiple imputation, which accounts for and incorporates uncertainty around the estimated imputations through repeated draws [@andridge_2010_review; @graham_1999_performance; @schafer_2002_missing; @white_2011_multiple]. I develop a method to impute discrete missing data specifically for the specific circumstances of ordinal variables. This method is based on multiple hot deck imputation and uses the same ordered probit approach as in chapter \ref{ordblock}. I apply a scaled solution with newly estimated numerical thresholds from an assumed underlying latent continuous variable to measure the distances between the categories and calculate affinity scores. I test this method by imputing artificially inserted missing values in external survey data and comparing its performance to common imputation techniques. \textcolor{red}{The results show that the method performs worse than existing techniques overall, with exceptions in specific cases.}

Chapter \ref{framing} applies both methods in an online survey experiment on political framing. Framing is the practice of presenting an issue to affect the way people see it [@chong_framing_2007]. It reorganizes existing information already present in people's minds and attempts to direct people's attention towards particular considerations [@druckman_2003_framing]. While numerous experiments have shown that frames can have substantial influence on people's opinions, we still don't know what aspects cause this influence. I provide an avenue of clarification by testing the influence of morality and self-interest in framing in direct juxtaposition. The findings show tentative evidence for the importance of morality in issue-opposing frames.








