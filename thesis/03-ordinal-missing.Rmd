# QUALITY COMPARISON OF MAJOR MISSING DATA SOLUTIONS WITH A PROPOSED NEW METHOD FOR ORDINAL VARIABLES {#ordmiss}

## Introduction {#ordmiss-intro}

```{r include=FALSE}
# function to make all diff columns absolute values
abs.diff <- function(df){
  df$diff <- df$diff %>% abs
  return(df)
}

# function to add a plus/minus sign for the diff numeric columns in data
addPlus <- function (df, digits = 4){
  x <- dplyr::select_if(df, is.numeric)
  for(i in 1:ncol(x)){
    x[,i] <- round(as.numeric(x[,i]), digits)
    x[,i] <- sprintf(paste0("%.", digits, "f"), x[,i])
    # x[,i] <- gsub("^0(?=\\.)|(?<=-)0", "", x[,i], perl = TRUE) # took it out because Jeff wants leading zeros ...
  }
  df[, colnames(x)] <- x
  
  for(i in 1:nrow(df)){
    if(grepl("-", df$diff[i]) == FALSE){
      df$diff[i] <- paste0("+", df$diff[i])
    }else{
      df$diff[i] <- paste0("-", df$diff[i])
    }
  }
  return(df)
}

```

```{r Levels Overall, include=FALSE}
levs <- c("amelia", "hot.deck", "hd.ord", "mice", "na.omit", "true")
col.names <- c("Method", "Variable", "ANES", "CCES")
# col.names <- c("Method", "Variable", "ANES", "CCES", "Framing")
```


Missing data are ubiquitous in survey research [@allison_2002_missing;@raghunathan_2016_missing]. Respondents frequently refuse to answer questions, select "Don't Know" as a response option, or drop out during the response collection process [@honaker_2010_what]. Missingness in data sets poses a big problem since such data cannot be appropriately analyzed with statistical software without pre-processing [@little_2002_statistical;@molenberghs_2007_missing]. Scholars have developed several ways to treat missing data. These can be roughly categorized into deletion, single imputation, and multiple imputation. 

Deletion, also called case-wise deletion, list-wise deletion or complete case analysis, simply removes all observations with missing values from the sample. Single imputation concerns the replacement of missing values with substitute estimates such as the mean, regression coefficients, or values from randomly drawn 'similar' respondents in the data. Multiple imputation estimates missing values from conditional distributions and subsequently averages the results into a single parameter of inference [@rubin_1976_inference;@king_2001_analyzing;@fay_1996_alternative].

Multiple imputation has become the state of the art in missing data management since it accounts for and incorporates uncertainty around the estimated imputations through repeated draws [@andridge_2010_review; @graham_1999_performance; @schafer_2002_missing; @white_2011_multiple]. This is missing from single imputation techniques which treat the single estimated replacement value as a de-facto data entry on par with observed values. Uncertainty is not reflected in the imputed values, which leads to biased standard errors and confidence intervals [@kroh_2006_taking;@gill_2013_bayesian]. Similarly, list-wise deletion has been shown to induce bias with political data [@bodner_2008_what;@collins_2001_comparison;@pigott_review_2001;@rees_1997_methods;@reilly_1993_data].

However, parametric multiple imputation as applied by the most popular imputation packages in `R` is not necessarily always the most suitable method for all types of variables. For discrete data, multiple hot deck imputation, a combination of the single imputation method hot decking and multiple imputation, proves more precise as it avoids the common multiple imputation technique of imputing discrete data on a continuous scale [@gill_2012_have]. The latter turns discrete variables into continuous variables which changes their nature and can result in non-observable and biased imputation values with artificially smaller standard errors [@fuller_2005_deck; @kim_2004_finite; @kim_2004_fractional]. Multiple hot deck imputation on the other hand preserves the integrity of discrete data, does not change the size of standard errors, and produces more accurate imputations. It estimates affinity scores for each missing value to measure how similar a respondent with a missing value is to another respondent across all variables except the missing one. 

However, multiple hot deck imputation does not account for ordinal variables as its affinity score algorithm assumes even distances between categories in discrete data. This assumption is not warranted for ordinal variables. I propose a method designed to impute discrete missing data specifically from ordinal variables. Because of the success of multiple hot deck imputation in its applicability to missing data with discrete variables with a small number of categories [@gill_2012_have], this method is based on multiple hot deck imputation and adapted to account for the specific circumstances of ordinal variables. Based on the ordered probit model approach described in section \ref{ordblock-theory-op}, it applies a scaled solution with newly estimated numerical thresholds from an assumed underlying latent continuous variable to measure the distances between the categories and calculate affinity scores.

<!--
The following is an outline of the remainder of this chapter: Sections \ref{ordmiss-theory-mechanisms} to \ref{ordmiss-theory-singimpute} will discuss the theory behind missing data mechanisms and provide an overview of list-wise deletion as well as the most common single imputation methods. Section \ref{ordmiss-theory-multimpute} explains the basics of multiple imputation and outlines major `R` packages that implement it. These include `Amelia`, `mice`, `hot.deck` (applying multiple hot deck imputation), and my self-penned multiple hot deck imputation function for ordinal variables, `hd.ord`. Since single imputation is widely condemned as a general imputation method, my focus lies on multiple imputation. Section \ref{ordmiss-data} describes the survey data these packages/functions are tested on: a selection of the 2016 ANES and a subset of the 2016 CCES data. Each sample is complete and randomly amputed to insert missing data. Each of the four `R` packages/functions is applied to each data set and tested for accuracy and speed for different types of missingness and variables. Section \ref{ordmiss-results} shows the results and assesses the usefulness of the proposed approach. Finally, section \ref{ordmiss-conclusion} features concluding remarks.
-->


## Theory {#ordmiss-theory}

### Missing Data Mechanisms {#ordmiss-theory-mechanisms}

Let there be $\bm{X}$, an $n \times v$ matrix with data on $n$ respondents for $v$ variables. Let there also be the response indicator $\bm{R}$ as an $n \times v$ matrix with values of 0 or 1. Let their respective elements be denoted by $x_{ij}$ and $r_{ij}$, with $i = 1, ..., n$ and $j = 1, ..., v$. If $x_{ij}$ is observed, $r_{ij} = 1$. If $x_{ij}$ is missing, $r_{ij} = 0$. All elements where $r_{ij} = 0$ make up the missing data, $\bm{X^{miss}}$. All elements where $r_{ij} = 1$ make up the observed data, $\bm{X^{obs}}$. Together, $\bm{X^{obs}}$ and $\bm{X^{miss}}$ form the complete data $\bm{X}$. Missing data can then generally be described by $\text{prob}(\bm{R} = 0 | \bm{X^{obs}}, \bm{X^{miss}}, \bm{\theta})$, i.e. the probability of missing data depends on the observed data, the missing data, and a vector of unknown parameters. Depending on the mechanism by which data is missing, this expression can be further simplified. Data can be missing by three basic mechanisms: It can be missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). Researchers need to make assumptions about how the data came to be missing. 

The general missing data expression can be simplified under the MCAR assumption to $\text{prob}(\bm{R} = 0 | \bm{\theta})$, i.e. the generic probability of missing data, independent of the data themselves and only dependent on $\bm{\theta}$. A MCAR assumption in political science surveys is rare and requires justification as missing data most often occur systematically. Respondents are known to withhold sensitive data, for instance in the attempt to hide private information (income, sexual orientation) or out of fear of political or social repercussions in the community (union membership, support for polarizing political candidates) [@groves_survey_2009]. Such information is often not refused randomly but occurs only in specific subsets of respondents. Answers criticizing the authorities, for instance, tend to be missing in surveys in non-democratic states, while the state-loyal responses are present. 

<!--
The simplest and easiest case is missing completely at random (MCAR). Under the MCAR assumption, there is no process guiding the missingness; it is inserted truly at random. 
In statistical terms, this means both the unobserved and the missing data independently from each other form a random sample of the population and have the same underlying distribution. 
-->

It is more commonly assumed among researchers that data are MAR. MAR means missing data are related to the observed but not the unobserved data. In practical terms, this for instance means missing data on income can be related to observed data on education or occupation. Here, the missing data are not a random sample of the entire data. MAR transforms the general missing data expression to $\text{prob}(\bm{R} = 0 | \bm{X^{obs}}, \bm{\theta})$, i.e. the chances of missing data depend on the observed data and $\bm{\theta}$.

Finally, data can be MNAR.^[Data MNAR is also sometimes called 'non-ignorable' [see for instance @gill_2012_have and Allison, 2002].] This is the case when missing data are related to unknown and/or unobserved parameters. Continuing the example of missing data on income, under MNAR we do not observe data on education or occupation that can be used to fill the missing income data slot. In the case of data MNAR, the general missing data expression remains unchanged, $\text{prob}(\bm{R} = 0 | \bm{X^{obs}}, \bm{X^{miss}}, \bm{\theta})$, i.e. the missingness of data depends on the observed data, the missing data, and $\bm{\theta}$.

As mentioned above, the assessment of the missing data mechanism underlying any respective data comes from researchers and their understanding of the data generating process. The statistical methods to address missing data can be broadly categorized into deletion, imputation, and multiple imputation.




### List-Wise Deletion {#ordmiss-theory-delete}

One of the most common methods of handling missing data in quantitative political science is list-wise deletion. This involves the removal of any incomplete observations, thereby reducing the sample size. The resulting sample is then ready for analysis. 

List-wise deletion is not biased with data MCAR as it removes a random sample of the population, but it reduces the degrees of freedom and might hinder sub-group analysis [@allison_2002_missing; @king_2001_analyzing; @little_2002_statistical; @schafer_2002_missing]. When data are MAR or MNAR, list-wise deletion is always biased, since the observed data is tilted towards respondents with characteristics that make them more likely to respond. Whether this bias is trivial or substantial depends on the research and circumstances in question [@collins_2001_comparison; @diggle_1994_informative; @glynn_1993_multiple; @graham_1997_analysis; @robins_1998_semiparametric].

While the bias inserted by list-wise deletion in each individual data analysis may not necessarily be drastic, studies have shown that it can be so severe as to alter substantive conclusions [@brown_1994_efficacy; @graham_1996_maximizing; @honaker_2010_what; @wothke_2000_longitudinal]. Even if that were not or only rarely the case and most data were MCAR, reducing the sample size is generally never a recommended approach as, among other aspects, standard errors from regression models are inflated. As @king_2001_analyzing put it, the result of list-wise deletion "is a loss of valuable information at best and severe selection bias at worst" (p. 49). In `R`, list-wise deletion can be implemented with the base function `na.omit`. 




### Single Imputation {#ordmiss-theory-singimpute}

Single imputation means replacing missing data with substituted values, i.e. the structural opposite of deletion. Imputation requires a predictive distribution, based on the observed data, from which value substitutions are picked. Single imputation, regardless of its exact nature, is not recommended to impute missing data since, similar to deletion, it downward biases standard errors and confidence intervals [@honaker_2010_what]. Crucially, uncertainty is not reflected in the imputed values [@little_2002_statistical]. The following is a mere selection of the most common single imputation methods and makes no claim of completeness. Since single imputation is widely condemned as a general imputation method and as my focus lies on multiple imputation, they are also brief.


#### Mean {#ordmiss-theory-impute-mean}

Mean imputation, sometimes also called unconditional mean imputation, involves replacing missing values within cells with the mean of the observed values, so $\bm{X^{miss}} = \bm{\overline{X^{obs}}}$. While it does not change the mean of the sample, this method distorts the empirical distribution of $\bm{X}$, which in turn produces biased estimates of any non-linear quantities such as variances and covariances [@haitovsky_1968_missing]. It is also bound to be inaccurate in most cases, since few values generally fall exactly on the mean, and can be non-sensical for discrete variables [@efron_1994_missing]. Mean imputation can be done in many ways in `R`, for instance with the `impute` function in the `Hmisc` package or by setting `method = "mean"` in the `mice` function in the `mice` package. 

<!--
It is also very simple to execute in base `R`, e.g. for a sample data frame `df` and the numerical column `x`: $\text{df} \leftarrow \text{transform(df, x = ifelse(is.na(x), mean(x, na.rm=TRUE), x))}$.
-->


#### Regression {#ordmiss-theory-impute-regress}

Imputation by regression, sometimes also called conditional mean imputation, imputes missing values conditional on observed values. Researchers predict observed variable values based on other variables, while the fitted values from the regression model are then used to impute variable values where they are missing. Let there be an explanatory variable $x$ in a multiple regression model. Assume that $x$ contains missing values, $x^{miss}$, and observed values, $x^{obs}$. We regress $x^{obs}$ on the other explanatory variables and use the estimated equation to generate predicted values for $x^{miss}$, $x^{pred}$. $x^{pred}$ then replace $x^{miss}$, thus completing the data set. While more accurate than mean imputation, particularly for large samples with data MCAR, regression imputation nonetheless suffers from the same flaw that accompanies all single imputation approaches: Uncertainty is not reflected in the imputed values [@horton_2007_much].

Differing variations of imputation by regression exist in `R`, such as the `aregImpute` function in the `Hmisc` package, which performs additive regression, and setting `method = "norm.predict"` in the `mice` function to conduct linear regression. The `predict` function in base `R` also applies linear regression imputation.


#### Hot Decking {#ordmiss-theory-impute-hd}

Hot deck imputation was developed in the 1970s and replaces missing values with values from similar respondents in the sample [@ernst_1978_weighting; @ford_1983_overview]. It is called 'hot decking' as a reference to taking draws from a deck of matching computer punch-cards. The deck was 'hot' since it was currently being processed, as opposed to pre-collected or 'cold' data [@andridge_2010_review; @little_2002_statistical]. In the most general version, researchers select all respondents that are 'similar' to a respondent with missing data and randomly draw one of those respondents (with replacement) to fill in the missing value. The respondent with the initially missing value is termed the \textit{recipient}, while the 'similar' respondent is called the \textit{donor}. Variations of the method include hot decking within adjustment cells, by nearest neighbor, and sequentially ordered by a covariate [@cox_1980_weighted; @david_1986_alternative; @kaiser_1983_effectiveness; @kalton_1981_efficient; @rockwell_1975_investigation].

Contrary to mean or regression imputation, hot deck imputation preserves the integrity of the data, i.e. only actually observed values are used to fill in missing slots [@bailar_1997_comparison]. In both other single imputation methods, it is possible and sometimes even likely that missing values are replaced by values not found amongst the observed values. Contrary to regression imputation, hot decking also does not require a fitted model and is thus less vulnerable to model misspecification. However, hot decking does necessitate the existence of at least some donors for a respondent at every variable value that is missing. With a lot of missing data and few 'similar' matches, the accuracy of hot decking greatly decreases [@young_2011_survey]. Hot decking works best for discrete data as continuous data are very unlikely to be matched or 'similar', though the definition of what might constitute a 'similar' respondent is somewhat subjective [@marker_2002_large-scale]. As is the case with all single imputation methods, uncertainty is not reflected in the imputed values. Selecting the initial sample of 'similar' respondents and the subsequent random sampling from that subsampling are treated as factual responses, which leads to smaller standard errors and confidence intervals than statistically valid [@little_2002_statistical].

To my knowledge, there is currently no `R` package that applies single hot deck imputation. Nonetheless, variations of hot decking are still in use by some government statistics agencies such as the National Center for Education Statistics (for parts of the Current Population Survey) or the U.S. Bureau of the Census [@census_2002_current; @education-statistics_2002_nces].




### Multiple Imputation {#ordmiss-theory-multimpute}

Multiple imputation was invented by Rubin in the 1970s to account for the absence of uncertainty in single imputation methods and allow more accurate standard error estimates. It fills missing values with a predictive model that includes observed data and prior knowledge [@honaker_2010_what]. Over the time of its development, it has become the dominant sophisticated strategy for handling missing data [@dempster_1977_maximum; @glynn_1993_multiple; @heitjan_1991_ignorability; @little_2002_statistical; @rubin_1976_inference; @rubin_1986_multiple; @rubin_1987_multiple; @rubin_1996_multiple]. Multiple imputation involves three general steps:

\vspace{0.3cm}
\begin{adjustwidth*}{+0.5cm}{+0.5cm}
\ssp
\begin{enumerate}
\item \noindent Impute data with missing values $m$ times. This results in $i$ complete
 \begin{sloppypar}\hspace{0.5cm} data sets (with $i = 1, ..., m$).\end{sloppypar}
\item Analyze each of the $i$ complete data sets.
\item Combine the results from each of the $i$ analyses into one collective result.
\end{enumerate}
\end{adjustwidth*}
\vspace{0.3cm}

\ssp

\begin{figure}
\centering
\begin{tikzpicture}
    \node [block3] (ds) {\scriptsize{Data set with missing values}};
    \node [block2r, above right=1.2cm and 1.8cm of ds] (imp1) {\scriptsize{Imputed data set 1}};
    \node [block2r, above right=-0.4cm and 1.8cm of ds] (imp2) {\scriptsize{Imputed data set 2}};
    \node [block2r, below right=1.2cm and 1.8cm of ds] (impi) {\scriptsize{Imputed data set $i$}};
	\coordinate[right=0cm of ds] (dsc);
	\coordinate[left=0cm of imp1] (imp1lc);	
	\coordinate[left=0cm of imp2] (imp2lc);	
	\coordinate[left=0cm of impi] (impilc);	
	\path [line] (dsc) -- (imp1lc);
	\path [line] (dsc) -- (imp2lc);
	\path (imp2) --node[auto=false]{\Large{\vdots}} (impi);
	\path [line] (dsc) -- (impilc);
    \node [block2r, right=1cm of imp1] (results1) {\scriptsize{Results 1}};
    \node [block2r, right=1cm of imp2] (results2) {\scriptsize{Results 2}};
    \node [block2r, right=1cm of impi] (resultsi) {\scriptsize{Results $i$}};
	\coordinate[right=0cm of imp1] (imp1rc);
	\coordinate[right=0cm of imp2] (imp2rc);
	\coordinate[right=0cm of impi] (impirc);
	\coordinate[left=0cm of results1] (results1lc);	
	\coordinate[left=0cm of results2] (results2lc);	
	\coordinate[left=0cm of resultsi] (resultsilc);	
	\path [line] (imp1rc) -- (results1lc);
	\path [line] (imp2rc) -- (results2lc);
	\path (results2) --node[auto=false]{\Large{\vdots}} (resultsi);
	\path [line] (impirc) -- (resultsilc);
    \node [block3, below right=1.2cm and 1.8cm of results1] (results) {\scriptsize{Combined result}};
	\coordinate[left=0cm of results] (resultsc);
	\coordinate[right=0cm of results1] (results1rc);	
	\coordinate[right=0cm of results2] (results2rc);	
	\coordinate[right=0cm of resultsi] (resultsirc);	
	\path [line] (results1rc) -- (resultsc);
	\path [line] (results2rc) -- (resultsc);
	\path [line] (resultsirc) -- (resultsc);
\end{tikzpicture}
\caption{Multiple Imputation Workflow} 
\label{mi-workflow}
\end{figure}

\dsp

Figure \ref{mi-workflow} provides a graphical overview of this workflow. Each missing value is imputed $m$ times from a conditional distribution using other present values for the respective value to create $i$ imputed complete data sets. The chosen statistical analysis $\bm{\tau}$, for instance a regression model, is applied to each of these $i$ data sets, resulting in $\bm{\tau_{i}}$, with $i = 1, ..., m$. Averaging $\bm{\tau_{i}}$ then gives us the single estimate, $\bm{\overline{\tau}}$. Together, this is expressed as: 

\begin{align}
\overline{\tau} = \frac{1}{m}\sum\limits_{i=1}^{m} \tau_{i}.
\end{align}


Following @rubin_1987_multiple, the total variance of $\bm{\overline{\tau}}$, $\bm{Var_T}$ consists of the mean variance of $\bm{\tau_i}$ within each data set $i$, $\bm{\overline{Var_W}}$, and the sample variance of $\bm{\tau}$ across both data sets, $\bm{Var_A}$:

\begin{align}
\overline{Var_W} &= \frac{1}{m} \sum\limits_{i=1}^{m} SE(\tau_i)^2\\
Var_A &= \sum\limits_{i=1}^{m} \frac{(\tau_{i} - \overline{\tau})^2}{m -1}\\
Var_T &= \overline{Var_W} + Var_A.
\end{align}

Multiplied by a factor correcting for small numbers of $m$ (as $m < \infty$), $\bm{Var_T}$ is adjusted to 

\begin{align}
Var_T = \overline{Var_W} + Var_A (1 + \frac{1}{m}).
\end{align}


Each imputed complete data set is identical to all other imputed complete data sets, with the exception of the imputed value. The imputed values for a missing value differ with each imputation of $\bm{M}$ in order to reflect uncertainty levels. The 'multiple' part of the imputation is a crucial aspect here since each imputation run will produce slightly different parameter estimates. Imputing multiple times and then averaging the results creates variability to adjust the standard errors upward, with the latter step taking the same form as an ANOVA calculation [@kroh_2006_taking]. This deliberate random variation included in a deterministic multiple imputation run removes the overconfidence from single imputation, where the standard error estimates are too low [@schafer_2002_missing]. In a case where the utilized multiple imputation model predicts missing values well, variation across the imputed values is small. In other cases, variation may be larger, depending on the level of certainty we have about the missing value. Multiple imputation has been shown to produce consistent, asymptotically efficient and normal estimates for a variety of data MAR [@allison_2002_missing]. It is also advised for data MCAR in order to retain degrees of freedom.

Choosing $m$, the number of imputations, is somewhat subjective. Originally, $m = 5$ was considered sufficient based on efficiency calculations [@rubin_1987_multiple] and is still the default in most software packages. More recent discussions stress the need for an increase of $m$ in order to estimate more nuanced standard errors. Various approaches continue to coexist, such as focusing on the parameter with the largest fraction of missing information [@kroh_2006_taking] or starting with $m = 5$ and gradually increasing it in subsequent runs [@raghunathan_2016_missing]. The most common current practice appears to be to set $m$ to the percentage of missing data, i.e. if 20 percent of data are missing, $m = 20$ [@bodner_2008_what; @white_2011_multiple]. 

There are numerous ways to implement multiple imputation. Up until the late 1990s, this required considerable statistical knowledge and sophisticated methodological skills [see @honaker_2010_what for an overview]. The use of multiple imputation was thus limited to a rather specialized audience of statisticians and methodologists. Since then, numerous `R` packages have emerged to facilitate user-friendliness. The by far most popular packages are `mice` and `Amelia`. Since its inception in 2001, `mice` has been downloaded nearly 3 million times from CRAN at the time of writing. `Amelia` was created in 1998 and has been downloaded over 600,000 times. They are both considered among the very best implementations of multiple imputation [@horton_2007_much]. Any improvement in multiple imputation thus needs to be measured against them. `hot.deck`, the method by @gill_2012_have upon which my proposed method of multiple hot deck imputation with ordinal variables, `hd.ord`, is based, follows this approach and demonstrates improved results when compared to `Amelia`. I extend this with `hd.ord` and also include `mice` as a further benchmark of performance.

The following sections do not cover the full list of functions available in each package, as this would go far beyond the scope of this chapter and fills articles of its own [e.g. @buuren_2007_multiple]. Instead, I will focus on the packages' core underlying mechanisms and their major functions to perform imputation, which are named after their package namesakes: `mice`, `amelia`, `hot.deck`, and `hd.ord`.^[For the remainder of this chapter and to avoid confusion, all names will refer to the functions unless explicitly stated otherwise.] I extend the focus on simplicity and user-friendliness further by running these major imputation functions with their default settings. Survey analysts usually do not possess the statistical expertise that enable them to dive deeply into distribution or chain properties. The vast majority of users can be assumed to use imputation functions with their default settings. If a package only proved superior over others by setting specific and highly technical function arguments, this would defeat the purpose of making multiple imputation the missing data approach for the masses. I apply only one very minor exception to the default settings by setting the number of imputations to the percentage of missingness instead of the default five.





#### `mice`: Multivariate Imputation by Chained Equations {#ordmiss-theory-multimpute-mice}

The `R` package `mice` was released in 2001 [@buuren_2000_multiple]. It stands for Multiple Imputation by Chained Equations (MICE), which means imputing incomplete multivariate data by full conditional specification [@buuren_2007_multiple; @buuren_2011_mice], a version of the imputation-posterior (IP) [@king_2001_analyzing]. Full conditional specification refers to imputation on a variable-by-variable basis, i.e. a set of conditional densities is used to impute data for each individual missing value. This approach does not require the specification of a multivariate distribution for the missing data, which separates it from competing methods like joint modeling [@schafer_1997_analysis]. The initial release of `mice` featured predictor selection, passive imputation, and automatic pooling. Subsequent releases included functionality for imputing multilevel data, post-processing imputed values, specialized pooling, stable imputation of categorical data, and model selection, among many others. Imputation by chained equations is extensively used across domains [see @buuren_2011_mice for a list of over 20 applied fields]. 

Chained equations are based on the Gibbs sampler, a randomized Markov chain Monte Carlo algorithm to estimate a sequence of observations from a specified multivariate probability distribution [@gelman_2013_bayesian; @gill_2014_bayesian]. In essence, chained equations fill in missing values through an iterative repetition of univariate procedures that are chained together -- hence the name for the procedure. As the term univariate signifies, specification happens at the variable level, i.e. each chained equation specifies the imputation model separately for each column of the data. Following deliberations by @rubin_1987_multiple and @buuren_2011_mice, imputation by chained equations takes the missing data generating process into account and maintains data relations as well as the uncertainty about these relations. With these conditions satisfied, the imputation model results in statistically valid and factual imputations. This has been proven empirically under various circumstances, for instance for regression models [@giorgi_2008_performance; @horton_2001_multiple; @horton_2007_much], continuous data [@yu_2007_evaluation], missing predictor variables [@moons_2006_using], large surveys [@schunk_2008_markov], and addressing issues of convergence [@brand_1999_development; @buuren_2006_fully; @drechsler_2008_does].

Continuing the notation from section \ref{ordmiss-theory-mechanisms} and incorporating @buuren_2011_mice, let there be $\bm{X}$, an $n \times v$ matrix with data on $n$ respondents for $v$ variables, that is formed of missing, $\bm{X^{miss}}$, and observed data, $\bm{X^{obs}}$. As before, let there also be a vector of unknown parameters $\bm{\theta}$. Now let $\bm{X}$ further be a random sample from the $z$-variate multivariate distribution, $\bm{Z}(\bm{X} | \bm{\theta})$, with $\bm{\theta}$ accounting for the multivariate distribution of $\bm{X}$. The proverbial pot of gold here is how to estimate the multivariate distribution of $\bm{\theta}$. Under the chained equations model, we estimate a posterior distribution of $\bm{\theta}$ by sampling repeatedly from conditional distributions, i.e.

\begin{align}
Z(X_1 &| X_{-1}, \theta_1) \nonumber\\
&\vdots \nonumber\\
Z(X_z &| X_{-z}, \theta_z).
\end{align}

Any iteration $n$ of chained equations is then a Gibbs sampler that sequentially draws

\begin{align}
\theta_1^{*(n)} &\sim Z(\theta_1 | X_1^{obs}, X_2^{(n-1)}, ..., X_z^{(n-1)}) \nonumber\\
X_1^{*(n)} &\sim Z(X_1 | X_1^{obs}, X_2^{(n-1)}, ..., X_z^{(n-1)}, \theta_1^{*(n)}) \nonumber\\
&\vdots \nonumber\\
\theta_z^{*(n)} &\sim Z(\theta_z | X_z^{obs}, X_1^{(n)}, ..., X_{z-1}^{(n)}) \nonumber\\
X_z^{*(n)} &\sim Z(X_z | X_z^{obs}, X_1^{(n)}, ..., X_z^{(n)}, \theta_z^{*(n)}),
\end{align}

with the chain starting from a random draw from observed marginal distributions and $\bm{X_i^{(n)}} = (\bm{X_i^{obs}}, \bm{X_i^{*(n)}})$ being the $i$th imputed variable at iteration $n$. Note that immediately preceding imputations, $\bm{X_i^{*(n-1)}}$, do not affect $\bm{X_i^{*(n)}}$ directly but only through connections with other variables. 

Figure \ref{mice-func} shows the package's main imputation function, `mice`, with all its arguments. As stated above, I will use `mice` with its default settings to ensure simplicity and user-friendliness.

\vspace{0.5cm}

\begin{figure}[!htbp] 
  \centering
  \includegraphics{figures/mice.png}
  \caption{The \texttt{mice} Function}
  \label{mice-func}
\end{figure}

\vspace{-0.5cm}

The majority of arguments are not of importance to general users. Arguments like `predictorMatrix`, which specifies the set of predictors to be used for each target column, and `blocks`, which provides the option to manually put variables into imputation blocks, require too much statistical knowledge to be of use to non-specialists. Other arguments do not affect the basic workings of the function. This applies for instance to `printFlag`, which sets the console printing preference, `seed`, which is used to offset the random number generator, and `data.init`, which specifies a data frame to be used to initialize imputations before the start of the iterative process. 

The only important arguments for general users are `data`, `m`, and `defaultMethod`. Only `data` requires user input. As mentioned above, `m` should also be set to the percentage of missing data. The argument `defaultMethod` does not require user input but is crucial for insights into the default workings of `mice`. Its options `pmm`, `logreg`, `polyreg`, and `polr` refer to the default imputation methods that are implemented depending on the type of variable in question. The argument `pmm` (predictive mean matching) is used for numerical data, `logreg` (logistic regression imputation) for binary and factor data with two levels, `polyreg` (polytomous regression imputation) for factor data with more than two unordered levels, and `polr` (proportional odds model) for factor data with more than two ordered levels. Note that `mice` thus distinguishes between ordered and unordered as well as the number of factor levels, but does not specifically incorporate ordinal variables, which feature ordered but unevenly spaced levels.


<!--
`data`
A data frame or a matrix containing the incomplete data. Missing values are coded as NA.
`m = 5`
Number of multiple imputations. The default is m=5.
`method = NULL`
Can be either a single string, or a vector of strings with length length(blocks), specifying the imputation method to be used for each column in data. If specified as a single string, the same method will be used for all blocks. The default imputation method (when no argument is specified) depends on the measurement level of the target column, as regulated by the defaultMethod argument. Columns that need not be imputed have the empty method "". See details.
`predictorMatrix`
A numerical matrix of length(blocks) rows and ncol(data) columns, containing 0/1 data specifying the set of predictors to be used for each target column. Each row corresponds to a variable block, i.e., a set of variables to be imputed. A value of 1 means that the column variable is used as a predictor for the target block (in the rows). By default, the predictorMatrix is a square matrix of ncol(data) rows and columns with all 1's, except for the diagonal. Note: For two-level imputation models (which have "2l" in their names) other codes (e.g, 2 or -2) are also allowed.
`where = NULL`
A data frame or matrix with logicals of the same dimensions as data indicating where in the data the imputations should be created. The default, where = is.na(data), specifies that the missing data should be imputed. The where argument may be used to overimpute observed data, or to skip imputations for selected missing values.
`blocks`
List of vectors with variable names per block. List elements may be named to identify blocks. Variables within a block are imputed by a multivariate imputation method (see method argument). By default each variable is placed into its own block, which is effectively fully conditional specification (FCS) by univariate models (variable-by-variable imputation). Only variables whose names appear in blocks are imputed. The relevant columns in the where matrix are set to FALSE of variables that are not block members. A variable may appear in multiple blocks. In that case, it is effectively re-imputed each time that it is visited.
`visitSequence = NULL`
A vector of block names of arbitrary length, specifying the sequence of blocks that are imputed during one iteration of the Gibbs sampler. A block is a collection of variables. All variables that are members of the same block are imputed when the block is visited. A variable that is a member of multiple blocks is re-imputed within the same iteration. The default visitSequence = "roman" visits the blocks (left to right) in the order in which they appear in blocks. One may also use one of the following keywords: "arabic" (right to left), "monotone" (ordered low to high proportion of missing data) and "revmonotone" (reverse of monotone).
`formulas`
A named list of formula's, or expressions that can be converted into formula's by as.formula. List elements correspond to blocks. The block to which the list element applies is identified by its name, so list names must correspond to block names. The formulas argument is an alternative to the predictorMatrix argument that allows for more flexibility in specifying imputation models, e.g., for specifying interaction terms.
`blots = NULL`
A named list of alist's that can be used to pass down arguments to lower level imputation function. The entries of element blots[[blockname]] are passed down to the function called for block blockname.
`post = NULL`
A vector of strings with length ncol(data) specifying expressions as strings. Each string is parsed and executed within the sampler() function to post-process imputed values during the iterations. The default is a vector of empty strings, indicating no post-processing.
`defaultMethod = c("pmm", "logreg", "polyreg", "polr")`
A vector of length 4 containing the default imputation methods for 1) numerical data, 2) factor data with 2 levels, 3) factor data with > 2 unordered levels, and 4) factor data with > 2 ordered levels. By default, the method uses pmm, predictive mean matching (numerical data) logreg, logistic regression imputation (binary data, factor with 2 levels) polyreg, polytomous regression imputation for unordered categorical data (factor > 2 levels) polr, proportional odds model for (ordered, > 2 levels).
`maxit = 5`
A scalar giving the number of iterations. The default is 5.
`printFlag = TRUE`
If TRUE, mice will print history on console. Use print=FALSE for silent computation.
`seed = NA`
An integer that is used as argument by the set.seed() for offsetting the random number generator. Default is to leave the random number generator alone.
`data.init = NULL`
A data frame of the same size and type as data, without missing data, used to initialize imputations before the start of the iterative process. The default NULL implies that starting imputation are created by a simple random draw from the data. Note that specification of data.init will start all m Gibbs sampling streams from the same imputation.
-->





#### `Amelia`: A Program for Missing Data {#ordmiss-theory-multimpute-amelia}

The `R` package `Amelia` was originally released in 1998 [@honaker_1998_amelia]. A second version, `Amelia II`, was released in 2010 [@honaker_2012_amelia]. Contrary to `mice`, which is based on IP, both versions of `Amelia` are based on the expectation-maximization (EM) algorithm [@dempster_1977_maximum; @gelman_2013_bayesian; @jackman_2000_estimation; @mclachlan_1997_algorithm; @tanner_1996_tools]. In EM, deterministic calculations of posterior means replace random draws from the entire posterior. This translates into running regressions to estimate the regression coefficient, imputing a missing value with a predicted value, re-estimating the regression coefficient, and repeating the process until convergence [@king_2001_analyzing]. While the iterations and parameters thus represent an entire density in IP, they are single maximum posterior values in EM. This makes EM comparatively much faster in finding the maximum of the likelihood function. On its own, however, EM is unsuitable for multiple imputation as it does not provide the rest of the distribution. 

The `Amelia` package circumvents this issue with expectation-maximization importance sampling (EMi) [@dempster_1977_maximum; @rubin_1987_multiple], which combines EM with the iterative simulation approach of importance sampling. This proved unsuitable for large data sets, however, as it led to high running times and system crashes. The `Amelia II` package addresses this by mixing EM with bootstrapping [@efron_1994_missing; @lahlrl_2003_impact; @rubin_1994_missing; @shao_1996_bootstrap], allowing the imputation of more variables for more observations more quickly.

`Amelia II` is based on the assumption that the complete data ($\bm{X^{obs}}$ and $\bm{X^{miss}}$) are multivariate normal (MVN), $\bm{X} \sim \bm{N_v}(\mu, \sum)$, with mean vector $\mu$ and covariance matrix $\sum$. The MVN model has been proven to work for a variety of variable types [@ezzati-rice_1995_simulation; @graham_1999_performance; @rubin_1986_multiple; @schafer_1997_analysis]. Continuing the notation from section \ref{ordmiss-theory-mechanisms} and incorporating @honaker_2010_what, let there be a vector of unknown parameters $\bm{\theta}$, with $\bm{\theta} = (\mu, \sum)$. Let there further be our missingness matrix $\bm{R}$ and the likelihood of $\bm{X^{obs}}$, $\text{prob}(\bm{X^{obs}}, \bm{R} | \bm{\theta})$. `Amelia II` is explicitly set up for the MAR assumption of missing data, $\text{prob}(\bm{R} = 0 | \bm{X^{obs}}, \bm{\theta})$. Under this assumption, the likelihood can be transformed as

\begin{align}
\text{prob}(X^{obs}, R | \theta) = \text{prob}(R | X^{obs}) \text{prob}(X^{obs} | \theta).
\end{align}

Since the missing mechanism is MAR, we are only interested in the inference on complete data parameters, thus the likelihood becomes

\begin{align}
L(\theta | X^{obs}) \propto \text{prob}(X^{obs} | \theta)
\end{align}

which further translates into

\begin{align}
\text{prob}(X^{obs} | \theta) = \int \text{prob}(X | \theta) x X^{miss}
\end{align}

under the law of iterated expectations. This results in the posterior

\begin{align}
\text{prob}(\theta | X^{obs}) \propto \text{prob}(X^{obs} | \theta) = \int \text{prob}(X | \theta) x X^{miss}.
\end{align}

Taking draws from this posterior is computationally intensive since the contents of $\mu$ and $\sum$ increase exponentially as the number of variables increases -- this is the perennial crux of multiple imputation, particularly for large data sets with many variables. `Amelia II` solves this through a combination of EM and bootstrapping. This process bootstraps the data to simulate estimation uncertainty for each posterior draw, runs the EM algorithm to find the mode of the posterior bootstrapped data, and then imputes by drawing from $\bm{X^{miss}}$ conditional on $\bm{X^{obs}}$ and the respective draws of $\bm{\theta}$. The latter is a linear regression with parameters that can be estimated from $\bm{\theta}$. This bootstrapped EM approach is faster than IP as Markov chains do not need to be assessed for convergence and an improvement over EMi since the variance matrix of $\mu$ and $\sum$ does not need to be calculated, allowing the algorithm to handle larger data sets.

Figure \ref{amelia-func} shows the package's main imputation function, `amelia`, with all its arguments. As stated above, I will use `amelia` with its default settings to ensure simplicity and user-friendliness.

\vspace{0.5cm}

\begin{figure}[!htbp] 
  \centering
  \includegraphics{figures/amelia.png}
  \caption{The \texttt{amelia} Function}
  \label{amelia-func}
\end{figure}

\vspace{-0.5cm}

As with `mice`, the majority of arguments are not of importance to general users. Specifications such as `splinetime`, which allows the control of cubic smoothing splines of time, and `lags`, which indicates columns in the data that should have their lags included in the imputation model, will only be used in very particular situations by a small minority of users. Other arguments likewise are not crucial to the basic workings of the function, such as `p2s`	to control console printing and `parallel` to identify any type of parallel operation to be used. 

The only argument that requires user input is `x`, which needs to be data with missing values that can be in a variety of formats. `m`, identical to `mice`, should be adjusted to reflect the percentage of missingness in the data. Three other arguments are important since they arguably comprise the core of `amelia`'s underlying imputation mechanism: `tolerance`, `autopri`, and `boot.type`. `tolerance`	sets the convergence threshold for the EM algorithm. `autopri`	allows the EM chain to increase the empirical prior if the path strays into an non-positive definite covariance matrix. `boot.type` offers the option to turn off the non-parametric bootstrap that is applied by default. 

General multiple imputation research treats independent ordinal variables as continuous variables. `amelia` supports this and treats ordinal variables as continuous variables as a default. This means missing ordinal variables are imputed on a continuous scale, rather than preserved as the factual levels present in the observed data. However, the `ords` argument allows users to 'disable' continuous ordinal imputation. In this case, ordinal variables are still imputed on a continuous scale, but these imputations are then scaled and used as the probability of success in a binomial distribution. The draw from this binomial distribution is then transformed into one of the ordinal levels present in the observed data by rounding. While `amelia` thus does incorporate ordinal variables to some extent, the rounding process changes the nature of ordinal variables to continuous variables. None of its features address or reflect the spacing between the ordinal variable categories. 


<!--
`x`
either a matrix, data.frame, a object of class "amelia", or an object of class "molist". The first two will call the default S3 method. The third a convenient way to perform more imputations with the same parameters. The fourth will impute based on the settings from moPrep and any additional arguments.
`m`	
the number of imputed data sets to create.
`p2s`	
an integer value taking either 0 for no screen output, 1 for normal screen printing of iteration numbers, and 2 for detailed screen output. See "Details" for specifics on output when p2s=2.
`frontend`	
a logical value used internally for the GUI.
`idvars`	
a vector of column numbers or column names that indicates identification variables. These will be dropped from the analysis but copied into the imputed data sets.
`ts`	
column number or variable name indicating the variable identifying time in time series data.
`cs`	
column number or variable name indicating the cross section variable.
`polytime`	
integer between 0 and 3 indicating what power of polynomial should be included in the imputation model to account for the effects of time. A setting of 0 would indicate constant levels, 1 would indicate linear time effects, 2 would indicate squared effects, and 3 would indicate cubic time effects.
`intercs`	
a logical variable indicating if the time effects of polytime should vary across the cross-section.
`lags`
a vector of numbers or names indicating columns in the data that should have their lags included in the imputation model.
`leads`	
a vector of numbers or names indicating columns in the data that should have their leads (future values) included in the imputation model.
`startvals`	
starting values, 0 for the parameter matrix from list-wise deletion, 1 for an identity matrix.
`tolerance`	
the convergence threshold for the EM algorithm.
`logs`
a vector of column numbers or column names that refer to variables that require log-linear transformation.
`sqrts`	
a vector of numbers or names indicating columns in the data that should be transformed by a sqaure root function. Data in this column cannot be less than zero.
`lgstc`	
a vector of numbers or names indicating columns in the data that should be transformed by a logistic function for proportional data. Data in this column must be between 0 and 1.
`noms`
a vector of numbers or names indicating columns in the data that are nominal variables.
`ords`
a vector of numbers or names indicating columns in the data that should be treated as ordinal variables.
`incheck`	
a logical indicating whether or not the inputs to the function should be checked before running amelia. This should only be set to FALSE if you are extremely confident that your settings are non-problematic and you are trying to save computational time.
`collect`	
a logical value indicating whether or not the garbage collection frequency should be increased during the imputation model. Only set this to TRUE if you are experiencing memory issues as it can significantly slow down the imputation process.
`arglist`	
an object of class "ameliaArgs" from a previous run of Amelia. Including this object will use the arguments from that run.
`empri`	
number indicating level of the empirical (or ridge) prior. This prior shrinks the covariances of the data, but keeps the means and variances the same for problems of high missingness, small N's or large correlations among the variables. Should be kept small, perhaps 0.5 to 1 percent of the rows of the data; a reasonable upper bound is around 10 percent of the rows of the data.
`priors`	
a four or five column matrix containing the priors for either individual missing observations or variable-wide missing values. See "Details" for more information.
`autopri`	
allows the EM chain to increase the empirical prior if the path strays into an non-positive definite covariance matrix, up to a maximum empirical prior of the value of this argument times $n$, the number of observations. Must be between 0 and 1, and at zero this turns off this feature.
`emburn`	
a numerical vector of length 2, where emburn[1] is a the minimum EM chain length and emburn[2] is the maximum EM chain length. These are ignored if they are less than 1.
`bounds`	
a three column matrix to hold logical bounds on the imputations. Each row of the matrix should be of the form c(column.number, lower.bound,upper.bound) See Details below.
`max.resample`	
an integer that specifies how many times Amelia should redraw the imputed values when trying to meet the logical constraints of bounds. After this value, imputed values are set to the bounds.
`overimp`	
a two-column matrix describing which cells are to be overimputed. Each row of the matrix should be a c(row, column) pair. Each of these cells will be treated as missing and replaced with draws from the imputation model.
`boot.type`	
choice of bootstrap, currently restricted to either "ordinary" for the usual non-parametric bootstrap and "none" for no bootstrap.
`parallel`	
the type of parallel operation to be used (if any). If missing, the default is taken from the option "amelia.parallel" (and if that is not set, "no").
`ncpus`	
integer: the number of processes to be used in parallel operation: typically one would choose the number of available CPUs.
`cl`
an optional parallel or snow cluster for use if parallel = "snow". If not supplied, a cluster on the local machine is created for the duration of the amelia call.
-->







#### `hot.deck`: Multiple Hot Deck Imputation {#ordmiss-theory-multimpute-hdnorm}

`hot.deck` is an `R` package released in 2012 [@gill_2012_have]. It combines a variation of non-parametric hot decking (see section \ref{ordmiss-theory-singimpute}) with multiple imputation and aims to fill gaps where parametric multiple imputation, i.e. the approach used in `mice` and `amelia`, falls short [@fuller_2005_deck; @kim_2004_finite; @kim_2004_fractional; @reilly_1993_data]. Like hot decking, `hot.deck` uses draws of actual observable values (\textit{donors}) to fill missing values (\textit{recipients}). In order to account for uncertainty around the drawn values, `hot.deck` iterates these draws over $m$ imputations and pools the results. 

The main proposed advantage of `hot.deck` lies in its applicability to missing data with discrete variables with a small number of categories. Approaches like the one used in `amelia`, for instance, by default impute discrete data on a continuous scale. This changes the nature of discrete variables and practically turns them into continuous variables. This can result in non-observable, biased, and sometimes even non-sensical imputation values with artificially smaller standard errors. The proposed `amelia` solution of rounding continuous imputations is problematic as well: Let imputation 1 of a binary variable between 0 and 1 be 0.4. Let further imputation 2 of the binary variable be 0.6. With rounding, these imputations become 0 and 1, when they are in fact 0.4 and 0.6. Rounding thus by definition introduces at least some level of bias. The problem is exacerbated for ordinal variables, where the spacing between the discrete variable categories is unknown, since it arbitrarily reduces or lengthens distances between the categories. This is not the case in `hot.deck` as it preserves the integrity of discrete data, does not change the size of standard errors, and produces more accurate imputations. `hot.deck` also does not require assumptions of a MVN distribution that are required by `amelia`.

Following @gill_2012_have, `hot.deck` estimates affinity scores, $\bm{\alpha}$, for each missing value to measure how similar a respondent with a missing value, the recipient $c$, is to another respondent, the potential donor $o$, across all variables except the missing one. Each score is bounded by 0 and 1. The total set of affinity scores is denoted by $\bm{\alpha_{co}}$. For each respondent, let there be vector $(p, v)$, with $p$ being the dependent variable and $v$ a vector of discrete explanatory variables of length $k$. If recipient $c$ has $q_c$ missing values in $v_c$, then the potential donor vector, $v_o$, has between 0 and $k-q_c$ exact matches with $c$. Let $w_{co}$ be the number of variables where $c$ and $o$ have non-identical values. This leaves $k-q_c -w_{co}$ as the number of variables where they have identical values. Scaled by the highest number of possible matches $(k-q_c)$, this value forms the affinity score

\begin{align}
\alpha_{co} = \frac{k-q_c-w_{co}}{k-q_c}
\end{align}

for each missing value recipient $c$. When the number of identical matches decreases, so does $\bm{\alpha_{co}}$. While this might work well for binary variables, it poses a problem for discrete variables with many levels, as the probability to find identical matches decreases. To account for this, `hot.deck` treats potential donors $o$ for the $h$th variable in $v_{o[h]}$ that are 'close' differently than potential donors $o$ that are further away. 'Close' is defined as $v_{o[h]}$ and $v_{c[h]}$ being in the same concentric standard deviation from $\overline{h}$, the mean of variable $h$. Values outside of this range are penalized while values within this range are counted as matches. All donors with the highest affinity scores, i.e. all matches, form the best imputation cell $\bm{B}$. Since all values of $v_{c[h]}$ in $\bm{B}$ are part of the same distribution of independent and identically distributed (iid) random variables, which satisfies the MCAR requirement, we can use random draws from $\bm{B}$ to impute the missing value. As with the other multiple imputation approaches, this process is then repeated $m$ times for each missing value to account for imputation uncertainty, following the logic displayed in Figure \ref{mi-workflow}.

Figure \ref{hot.deck-func} shows the package's main imputation function, `hot.deck`, with all its arguments. As before, I will use `hot.deck` with its default settings. 

\vspace{0.5cm}

\begin{figure}[!htbp] 
  \centering
  \includegraphics{figures/hot.deck.png}
  \caption{The \texttt{hot.deck} Function}
  \label{hot.deck-func}
\end{figure}

\vspace{-0.5cm}

Like `mice` and `amelia`, `hot.deck` only requires user input for `data`. `m` should once more be set to the percentage of missingness. Specialized arguments such as `optimStep` and `optimStop`, which can be tweaked to optimize standard deviation cutoff parameters, as well as `weightedAffinity`, which indicates whether a correlation-weighted affinity score should be used, do not apply to general users.

`method` and `cutoff` form the core of `hot.deck`. The default setting of `best.cell` in the `method` argument implements multiple hot deck imputation. The alternative, `p.draw`, on the other hand, merely conducts random probabilistic draws. `cutoff` allows users to specify which variables the algorithm should treat as discrete. By default, any variable up to and including 10 unique values is considered discrete. This thus includes the majority of political science survey measures, with the sensible exceptions of variables like age or for instance widely spread assessments of income levels.

Overall, `hot.deck` is a specialized function to improve the application of multiple imputation for discrete data and has been shown to do so for highly granular discrete data [@gill_2012_have]. Moreover, political science survey research relies on highly discrete measures. What is missing from `hot.deck`, however, is the incorporation of ordinal variables as a special form of discrete data. I thus identify this gap as a leverage point to improve the use of ordinal variables in the imputation of missing data. To do so, I adapt `hot.deck` to form `hd.ord`, a function specifically designed to utilize the ordered but unevenly spaced information contained in ordinal variables.






<!--
`data`
A data frame or matrix with missing values to be imputed using multiple hot deck imputation.
`m`	
Number of imputed data sets required.
`method`	
Method used to draw donors based on affinity either best.cell (the default) or p.draw for probabilistic draw
`cutoff`	
A numerical scalar such that any variable with fewer than cutoff unique non-missing values will be considered discrete and necessarily imputed with hot deck imputation.
`sdCutoff`	
Number of standard deviations between observations such that observations fewer than sdCutoff standard deviations away from each other are considered sufficiently close to be a match, otherwise they are considered too far away to be a match.
`optimizeSD`	
Logical indicating whether the sdCutoff parameter should be optimized such that the smallest possible value is chosen that produces no thin cells from which to draw donors. Thin cells are those where the number of donors is less than m.
`optimStep`	
The size of the steps in the optimization if optimizeSD is TRUE.
`optimStop`	
The value at which optimization should stop if it has not already found a value that produces no thin cells. If this value is reached and thin cells still exist, a warning will be returned, though the routine will continue using optimStop as sdCutoff.
`weightedAffinity`	
Logical indicating whether a correlation-weighted affinity score should be used.
`impContinuous`
Character string indicating how continuous missing data should be imputed. Valid options are HD (the default) in which case hot-deck imputation will be used, or mice in which case multiple imputation by chained equations will be used.
`IDvars`
A character vector of variable names not to be used in the imputation, but to be included in the final imputed data sets.
-->





#### `hd.ord`: Multiple Hot Deck Imputation with Ordinal Variables {#ordmiss-theory-multimpute-hdord}

`hd.ord` is a self-penned `R` function designed specifically to implement multiple hot deck imputation with ordinal variables. It is an extension of `hot.deck` and fully utilizes the unevenly spaced yet ordered information contained in ordinal variables. As described in section \ref{ordblock-theory-op}, ordinal variables matter in political science surveys because a key variable in such surveys is ordinal: education. The importance of the spacing between education values is best demonstrated with a simplified example shown in Table \ref{ordmiss-ordspace}.


\begin{table}[!htbp] 
  \centering
  \caption{Illustrative Data}
  \label{ordmiss-ordspace}
  \begin{tabular}{lccccc}
  \bottomrule 
  \midrule
  Respondent & Age & Party ID & Education & Income & Gender\\
  \hline
  A & 25 & Republican & High School Graduate & \$30-40,000 & Male \\
  B & 40 & NA & Some High School &  \$20-30,000 & Female\\
  C & 30 & Democrat & Bachelor's Degree &  \$50-60,000 & Female\\
  \bottomrule 
  \end{tabular}
\end{table}

Respondent B shows missing data for party ID. To impute a fill-in value, we look at how close respondents A and C are to B in terms of age, education, income, and gender. C is closer to B in terms of age and they share the same gender. A is closer to B on education and income. `hot.deck` measures these distances and estimates affinity scores for respondents A and C. The affinity scores measure how close A and C are to B on all variables except the missing one, i.e. party ID. B then receives the party ID fill-in value from whichever respondent has the higher score. The algorithm building the affinity score is based on evenly spaced sequential numerical values, e.g. 1, 2, 3 etc. to represent the distances between the variable categories. This makes sense for age, income, and gender, but not for education, since education is an ordinal variable. Applying `hot.deck` to such a numerical representation might thus misrepresent the data.

To avoid this, `hd.ord` uses the ordered probit approach described in section \ref{ordblock-theory-op}. It applies `polr` from the `MASS` package to any specified number of ordinal variables in the data to estimate the underlying latent continuous variable. This estimates cutoff thresholds between the ordinal categories and bins data cases according to the linear predictors. The binned cases determine which variable categories make sense, given the underlying latent continuous variable. This can result in a reduction of education categories if the categories are too finely thinned out. `hd.ord` estimates the mid-cutpoints between each of these newly estimated categories based on the `polr` results. We then replace the ordinal variable categories with the newly estimated numerical mid-cutpoints in the data. Finally, these values are scaled and used to assess distances to calculate affinity scores.


```{r Illustrative Data Code, include=FALSE}

load("functions/OPMord.Rdata")
load("functions/OPMcut.Rdata")
ill.data <- readRDS("data/anes/anes_1000.rds")

ill.data <- ill.data[,!names(ill.data) %in% c("Liberal", "Conservative", "Single")]
dv <- "Educ"
dplyr::rename(ill.data, 
              Democrat = Dem,
              Income = Inc)

all.evs <- colnames(ill.data)[-which(colnames(ill.data) == dv)]                         
add.nas.columns <- c("Democrat", "Male", "Interest", "Income", "Age")                        
no.nas <- ill.data[,!names(ill.data) %in% add.nas.columns]                                  
yes.nas <- ill.data[,names(ill.data) %in% add.nas.columns]
prop <- .2

set.seed(123)
data.amp <- cbind(no.nas, ampute(yes.nas, prop = prop, mech = "MAR")$amp)
OPMord.full <- OPMord(data.amp, dv = dv, evs = all.evs)
OPMcut.dat <- OPMcut(data = OPMord.full$data.full.nas, dv = dv, OPMordOut = OPMord.full)

OPMord.full$int.df$Intercepts <- OPMord.full$int.df$Intercepts %>% as.character 
OPMord.full$int.df$Intercepts <- c("Less Than High School|Some High School", "Some High School|High School Graduate", "High School Graduate|Some College", "Some College|Bachelor's Degree", "Bachelor's Degree|Master's Degree")
OPMord.full$int.df$Values <- round(OPMord.full$int.df$Values, digits = 3)
ill.data.int.df <- OPMord.full$int.df[,1:2]
ill.data.int.df.tab <- ill.data.int.df
colnames(ill.data.int.df.tab) <- c("Thresholds", "Coefficients")

ill.data.cutp <- data.frame(cbind(c("Less Than High School", "Some High School", "High School Graduate", "Some College", "Bachelor's Degree", "Master's Degree"), OPMcut.dat$Educ %>% unique %>% round(., digits = 3) %>% sort))
ill.data.cutp$X2 <- ill.data.cutp$X2 %>% as.character %>% as.numeric
# ill.data.cutp <- ill.data.cutp %>% round(digits = 3)
colnames(ill.data.cutp) <- c("origed", "midc")
ill.data.cutp.tab <- ill.data.cutp
colnames(ill.data.cutp.tab) <- c("Original Education Categories", "Mid-Cutpoints")

```


Table \ref{ordmiss-ill-res} illustrates this procedure with results from running `polr` on rudimentary survey data, with column "Thresholds" showing the estimated cutoff thresholds between the education categories. Table \ref{ordmiss-ill-mid} in turn shows the estimated mid-cutpoints for each of the education categories. The mid-cutpoint values for the categories in Table \ref{ordmiss-ill-mid} fall between the adjacent values in Table \ref{ordmiss-ill-res}, i.e. the mid-cutpoint of `r ill.data.cutp$midc[ill.data.cutp$origed == "Some High School"]` for "Some High School" lies between the respective thresholds of `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"]` and `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"]`. To estimate the beginning cutpoint for the first category ("Less Than High School"), we halve the difference between the first and second threshold and subtract this value from the first threshold: `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"]` $-$ (`r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"]` $-$ `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric`) / 2 =  `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric - ((ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"] %>% as.numeric - ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric) / 2) %>% round(digits = 3)`. The same process is applied to estimate the ending cutpoint for the last category ("Master's Degree"). The mid-cutpoint values are then scaled and used for the calculation of the affinity scores.


```{r Illustrative Data Table 1, include=FALSE}

stargazer(ill.data.int.df.tab, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Illustrative Data `polr` Results",
          label = "ordmiss-ill-res")
```


\begin{table}[!htbp] \centering 
  \caption{Illustrative Data `polr` Results} 
  \label{ordmiss-ill-res} 
\begin{tabular}{r@{}lr@{}l} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{2}{c}{Thresholds} & \multicolumn{2}{c}{Coefficients} \\ 
\hline \\[-1.8ex] 
Less Than High School $\mid$ & \,Some High School & 2.&418 \\ 
Some High School $\mid$ & \,High School Graduate & 3.&495 \\ 
High School Graduate $\mid$ & \,Some College & 4.&214 \\ 
Some College $\mid$ & \,Bachelor's Degree & 5.&727 \\ 
Bachelor's Degree $\mid$ & \,Master's Degree & 7.&412 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 


```{r Illustrative Data Table 2, results='asis', echo=FALSE}

stargazer(ill.data.cutp.tab, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Illustrative Data Value Replacements",
          label = "ordmiss-ill-mid")

```




Figure \ref{hd.ord-func} shows `hd.ord` with all its arguments. As before, I will use `hd.ord` with its default settings. Since `hd.ord` is an adaptation of `hot.deck`, the two functions are identical except for the `ord` argument, which allows users to specify the ordinal variables for `polr` treatment.

\vspace{0.5cm}

\begin{figure}[!htbp]
  \centering
  \includegraphics{figures/hd.ord.png}
  \caption{The \texttt{hd.ord} Function}
  \label{hd.ord-func}
\end{figure}

\vspace{-0.5cm}




<!--
`data`
A data frame or matrix with missing values to be imputed using multiple hot deck imputation.
`ord`
An ordinal variable to be treated with polr()
`m`	
Number of imputed data sets required.
`method`	
Method used to draw donors based on affinity either best.cell (the default) or p.draw for probabilistic draw
`cutoff`	
A numerical scalar such that any variable with fewer than cutoff unique non-missing values will be considered discrete and necessarily imputed with hot deck imputation.
`sdCutoff`	
Number of standard deviations between observations such that observations fewer than sdCutoff standard deviations away from each other are considered sufficiently close to be a match, otherwise they are considered too far away to be a match.
`optimizeSD`	
Logical indicating whether the sdCutoff parameter should be optimized such that the smallest possible value is chosen that produces no thin cells from which to draw donors. Thin cells are those where the number of donors is less than m.
`optimStep`	
The size of the steps in the optimization if optimizeSD is TRUE.
`optimStop`	
The value at which optimization should stop if it has not already found a value that produces no thin cells. If this value is reached and thin cells still exist, a warning will be returned, though the routine will continue using optimStop as sdCutoff.
`weightedAffinity`	
Logical indicating whether a correlation-weighted affinity score should be used.
`impContinuous`
Character string indicating how continuous missing data should be imputed. Valid options are HD (the default) in which case hot-deck imputation will be used, or mice in which case multiple imputation by chained equations will be used.
`IDvars`
A character vector of variable names not to be used in the imputation, but to be included in the final imputed data sets.
-->






## Data {#ordmiss-data}

To test the performance of imputation methods, we need to work with complete data, as only complete data allow us to obtain the true values needed as a benchmark for comparison. I choose two different sets of survey data from the 2016 ANES and the 2016 CCES. Data for all selected variables in both data sets is complete. In order to test the accuracy of several imputation methods, I delete data from these complete data sets with the `ampute` function from the `mice` package [@buuren_2020_package]. `ampute` allows the removal of data MCAR, MAR, and MNAR. Particularly the availability of the latter offers unique opportunities, as it can be a difficult feat to establish whether real-life missing data is MNAR. Data that are artificially created to be MNAR, however, circumvent this problem and allow us to test the accuracy of imputation methods for this type of missing data as well. `ampute` has been shown to accurately remove data MCAR, MAR, and MNAR [@schouten_2018_generating].

Each data set is imputed with four different `R` functions: `hd.ord`, `hot.deck`, `amelia`, and `mice`. I also apply list-wise deletion with `na.omit`. As outlined in section \ref{ordmiss-theory-multimpute}, all functions are used with their default settings but with the number of imputations set to the percentage of missingness.

I test each function for accuracy and speed for binary, ordinal, and interval variables in both data sets. Each data set contains two ordinal (`Education`, `Interest`), two interval (`Age`, `Income`) and numerous binary variables. In order to enable factually accurate comparison and unless explicitly specified otherwise, each data set contains 1,000 observations and six levels of the ordinal variable `Education`. 1,000 observations represent a common size for survey and survey experiment data, and the `polr` analysis from section \ref{ordblock-data} estimates five or six levels to best represent `Education` in a US context. Each data set was imputed 1,000 times with each of the four imputation methods. With the exception of Figure \ref{accuracy50} and Table \ref{run.cces.perc}, 20 percent missing data were randomly amputed in each iteration for each data set.^[The decision to reduce the number of education categories in the ANES data is made out of necessity. It is not feasible to use all ANES observations, as repeated multiple imputation is computationally intensive for a sample of this size. The reduction to 1,000 observations in turn makes it impossible to use all original categories, since the insertion of missing data would consistently lead to dropped categories, which in turn would render a comparison of imputation runs pointless. See appendix section \ref{app-ordmiss-allObs} for imputations for all ANES and CCES observations (as much as computationally possible).] 

Following @collins_2001_comparison and @honaker_2010_what, as many relevant observed variables as possible were used to impute each of the data sets. For the ANES data, up to 14 predictor variables were used: `Independent`, `Moderate`, `Black`, `Hispanic`, `Asian`, `Employed`, `Student`, `Religious`, `InternetHome`, `OwnHome`, `Rally` (have you attended a political rally), `Donate` (have you donated to a political candidate), `Married`, and `Separated`. For the CCES data, up to 17 predictor variables were used: `Republican`, `Moderate`, `Liberal`, `Black`, `Hispanic`, `Asian`, `Employed`, `Unemployed`, `Student`, `Gay`, `Bisexual`, `StudLoans` (do you have student loans), `InternetHome`, `NotReligious`, `RentHome`, `Separated`, and `Single`. Highly collinear variables were excluded with a cutoff of 0.6.
<!-- For the Framing data, up to 13 predictor variables were used: `Ind`, `Conservative`, `Liberal`, `Black`, `Hisp`, `White`, `Asian`, `Unempl`, `Ret` (Retired), `Stud`, `Official` (have you written to a political official), `Media` (how much do you follow public affairs in the media), and `Participation` (how many political activities have you participated in).  -->

The variable mean serves as the baseline of comparison for the performance of each imputation method. Since each data set is complete, we know the true variable mean of all variables. The closer a method comes to the true mean, the better its performance. I impute both data sets for data MAR and MNAR for five amputed variables: `Democrat` (binary), `Male` (binary), `Interest` (ordinal, scaled from 1 to 4), `Income` (interval), and `Age` (interval). Imputation is not necessary for data MCAR because simple deletion leads to unbiased and therefore valid results. I subsequently increase the number of ordinal variables to be treated by `polr` in `hd.ord` by including `Interest`. Imputations are again conducted MAR and MNAR, this time for four amputed variables. The amputed variables are the same as before but omitting `Interest`. Next, I test the effect of increasing the amount of missing data to 50 percent. Finally, I compare the imputation runtimes for each method.








## Results {#ordmiss-results}

<!--
I didn't find any scenario where hd.ord actually did significantly and consistently better than amelia or mice. It comes close on occasion for some binary variables and MNAR, but not often and consistently enough. I told Jeff this, and he asked me to write the chapter along my findings -- that the assumption behind polr didn't pan out, at least not for missing data. Everything below is built around that argument.

If it should come up, maybe because a committee member is unhappy, who knows, these are half-hearted ideas I currently have that I might possibly still investigate if I had to:
-- Something with correlation. When I made the coding error that resulted in sampling from 85 observations, amelia was awful and hd.ord much better. Jeff also uses very high correlation in his hot.deck paper (I believe it was 0.8). Jeff also used data on modernization theory for 135 countries between 1950 and 1990, though. That data is very different from surveys and doesn't suffer from any external validity problems. I'm not sure this can be applied to surveys. I frame my stuff around surveys, and what good are artificial data with super high correlations that don't occur in the 'wild' in actual surveys?
-- My second idea is related to the first, I suspect: If hd.ord was close or good in my analyses so far, it was for framing. Why? I would guess that it has to do with correlations and external validity. My sample is markedly different from ANES or CCES when you look at the true variable means. The correlations are probably also very different. That would mean that hd.ord works well for non-externally valid samples -- and what good would that be, really?
-->



<!-- ---- MAIN TEXT ---- -->

<!-- MAR  -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- 12 variables: Dem, Male, Interest, Inc, Age, Black, Empl + 5 selected -->

<!-- MNAR -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- 12 variables: Dem, Male, Interest, Inc, Age, Black, Empl + 5 selected -->

<!-- INCREASED ORDINAL VARIABLES (DVs: educ, interest): -->
<!-- MAR -->
<!-- 4 variables: Dem, Male, Inc, Age -->
<!-- 11 variables: Dem, Male, Inc, Age, Black, Empl + 5 selected -->

<!-- MNAR -->
<!-- 4 variables: Dem, Male, Inc, Age -->
<!-- 11 variables: Dem, Male, Inc, Age, Black, Empl + 5 selected -->

<!-- INCREASED MISSINGNESS (MAR, 20, 50 percent): -->
<!-- Framing, 1000 iterations, 5 variables: Dem, Male, Interest, Inc, Age -->

<!-- SPEED: -->
<!-- MAR 5 variables, MAR 12 variables -->
<!-- MAR 20, 50 percent CCES -->



<!-- ---- APPENDIX ---- -->

<!-- ALL OBSERVATIONS ( section \ref{app-ordmiss-allObs}): -->
<!-- ANES all obs. 1000 iterations, CCES all obs. 10 iterations (maxed out RAM) -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR (Table \ref{mar.5var.all}) and MNAR (\ref{mnar.5var.all}) -->


<!-- INCREASED MISSINGNESS ( section \ref{app-ordmiss-increaseNA}): -->
<!-- CCES 10,000 iterations -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->
<!-- 20, 50 percent -->


<!-- SPEED ( section \ref{app-ordmiss-speed}): -->
<!-- CCES 10,000 iterations -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->
<!-- 20, 50 percent -->

<!-- ANES all obs. 1000 iterations, CCES all obs. 10 iterations (maxed out RAM) -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->


<!--
Problem with CCES 1,000 n 10,000 its MAR 5 Var 20, 50 percent (which I had planned for the appendix):
The 20 percent worked fine on CO, but 50 percent failed because RAM was maxed out

Problem with CCES 5 Var all obs. (MAR + MNAR) (which I had planned for the appendix):
No more than 10 iterations are possible before RAM is maxed out, on Jeff and CO
-->




### MAR {#ordmiss-results-mar}

```{r MAR 5 Variables, include=FALSE}

mar.5var.anes <- read.csv("data/anes/mar/results/anes.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% addPlus
mar.5var.cces <- read.csv("data/cces/mar/results/cces.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% addPlus
# mar.5var.frame <- read.csv("data/framing/mar/results/framing.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% addPlus

mar.5var.anes[1:6, 2] <- mar.5var.cces[1:6, 2] <- rep("Democrat", 6)
mar.5var.anes[19:24, 2] <- mar.5var.cces[19:24, 2] <- rep("Income", 6)

mar.5var.anes$diff[mar.5var.anes$method == "true"] <- mar.5var.anes$value[mar.5var.anes$method == "true"]
mar.5var.cces$diff[mar.5var.cces$method == "true"] <- mar.5var.cces$value[mar.5var.cces$method == "true"]
# mar.5var.frame$diff[mar.5var.frame$method == "true"] <- mar.5var.frame$value[mar.5var.frame$method == "true"]

levels(mar.5var.anes$method) <- levels(mar.5var.cces$method) <- levs
# levels(mar.5var.frame$method) <- levs
  
mar.5var <- cbind(mar.5var.anes[, c(1,2,4)], mar.5var.cces[,4])
# mar.5var <- cbind(mar.5var.anes[, c(1,2,4)], mar.5var.cces[,4], mar.5var.frame[,4])
colnames(mar.5var) <- col.names

# to make the in-text citations shorter
mar.5.anes <- mar.5var$ANES
mar.5.cces <- mar.5var$CCES
# mar.5.frame <- mar.5var$Framing
mar.5.meth <- mar.5var$Method
mar.5.var <- mar.5var$Variable

tab.mar.5var <- stargazer(mar.5var, 
                          summary = FALSE,
                          align = TRUE,
                          header = FALSE,
                          rownames = FALSE,
                          digits = 4,
                          title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, MAR, 5 Variables with NA",
                          label = "mar.5var")

dt <- gsub("\\multicolumn{1}{c}{", "", tab.mar.5var, fixed = TRUE)
cat(dt)



```

<!--
MAR 5 Var
Explain vars: Dem and Male binary, Interest ordinal (scale 1-4), Inc and Age interval
	Binary
		hd.ord on par or worse than hot.deck for all three ds for both vars
		mice best overall across all ds and vars
		mice and amelia very close (.0001 difference)
		More difference for ANES (.0003 mice vs .0011 hd.ord Dem, .0001 mice vs. .0013 hd.ord Male) and Male CCES (.0001 amelia vs. .0014 hd.ord)
		Fewer difference for Dem CCES (.0001 amelia vs. .0004 hd.ord)
		hot.deck performs on par with mice and amelia for framing for both vars
		hd.ord further off for framing as well (.0008 vs. .0000 mice Dem; .0005 vs. .0000 amelia Male)
	Ordinal
		hd.ord worst for all three ds, with considerable distance to hot.deck (.0191 vs. .0130 ANES; .0196 vs. .0125 CCES; .0248 vs. .0213 framing)
		mice and amelia by far best across all ds
		Much larger performance difference between methods for ordinal than for binary: mice is not more than .0003 (Dem ANES) away from true value for all ds. hd.ord's max difference is .0248 (framing)
		Equal performance of mice and amelia, with the edge to mice because of CCES (.0000 vs. 0.0003)
	interval
		hd.ord worst for all three ds for both vars
		Again considerable distance to hot.deck
		mice best for Inc
		amelia best for Age
		Even larger performance difference between methods: mice is no more than .0014 (framing) away from true value across ds for Inc. hd.ord's max difference is .1278 (ANES)
		Same for amelia: amelia's max difference is .0039 (framing). hd.ord's is .4597 (ANES)
-->

This section shows the imputation results for the MAR missing data mechanism. MAR amputation was achieved by setting the `mech` argument in the `ampute` function to `MAR`. Table \ref{mar.5var} shows the results of imputing both data sets MAR for five amputed variables. For the two binary variables, `Democrat` and `Male`, `hd.ord` performs on par or worse than `hot.deck` for both data sets, while `mice` and `amelia` perform best. `hd.ord` is relatively close for CCES `Democrat` (`r mar.5.cces[mar.5.meth == "amelia" & mar.5.var == "Democrat"]` `amelia` vs. `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Democrat"]` `hd.ord`) but further away for ANES ( `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Democrat"]` `mice` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Democrat"]` `hd.ord` `Democrat`; `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Male"]` `mice` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` `hd.ord` `Male`) and CCES `Male` (`r mar.5.cces[mar.5.meth == "amelia" & mar.5.var == "Male"]` `amelia` vs. `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` `hd.ord`).



 \begin{table}[!htbp] \centering   
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, MAR, 5 Variables with NA}   
 \label{mar.5var}  
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l}  
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex]  
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex]  
 true & Democrat & 0.&3420 & 0.&3770 \\
 hd.ord & Democrat & --0.&0010 & +0.&0000 \\ 
 hot.deck & Democrat & --0.&0011 & --0.&0004 \\ 
 amelia & Democrat & +0.&0004 & +0.&0001 \\
 mice & Democrat & +0.&0003 & +0.&0002 \\
 na.omit & Democrat & --0.&0290 & --0.&0229 \\
 true & Male & 0.&4890 & 0.&4830 \\
 hd.ord & Male & --0.&0013 & --0.&0011 \\
 hot.deck & Male & --0.&0013 & --0.&0014 \\
 amelia & Male & +0.&0002 & --0.&0001 \\
 mice & Male & +0.&0001 & --0.&0001 \\
 na.omit & Male & --0.&0392 & --0.&0414 \\
 true & Interest & 2.&9340 & 3.&3290 \\
 hd.ord & Interest & --0.&0130 & --0.&0125 \\
 hot.deck & Interest & --0.&0191 & --0.&0196 \\ 
 amelia & Interest & +0.&0003 & +0.&0003 \\ 
 mice & Interest & +0.&0003 & +0.&0000 \\
 na.omit & Interest & --0.&0705 & --0.&0724 \\
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&1068 & --0.&0259 \\
 hot.deck & Income & --0.&1278 & --0.&0407 \\
 amelia & Income & +0.&0008 & --0.&0004 \\
 mice & Income & +0.&0003 & --0.&0002 \\
 na.omit & Income & --0.&5631 & --0.&2468 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&3888 & --0.&2616 \\
 hot.deck & Age & --0.&4597 & --0.&3895 \\
 amelia & Age & +0.&0007 & --0.&0033 \\
 mice & Age & +0.&0017 & --0.&0073 \\
 na.omit & Age & --1.&1875 & --1.&2361 \\
 \hline \\[-1.8ex]  
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 


For the ordinal variable, `Interest`, `hd.ord` performs worst for both data sets, with considerable distance to `hot.deck` (`r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` vs. `r mar.5.anes[mar.5.meth == "hot.deck" & mar.5.var == "Interest"]` ANES; `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` vs. `r mar.5.cces[mar.5.meth == "hot.deck" & mar.5.var == "Interest"]` CCES). `mice` and `amelia` perform by far best across both data sets. The performance differences between the methods are far larger for the ordinal than for the binary variables: `mice` is not more than `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Interest"]` (ANES) away from the true value across both data sets, while the maximum difference for `hd.ord` amounts to `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` (ANES).

For the interval variables, `Income` and `Age`, `hd.ord` also performs worst for both data sets. The distance to `hot.deck` is once more considerable. `mice` performs best for `Income` and `amelia` shows the best results for `Age`. The performance differences between the methods are even larger here: For `Income`, `mice` is not more than `r mar.5.cces[mar.5.meth == "mice" & mar.5.var == "Income"]` (CCES) away from the true value across both data sets, but the maximum difference for `hd.ord` is `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Income"]` (ANES). Similarly, `amelia`'s largest deviation from the true value for `Age` is `r mar.5.cces[mar.5.meth == "amelia" & mar.5.var == "Age"]` (CCES) as opposed to `hd.ord`'s `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Age"]` (ANES).^[For a repeat of this MAR analysis for 12 amputed variables, see appendix section \ref{app-ordmiss-12var}. The results do not change substantively.]








### MNAR {#ordmiss-results-mnar}

```{r MNAR 5 Variables, include=FALSE}

mnar.5var.anes <- read.csv("data/anes/mnar/results/anes.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mnar.5var.cces <- read.csv("data/cces/mnar/results/cces.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mnar.5var.frame <- read.csv("data/framing/mnar/results/framing.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mnar.5var.anes[1:6, 2] <- mnar.5var.cces[1:6, 2] <- rep("Democrat", 6)
mnar.5var.anes[19:24, 2] <- mnar.5var.cces[19:24, 2] <- rep("Income", 6)

mnar.5var.anes$diff[mnar.5var.anes$method == "true"] <- mnar.5var.anes$value[mnar.5var.anes$method == "true"]
mnar.5var.cces$diff[mnar.5var.cces$method == "true"] <- mnar.5var.cces$value[mnar.5var.cces$method == "true"]
# mnar.5var.frame$diff[mnar.5var.frame$method == "true"] <- mnar.5var.frame$value[mnar.5var.frame$method == "true"]

levels(mnar.5var.anes$method) <- levels(mnar.5var.cces$method) <- levs
# levels(mnar.5var.frame$method) <- levs

mnar.5var <- cbind(mnar.5var.anes[,c(1,2,4)], mnar.5var.cces[,4])
# mnar.5var <- cbind(mnar.5var.anes[,c(1,2,4)], mnar.5var.cces[,4], mnar.5var.frame[,4])
colnames(mnar.5var) <- col.names

# to make the in-text citations shorter
mnar.5.anes <- mnar.5var$ANES
mnar.5.cces <- mnar.5var$CCES
# mnar.5.frame <- mnar.5var$Framing
mnar.5.meth <- mnar.5var$Method
mnar.5.var <- mnar.5var$Variable

tab.mnar.5var <- stargazer(mnar.5var,
                           summary = FALSE,
                           align = TRUE,
                           header = FALSE,
                           rownames = FALSE,
                           digits = 4,
                           title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, MNAR, 5 Variables with NA",
                           label = "mnar.5var")

et <- gsub("\\multicolumn{1}{c}{", "", tab.mnar.5var, fixed = TRUE)
cat(et)


```

<!--
MNAR 5 VAR
Vars the same as for MAR 5 Var
	Binary
		Overall difference to true value much higher for all methods for all ds for all vars. Around .0100 for Dem ANES and CCES, around .004 for Dem framing. Around .0125 for Male across all ds. For comparison: Around .0005 for Dem and Male for all ds for MAR 5 Var.
		hd.ord is closer to amelia and mice than for MAR, sometimes more (.0133 hd.ord vs. .0132 amelia Male ANES), sometimes less (.0120 hd.ord vs. .0099 mice Dem ANES). hd.ord actually best of all methods for framing for both vars, but overall amelia and mice perform better
		Interesting: na.omit as good as amelia and mice for Male framing and rather close for the other vars and ds too
	Ordinal
		Consistent picture of overall much higher difference to true value. In terms of method performance, same picture as for MAR: hd.ord worst across all ds. 
		amelia and mice best by far, virtually identical, though with much higher differences than for MAR
		Interesting: na.omit close to hd.ord for framing. 
	interval
		Much higher differences to true value, as consistent for all MNAR results. Same picture in terms of method performance, similar to Ordinal
		Interesting: na.omit better than hd.ord for Age ANES. na.omit also better than hd.ord and hot.deck for Age framing
-->

This section shows the imputation results for the MNAR missing data mechanism. MNAR amputation was achieved by setting the `mech` argument in the `ampute` function to `MNAR`. All MAR and MNAR analyses are otherwise identical. Table \ref{mnar.5var} shows the results of imputing both data sets MNAR for five amputed variables. It is immediately noticeable that the differences between the methods' imputation results and the true values are much higher for all methods for all variables for both data sets. The results for `Democrat` and `Male`, for instance, hover around 0.0100 and 0.0125 for both data sets. In the corresponding MAR analysis, however, the results for `Democrat` and `Male` showed around 0.0005. 



 \begin{table}[!htbp] \centering 
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, MNAR, 5 Variables with NA}  
 \label{mnar.5var} 
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex]  
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex]  
 true & Democrat & 0.&3420 & 0.&3770 \\
 hd.ord & Democrat & --0.&0114 & --0.&0099 \\ 
 hot.deck & Democrat & --0.&0120 & --0.&0105 \\
 amelia & Democrat & --0.&0106 & --0.&0102 \\
 mice & Democrat & --0.&0099 & --0.&0101 \\ 
 na.omit & Democrat & --0.&0176 & --0.&0140 \\
 true & Male & 0.&4890 & 0.&4830 \\ 
 hd.ord & Male & --0.&0136 & --0.&0116 \\ 
 hot.deck & Male & --0.&0133 & --0.&0124 \\
 amelia & Male & --0.&0132 & --0.&0121 \\
 mice & Male & --0.&0132 & --0.&0120 \\
 na.omit & Male & --0.&0214 & --0.&0219 \\
 true & Interest & 2.&9340 & 3.&3290 \\ 
 hd.ord & Interest & --0.&0288 & --0.&0246 \\ 
 hot.deck & Interest & --0.&0335 & --0.&0296 \\
 amelia & Interest & --0.&0167 & --0.&0146 \\
 mice & Interest & --0.&0167 & --0.&0146 \\
 na.omit & Interest & --0.&0379 & --0.&0372 \\
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&2299 & --0.&0928 \\ 
 hot.deck & Income & --0.&2554 & --0.&1038 \\
 amelia & Income & --0.&1225 & --0.&0578 \\
 mice & Income & --0.&1229 & --0.&0566 \\
 na.omit & Income & --0.&2770 & --0.&1334 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&6319 & --0.&4596 \\ 
 hot.deck & Age & --0.&7415 & --0.&5929 \\ 
 amelia & Age & --0.&2450 & --0.&2266 \\
 mice & Age & --0.&2369 & --0.&2160 \\ 
 na.omit & Age & --0.&6427 & --0.&6392 \\
 \hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table}


For the binary variables, `hd.ord` performs more closely on par with `amelia` and `mice` than in the corresponding MAR analysis above, sometimes more (`r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]` `hd.ord` vs. `r mnar.5.anes[mnar.5.meth == "amelia" & mnar.5.var == "Male"]` `amelia` ANES `Male`) and sometimes less so (`r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Democrat"]` `hd.ord` vs. `r mnar.5.anes[mnar.5.meth == "mice" & mnar.5.var == "Democrat"]` `mice` ANES `Democrat`). The results for the ordinal variables confirm those of the MAR analysis: `hd.ord` represents the worst method across both data sets. `amelia` and `mice` show by far the best results and are virtually identical with each other, though the differences to the true values are much higher than in the MAR analysis, as is the case for the entire MNAR analysis. The results for the interval variables paint the same picture as the ordinal ones. `hd.ord` shows the worst performance. Note that `na.omit` performs equally well as `hd.ord` for ANES `Age`.^[For a repeat of this MNAR analysis for 12 amputed variables, see appendix section \ref{app-ordmiss-12var}. The results do not change substantively.]




### Increased Number of Ordinal Variables {#ordmiss-results-increaseOrd}

```{r MULT MAR 4 Variables, include=FALSE}

mult.mar.4var.anes <- read.csv("data/anes/mar/results/anes.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mult.mar.4var.cces <- read.csv("data/cces/mar/results/cces.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mult.mar.4var.frame <- read.csv("data/framing/mar/results/framing.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mult.mar.4var.anes[1:6, 2] <- mult.mar.4var.cces[1:6, 2] <- rep("Democrat", 6)
mult.mar.4var.anes[13:18, 2] <- mult.mar.4var.anes[13:18, 2] <- rep("Income", 6)

mult.mar.4var.anes$diff[mult.mar.4var.anes$method == "true"] <- mult.mar.4var.anes$value[mult.mar.4var.anes$method == "true"]
mult.mar.4var.cces$diff[mult.mar.4var.cces$method == "true"] <- mult.mar.4var.cces$value[mult.mar.4var.cces$method == "true"]
# mult.mar.4var.frame$diff[mult.mar.4var.frame$method == "true"] <- mult.mar.4var.frame$value[mult.mar.4var.frame$method == "true"]

levels(mult.mar.4var.anes$method) <- levels(mult.mar.4var.cces$method) <- levs
# levels(mult.mar.4var.frame$method) <- levs

mult.mar.4var <- cbind(mult.mar.4var.anes[, c(1,2,4)], mult.mar.4var.cces[,4])
# mult.mar.4var <- cbind(mult.mar.4var.anes[, c(1,2,4)], mult.mar.4var.cces[,4], mult.mar.4var.frame[,4])
colnames(mult.mar.4var) <- col.names

# to make the in-text citations shorter
mult.mar.4.anes <- mult.mar.4var$ANES
mult.mar.4.cces <- mult.mar.4var$CCES
# mult.mar.4.frame <- mult.mar.4var$Framing
mult.mar.4.meth <- mult.mar.4var$Method
mult.mar.4.var <- mult.mar.4var$Variable

tab.mult.mar.4var <- stargazer(mult.mar.4var, 
                               summary = FALSE,
                               align = TRUE,
                               header = FALSE,
                               rownames = FALSE,
                               digits = 4,
                               title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MAR, 4 Variables with NA",
                               label = "mult.mar.4var")

ft <- gsub("\\multicolumn{1}{c}{", "", tab.mult.mar.4var, fixed = TRUE)
cat(ft)


```

<!--
Increased Ordinal
Treats Education and Interest with polr
Also split into MAR/MNAR and 4/11 Variables
MAR 4 Var
Vars the same as for MAR and MNAR 5 Var, just without Interest (so no ordinal vars here)
	Binary
		hd.ord worst for Dem across all ds. hd.ord better than hot.deck for Male across all ds.
		amelia and mice best, virtually identical, often zero
		Results for hd.ord get slightly worse when compared to MAR 5 Var:
		MAR 5 Var hd.ord: Dem .0011, .0004, .0008. Male .0013, .0014, .0005
		MAR 4 Var hd.ord: Dem .0018, .0005, .0012. Male .0015, .0018, .0011		
	interval
		hd.ord worst for all ds for all vars
		amelia and mice far best. mice does better for Inc and Age CCES, amelia does better for Inc and Age framing. Both equally good for Inc and Age ANES
		Consistent with the binary changes from one to two ordinal variables, the results for hd.ord get slightly worse when compared to MAR 5 Var:
		MAR 5 Var hd.ord: Inc .1278, .0407, .0192. Age .4597, .3895, .3923
		MAR 4 Var hd.ord: Inc .1523, .0516, .0225. Age .5431, .4664, .4583
-->

This section shows the imputation results for an increased number of ordinal variables to be treated by `polr` in `hd.ord`. Specifically, I add `Interest` to the `ord` argument in `hd.ord`. The intuition behind this is a strengthening of the underlying latent continuous variable assumption. The results of the previous analyses do not show superior performance by `hd.ord`, but perhaps this might be due to a lack of 'influence' so far. Perhaps one ordinal variable treated with `polr` is not enough to manifest itself in improved results. By including another ordinal variable in the treatment, this 'influence' is strengthened and the `polr` assumption is put to another test. As in sections \ref{ordmiss-results-mar} and \ref{ordmiss-results-mnar}, imputations are conducted MAR and MNAR. Because `Interest` 'moves' to the `polr` treatment, the number of imputed variables is reduced to four and 11, respectively, to ensure accurate comparison. This means the amputed variables do not include an ordinal variable any more. The remaining variables are the same as before.

Table \ref{mult.mar.4var} shows the results of imputing both data sets with two `polr`-treated variables MAR for four amputed variables. `hd.ord` displays the worst results for `Democrat` across both data sets and beats only `hot.deck` for `Male`. `amelia` and `mice` perform best and show virtually identically results that often match the true variable means. A comparison with the MAR analysis of five imputed variables reveals that `hd.ord` consistently performs slightly worse here: `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Democrat"]` and `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Democrat"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Democrat"]` and `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Democrat"]` for `Democrat`; `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Male"]` and `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Male"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Male"]`and `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` for `Male`.

 \begin{table}[!htbp] \centering  
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MAR, 4 Variables with NA} 
 \label{mult.mar.4var} 
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex]
 true & Democrat & 0.&3420 & 0.&3770 \\
 hd.ord & Democrat & --0.&0008 & +0.&0002 \\
 hot.deck & Democrat & --0.&0018 & --0.&0005 \\
 amelia & Democrat & +0.&0002 & +0.&0001 \\
 mice & Democrat & +0.&0001 & +0.&0002 \\
 na.omit & Democrat & --0.&0333 & --0.&0294 \\
 true & Male & 0.&4890 & 0.&4830 \\
 hd.ord & Male & --0.&0022 & --0.&0019 \\
 hot.deck & Male & --0.&0015 & --0.&0018 \\
 amelia & Male & +0.&0001 & +0.&0000 \\
 mice & Male & +0.&0000 & +0.&0000 \\ 
 na.omit & Male & --0.&0396 & --0.&0407 \\
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&0830 & --0.&0246 \\
 hot.deck & Income & --0.&1523 & --0.&0516 \\
 amelia & Income & +0.&0010 & --0.&0006 \\
 mice & Income & --0.&0008 & +0.&0002 \\
 na.omit & Income & --0.&5771 & --0.&2564 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&2889 & --0.&2350 \\
 hot.deck & Age & --0.&5431 & --0.&4664 \\
 amelia & Age & +0.&0018 & +0.&0085 \\
 mice & Age & +0.&0024 & --0.&0002 \\ 
 na.omit & Age & --1.&1521 & --1.&1228 \\
 \hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 
 
 

`hd.ord` also performs worst for both data sets across both interval variables. `amelia` and `mice` again perform best. `amelia` and `mice` perform equally well for ANES, while `mice` does better for CCES. As for the binary variables, the results for `hd.ord` consistently get slightly worse in the switch from one to two ordinal variables in `polr`-treatment: `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Income"]` and `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Income"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Income"]` and `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Income"]` for `Income`; `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Age"]` and `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Age"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Age"]` and `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Age"]` for `Age`.^[For a repeat of this MAR analysis for 11 amputed variables and two ordinal variables, see appendix section \ref{app-ordmiss-mult-11var}. The results do not change substantively.]






```{r MULT MNAR 4 Variables, include=FALSE}

mult.mnar.4var.anes <- read.csv("data/anes/mnar/results/anes.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mult.mnar.4var.cces <- read.csv("data/cces/mnar/results/cces.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mult.mnar.4var.frame <- read.csv("data/framing/mnar/results/framing.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mult.mnar.4var.anes[1:6, 2] <- mult.mnar.4var.cces[1:6, 2] <- rep("Democrat", 6)
mult.mnar.4var.anes[13:18, 2] <- mult.mnar.4var.anes[13:18, 2] <- rep("Income", 6)

mult.mnar.4var.anes$diff[mult.mnar.4var.anes$method == "true"] <- mult.mnar.4var.anes$value[mult.mnar.4var.anes$method == "true"]
mult.mnar.4var.cces$diff[mult.mnar.4var.cces$method == "true"] <- mult.mnar.4var.cces$value[mult.mnar.4var.cces$method == "true"]
# mult.mnar.4var.frame$diff[mult.mnar.4var.frame$method == "true"] <- mult.mnar.4var.frame$value[mult.mnar.4var.frame$method == "true"]

levels(mult.mnar.4var.anes$method) <- levels(mult.mnar.4var.cces$method) <- levs
# levels(mult.mnar.4var.frame$method) <- levs

mult.mnar.4var <- cbind(mult.mnar.4var.anes[, c(1,2,4)], mult.mnar.4var.cces[,4])
# mult.mnar.4var <- cbind(mult.mnar.4var.anes[, c(1,2,4)], mult.mnar.4var.cces[,4], mult.mnar.4var.frame[,4])
colnames(mult.mnar.4var) <- col.names

# to make the in-text citations shorter
mult.mnar.4.anes <- mult.mnar.4var$ANES
mult.mnar.4.cces <- mult.mnar.4var$CCES
# mult.mnar.4.frame <- mult.mnar.4var$Framing
mult.mnar.4.meth <- mult.mnar.4var$Method
mult.mnar.4.var <- mult.mnar.4var$Variable

tab.mult.mnar.4var <- stargazer(mult.mnar.4var, 
                                summary = FALSE,
                                align = TRUE,
                                header = FALSE,
                                rownames = FALSE,
                                digits = 4,
                                title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MNAR, 4 Variables with NA",
                                label = "mult.mnar.4var")

gt <- gsub("\\multicolumn{1}{c}{", "", tab.mult.mnar.4var, fixed = TRUE)
cat(gt)


```

<!--
MNAR 4 Var
Vars the same as for MAR 4 Var
	Binary
		hd.ord equally close to amelia and mice as for MNAR 5 Var, sometimes more (.0131 hd.ord vs. .0130 mice Dem CCES), sometimes less (.0155 hd.ord vs. .0127 mice Dem ANES). hd.ord actually best of all methods for framing for both vars, but overall amelia and mice perform better
		na.omit not as close as for MNAR 5 Var
		hd.ord consistently gets slightly worse when compared to MNAR 5 Var:		
		MNAR 5 Var hd.ord: Dem .0120, .0105, .0033. Male .0133, .0124, .0125
		MNAR 4 Var hd.ord: Dem .0155, .0131, .0042. Male .0172, .0160, .0157
	interval
		Consistent with previous analyses. 
		na.omit better than hd.ord for Age for all three ds and for Inc ANES
		hd.ord consistently gets slightly worse when compared to MNAR 5 Var:		
		MNAR 5 Var hd.ord: Inc .2554, .1038, .0648. Age .7415, .5929, .7477
		MNAR 4 Var hd.ord: Inc .3174, .1303, .0812. Age .8997, .7259, .9349
-->

Table \ref{mult.mnar.4var} shows the results of imputing both data sets with two `polr`-treated variables MNAR for four amputed variables. For the binary variables, `hd.ord` performs on the same level as `amelia` and `mice` when compared to the MNAR analysis of five imputed variables with only `Education` treated by `polr`; sometimes more (`r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Democrat"]` `hd.ord` vs. `r mult.mnar.4.cces[mult.mnar.4.meth == "mice" & mult.mnar.4.var == "Democrat"]` `mice` CCES `Democrat`), sometimes less so (`r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Democrat"]` `hd.ord` vs. `r mult.mnar.4.anes[mult.mnar.4.meth == "mice" & mult.mnar.4.var == "Democrat"]` `mice` ANES `Democrat`). `na.omit` does not perform as well as it does in Table \ref{mnar.5var}. `hd.ord` again consistently performs slightly worse with the two-ordinal-variable-`polr`-treatment: `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Democrat"]` and `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Democrat"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Democrat"]` and `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Democrat"]` for `Democrat`; `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Male"]` and `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Male"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]` and `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]` for `Male`.


 \begin{table}[!htbp] \centering
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MNAR, 4 Variables with NA} 
 \label{mult.mnar.4var} 
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline  
 \hline \\[-1.8ex]
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3420 & 0.&3770 \\ 
 hd.ord & Democrat & --0.&0142 & --0.&0133 \\ 
 hot.deck & Democrat & --0.&0155 & --0.&0131 \\
 amelia & Democrat & --0.&0136 & --0.&0131 \\
 mice & Democrat & --0.&0127 & --0.&0130 \\
 na.omit & Democrat & --0.&0211 & --0.&0185 \\
 true & Male & 0.&4890 & 0.&4830 \\
 hd.ord & Male & --0.&0180 & --0.&0162 \\
 hot.deck & Male & --0.&0172 & --0.&0160 \\
 amelia & Male & --0.&0170 & --0.&0154 \\
 mice & Male & --0.&0170 & --0.&0153 \\ 
 na.omit & Male & --0.&0233 & --0.&0241 \\
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&2481 & --0.&1034 \\ 
 hot.deck & Income & --0.&3174 & --0.&1303 \\
 amelia & Income & --0.&1555 & --0.&0741 \\
 mice & Income & --0.&1568 & --0.&0730 \\
 na.omit & Income & --0.&3114 & --0.&1513 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&6020 & --0.&4844 \\
 hot.deck & Age & --0.&8997 & --0.&7259 \\
 amelia & Age & --0.&3103 & --0.&2831 \\
 mice & Age & --0.&2994 & --0.&2702 \\
 na.omit & Age & --0.&6726 & --0.&6482 \\
 \hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 
 
 


The results for the interval variables are consistent with the previous analyses. In addition, note that `na.omit` performs better than `hd.ord` for `Age` in both data sets and for ANES `Income`. Once more, `hd.ord` consistently performs slightly worse with more than two ordinal variables: `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Income"]` and `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Income"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Income"]` and `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Income"]` for `Income`; `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Age"]` and `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Age"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Age"]` and `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Age"]` for `Age`.^[For a repeat of this MNAR analysis for 11 amputed variables and two ordinal variables, see appendix section \ref{app-ordmiss-mult-11var}. The results do not change substantively.]



\clearpage

### Increased Percentage of Missingness {#ordmiss-results-increaseNA}


```{r Increasing Missingness Percentage Table, include=FALSE}

# There is currently no CCES 80 percent .csv file because polr is acting up. In case the committee wants that, I can address it then

mar.5var.cces.20 <- mar.5var.cces
mar.5var.cces.50 <- read.csv("data/cces/mar/results/cces.mar.results.5var.1000n.1000it.50perc.csv") %>% .[,-1] %>% addPlus
# mar.5var.cces.80 <- read.csv("data/cces/mar/results/cces.mar.results.5var.1000n.1000it.80perc.csv") %>% .[,-1] %>% addPlus

mar.5var.cces.50[1:6, 2] <- rep("Democrat", 6)
mar.5var.cces.50[19:24, 2] <- rep("Income", 6)

mar.5var.cces.20$diff[mar.5var.cces.20$method == "true"] <-
  mar.5var.cces.50$diff[mar.5var.cces.50$method == "true"] <-
  # mar.5var.cces.80$diff[mar.5var.cces.80$method == "true"] <-
  mar.5var.cces.20$value[mar.5var.cces.20$method == "true"]

levels(mar.5var.cces.20$method) <-
  levels(mar.5var.cces.50$method) <-
  # levels(mar.5var.cces.80$method) <-
  levs

mar.5var.cces.perc <- cbind(mar.5var.cces.20[,c(1,2,4)], mar.5var.cces.50[,4])
# mar.5var.cces.perc <- cbind(mar.5var.cces.20[,c(1,2,4)], mar.5var.cces.50[,4], mar.5var.cces.80[,4])
colnames(mar.5var.cces.perc) <- c("Method", "Variable", "20% NA", "50% NA")
# colnames(mar.5var.cces.perc) <- c("Method", "Variable", "20% NA", "50% NA", "80% NA")

# to make the in-text citations shorter
cces.NA20 <- mar.5var.cces.perc$`20% NA`
cces.NA50 <- mar.5var.cces.perc$`50% NA`
# cces.NA80 <- mar.5var.cces.perc$`80% NA`
cces.perc.meth <- mar.5var.cces.perc$Method
cces.perc.var <- mar.5var.cces.perc$Variable

tab.mar.5var.cces.perc <- stargazer(mar.5var.cces.perc,
                                    summary = FALSE,
                                    align = TRUE,
                                    header = FALSE,
                                    rownames = FALSE,
                                    digits = 4,
                                    title = "Accuracy of Multiple Imputation Methods for Increasing Percentages of Missingness. CCES Data, MAR, Five Variables with NA",
                                    label = "mar.5var.cces.perc")

ht <- gsub("\\multicolumn{1}{c}{", "", tab.mar.5var.cces.perc, fixed = TRUE)
cat(ht)


```


<!--
Increased Missingness
Framing MAR 5 Var
	Binary
		hd.ord worst for all percentages for all vars
		hot.deck outperforms amelia and mice for Dem and Male CCES
		amelia and mice virtually identical
	Ordinal
		hd.ord worst for all percentages for all vars
		amelia and mice far best and virtually identical
	interval
		hd.ord worst for all percentages for all vars
		amelia and mice far best
		amelia outperforms mice for all percentages for all vars	
-->

This brief section shows the imputation results when the percentage of missingness is increased. I conduct this for data MAR for five variables with the CCES data. Figures \ref{accuracy20} and \ref{accuracy50} show the results for 20 and 50 percent missingness, respectively.


```{r Increasing Missingness Percentage Plots, include=FALSE}

# 20 percent missingness
mar.5var.cces.20.plot <- mar.5var.cces.20[,1:3]
mar.5var.cces.20.plot$value <- as.numeric(mar.5var.cces.20.plot$value)
mar.5var.cces.20.plot.dem <- filter(mar.5var.cces.20.plot, variable == "Democrat")

yint <- mar.5var.cces.20.plot.dem$value[mar.5var.cces.20.plot.dem$variable == "Democrat" & mar.5var.cces.20.plot.dem$method == "true"]
var.filter <- filter(mar.5var.cces.20.plot.dem, variable == "Democrat", method != "true")
var.filter$method <- factor(var.filter$method, levels = c("hd.ord", "hot.deck", "amelia", "mice", "na.omit"))
plot.dem.20 <- ggplot(var.filter, aes(x=method, value)) + 
    geom_point(size = 3) + 
    geom_hline(yintercept= yint, linetype = "dashed", color = "red", size = 1) + 
    theme(axis.title = element_blank()) + 
    annotate(geom="text", x=4.4, y=yint+0.0025, label="True benchmark",
             color="red") + 
  ggtitle("Democrat") +
  theme(plot.title = element_text(hjust = 0.5))

mar.5var.cces.20.plot.other <- filter(mar.5var.cces.20.plot, variable != "Democrat")
vars.unique <- mar.5var.cces.20.plot.other$variable %>% unique
plot.list.20 <- list()
plot.list.20[[1]] <- plot.dem.20

for(i in 1:length(vars.unique)){
  yint <- mar.5var.cces.20.plot.other$value[mar.5var.cces.20.plot.other$variable == vars.unique[i] & mar.5var.cces.20.plot.other$method == "true"]
  var.filter <- filter(mar.5var.cces.20.plot.other, variable == vars.unique[i], method != "true")
  var.filter$method <- factor(var.filter$method, levels = c("hd.ord", "hot.deck", "amelia", "mice", "na.omit"))
  plot.list.20[[i+1]] <- ggplot(var.filter, aes(x=method, value)) + 
    geom_point(size = 3) + 
    geom_hline(yintercept= yint, linetype = "dashed", color = "red", size = 1) + 
    theme(axis.title = element_blank()) +  
    ggtitle(vars.unique[i]) +
    theme(plot.title = element_text(hjust = 0.5))
}


# 50 percent missingness
mar.5var.cces.50.plot <- mar.5var.cces.50[,1:3]
mar.5var.cces.50.plot$value <- as.numeric(mar.5var.cces.50.plot$value)
mar.5var.cces.50.plot.dem <- filter(mar.5var.cces.50.plot, variable == "Democrat")

yint <- mar.5var.cces.50.plot.dem$value[mar.5var.cces.50.plot.dem$variable == "Democrat" & mar.5var.cces.50.plot.dem$method == "true"]
var.filter <- filter(mar.5var.cces.50.plot.dem, variable == "Democrat", method != "true")
var.filter$method <- factor(var.filter$method, levels = c("hd.ord", "hot.deck", "amelia", "mice", "na.omit"))
plot.dem.50 <- ggplot(var.filter, aes(x=method, value)) + 
    geom_point(size = 3) + 
    geom_hline(yintercept= yint, linetype = "dashed", color = "red", size = 1) + 
    theme(axis.title = element_blank()) + 
    annotate(geom="text", x=4.4, y=yint+0.006, label="True benchmark",
             color="red") + 
  ggtitle("Democrat") +
  theme(plot.title = element_text(hjust = 0.5))

mar.5var.cces.50.plot.other <- filter(mar.5var.cces.50.plot, variable != "Democrat")
vars.unique <- mar.5var.cces.50.plot.other$variable %>% unique
plot.list.50 <- list()
plot.list.50[[1]] <- plot.dem.50

for(i in 1:length(vars.unique)){
  yint <- mar.5var.cces.50.plot.other$value[mar.5var.cces.50.plot.other$variable == vars.unique[i] & mar.5var.cces.50.plot.other$method == "true"]
  var.filter <- filter(mar.5var.cces.50.plot.other, variable == vars.unique[i], method != "true")
  var.filter$method <- factor(var.filter$method, levels = c("hd.ord", "hot.deck", "amelia", "mice", "na.omit"))
  plot.list.50[[i+1]] <- ggplot(var.filter, aes(x=method, value)) + 
    geom_point(size = 3) + 
    geom_hline(yintercept= yint, linetype = "dashed", color = "red", size = 1) + 
    theme(axis.title = element_blank()) +  
    ggtitle(vars.unique[i]) +
    theme(plot.title = element_text(hjust = 0.5))
}

```

```{r Increased-Missingness-20-Percent, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Accuracy of Multiple Imputation Methods for 20 Percent Missingness. CCES Data, MAR, Five Variables with NA. Y-Axis Shows Percentages/Means.\\label{accuracy20}"}

grid.arrange(grobs = plot.list.20, ncol = 3, nrow = 2)

```

```{r Increased-Missingness-50-Percent, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Accuracy of Multiple Imputation Methods for 50 Percent Missingness. CCES Data, MAR, Five Variables with NA. Y-Axis Shows Percentages/Means.\\label{accuracy50}"}

grid.arrange(grobs = plot.list.50, ncol = 3, nrow = 2)

```

`hd.ord` performs comparatively well for 50 percent missing data `Democrat` (`r cces.NA50[cces.perc.meth == "hd.ord" & cces.perc.var == "Democrat"]` vs. `r cces.NA50[cces.perc.meth == "amelia" & cces.perc.var == "Democrat"]` `amelia`) but falls short for `Male` (`r cces.NA50[cces.perc.meth == "hd.ord" & cces.perc.var == "Male"]` vs. `r cces.NA50[cces.perc.meth == "mice" & cces.perc.var == "Male"]` `mice`). `hd.ord` also represents the second-worst imputation method for both percentages for the ordinal and interval variables. `amelia` and `mice` show virtually identical results, are hardly affected by the increase in missingness, and far outperform the other methods.


<!--
Table \ref{mar.5var.cces.perc}: `hd.ord` performs comparatively well for 50 percent missing data `Democrat` (`r cces.NA50[cces.perc.meth == "hd.ord" & cces.perc.var == "Democrat"]` vs. `r cces.NA50[cces.perc.meth == "amelia" & cces.perc.var == "Democrat"]` `amelia`) but falls short for `Male` (`r cces.NA50[cces.perc.meth == "hd.ord" & cces.perc.var == "Male"]` vs. `r cces.NA50[cces.perc.meth == "mice" & cces.perc.var == "Male"]` `mice`).


 \begin{table}[!htbp] \centering 
 \caption{Accuracy of Multiple Imputation Methods for Increasing Percentages of Missingness. CCES Data, MAR, Five Variables with NA} 
 \label{mar.5var.cces.perc}
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{20\% NA} & \multicolumn{2}{c}{50\% NA} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3770 & 0.&3770 \\
 hd.ord & Democrat & +0.&0000 & --0.&0006 \\
 hot.deck & Democrat & --0.&0004 & --0.&0012 \\
 amelia & Democrat & +0.&0001 & +0.&0000 \\
 mice & Democrat & +0.&0002 & +0.&0002 \\ 
 na.omit & Democrat & --0.&0229 & --0.&0516 \\
 true & Male & 0.&4830 & 0.&4830 \\
 hd.ord & Male & --0.&0011 & --0.&0020 \\
 hot.deck & Male & --0.&0014 & --0.&0036 \\
 amelia & Male & --0.&0001 & --0.&0001 \\
 mice & Male & --0.&0001 & +0.&0000 \\
 na.omit & Male & --0.&0414 & --0.&1032 \\ 
 true & Interest & 3.&3290 & 3.&3290 \\
 hd.ord & Interest & --0.&0125 & --0.&0336 \\
 hot.deck & Interest & --0.&0196 & --0.&0538 \\
 amelia & Interest & +0.&0003 & +0.&0001 \\
 mice & Interest & +0.&0000 & --0.&0003 \\
 na.omit & Interest & --0.&0724 & --0.&2014 \\ 
 true & Income & 6.&4810 & 6.&4810 \\
 hd.ord & Income & --0.&0259 & --0.&0809 \\
 hot.deck & Income & --0.&0407 & --0.&1240 \\
 amelia & Income & --0.&0004 & +0.&0010 \\
 mice & Income & --0.&0002 & +0.&0019 \\
 na.omit & Income & --0.&2468 & --0.&5958 \\
 true & Age & 52.&8230 & 52.&8230 \\ 
 hd.ord & Age & --0.&2616 & --0.&7685 \\
 hot.deck & Age & --0.&3895 & --1.&1573 \\ 
 amelia & Age & --0.&0033 & --0.&0075 \\
 mice & Age & --0.&0073 & --0.&0137 \\
 na.omit & Age & --1.&2361 & --3.&1442 \\
 \hline \\[-1.8ex]  
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 
-->








### Speed {#ordmiss-results-speed}

```{r Runtimes 5 Variables MAR, include=FALSE}

run.5var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.5var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.5var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.5var <- cbind(run.5var.anes, run.5var.cces[,2])
colnames(run.5var) <- c("", "ANES", "CCES")

## Older table for 5var and 12var together
# run.5var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.5var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# # run.5var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.12var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.12var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# # run.12var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.5var.12var <- data.frame(matrix(NA, 4, 5))
# # run.5var.12var <- data.frame(matrix(NA, 4, 7))
# colnames(run.5var.12var) <- c("Method", "ANES5", "CCES5", "ANES12", "CCES12")
# # colnames(run.5var.12var) <- c("Method", "ANES5", "CCES5", "Framing5", "ANES12", "CCES12", "Framing12")
# run.5var.12var$Method <- run.5var.anes$V2
# 
# runs <- list(run.5var.anes, run.5var.cces,
#              run.12var.anes, run.12var.cces)
# # runs <- list(run.5var.anes, run.5var.cces, run.5var.frame,
# #              run.12var.anes, run.12var.cces, run.12var.frame)
# 
# for(x in 1:length(runs)){
#   run.5var.12var[1:4, x+1] <- runs[[x]][,2]
# }
# 
# run.meth <- run.5var.12var$Method
# 
# # so I can use how much slower amelia is than hd.ord
# var.hd.am.div <- sapply(2:5, 
#                     function(x)
#                       run.5var.12var[run.meth == "amelia", x] / 
#                       run.5var.12var[run.meth == "hd.ord", x]) 
# 
# # so I can use how much slower mice is than hd.ord
# var.hd.mi.div <- sapply(2:5, 
#                     function(x)
#                       run.5var.12var[run.meth == "mice", x] / 
#                       run.5var.12var[run.meth == "hd.ord", x]) 
# 
# stargazer(run.5var.12var, 
#           summary = FALSE,
#           align = TRUE,
#           header = FALSE,
#           rownames = FALSE,
#           title = "Runtimes of Multiple Imputation Methods (in Minutes) by Number of Imputed Variables. ANES and CCES Data",
#           label = "runtimes5var12var")
# 
# 
# var.hd.am.div %>% min %>% round(., digits = 1)
# var.hd.am.div 
# var.hd.mi.div %>% max %>% round(., digits = 1)
# var.hd.mi.div
# var.hd.mi.div %>% min %>% round(., digits = 1)
```

<!--Speed
Analyzed by number of imputed variables for all ds (MAR)
	hd.ord and hot.deck much faster and virtually identical
	amelia consistently several times slower for all ds for both numbers of imputed vars
	mice much, much worse than all other methods across the board
Analyzed by percentage of missingness for framing data (MAR 5 Var)
	amelia and mice get closer as the percentage of missingess increases-->

This section shows the running times for all methods. I outline the speed differences for both data sets (Table \ref{runtimes5var}) and by the percentage of missingness for the CCES data (Table \ref{run.cces.perc}). Both analyses are conducted MAR for five imputed variables. All running times are given in minutes, apply to all 1,000 imputation iterations combined, and were achieved on a Code Ocean AWS EC2 instance with 16 cores and 120 GB of memory.

Table \ref{runtimes5var} shows `hd.ord` and `hot.deck` with virtually identical running times for both data sets. This is to be expected as both methods are very similar in terms of their code build-up. More importantly, however, we observe that both methods are much faster than `amelia` and `mice`: `amelia` is `r (run.5var[3, "ANES"] / run.5var[1, "ANES"]) %>% round(., digits = 1)` times slower than `hd.ord` for the ANES data and `r (run.5var[3, "CCES"] / run.5var[1, "CCES"]) %>% round(., digits = 1)` times slower for the CCES data. `mice`, however, is by far the slowest method and takes `r (run.5var[4, "CCES"] / run.5var[1, "CCES"]) %>% round(., digits = 1)` (CCES) and `r (run.5var[4, "ANES"] / run.5var[1, "ANES"]) %>% round(., digits = 1)` (ANES) times as long as `hd.ord`.^[For the runtimes for 12 imputed variables, see appendix section \ref{app-ordmiss-speed-12var}. The results do not change substantively.]


```{r Runtimes Increased Missingness MAR 5 Variables Table, results='asis', echo=FALSE}

stargazer(run.5var,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes). ANES and CCES Data, MAR, 5 Variables with NA",
          label = "runtimes5var")

```

<!--
\begin{table}[!htbp] \centering 
  \caption{Runtimes of Multiple Imputation Methods (in Minutes) by Number of Imputed Variables. ANES and CCES Data} 
  \label{runtimes5var12var} 
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{5 Variables with NA} & \multicolumn{2}{c}{12 Variables with NA}\\ 
\cline{2-5}  \\[-1.8ex]
 & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES}\\ 
\cline{2-3} 
\cline{4-5}  \\[-1.8ex]
\hline \\[-1.8ex] 
\multicolumn{1}{c}{hd.ord} & 2.632 & 2.778 & 2.614 & 2.679 \\ 
\multicolumn{1}{c}{hot.deck} & 2.628 & 2.786 & 2.640 & 2.690 \\ 
\multicolumn{1}{c}{amelia} & 9.052 & 10.611 & 9.038 & 10.345 \\ 
\multicolumn{1}{c}{mice} & 50.445 & 57.205 & 104.143 & 113.390 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}
-->


```{r Runtimes Increased Missingness MAR Code, include=FALSE}

# There is currently no CCES 80 percent .csv file because polr is acting up. In case the committee wants that, I can address it then

run.cces.20 <- run.5var.cces
run.cces.50 <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.50perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.cces.80 <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.80perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.cces.perc <- cbind(run.cces.20$V1, run.cces.50$V1) %>% as.data.frame %>% round(., digits = 2)
# run.cces.perc <- cbind(run.cces.20$V1, run.cces.50$V1, run.cces.80$V1) %>% as.data.frame %>% round(., digits = 2)
run.cces.perc$V3 <- run.cces.20$V2 %>% as.character
# run.cces.perc$V4 <- run.cces.20$V2 %>% as.character
run.cces.perc <- run.cces.perc[,c(3, 1:2)]
# run.cces.perc <- run.cces.perc[,c(4, 1:3)]
colnames(run.cces.perc) <- c("Method", "20% NA", "50% NA")
# colnames(run.cces.perc) <- c("Method", "20% NA", "50% NA", "80% NA")

perc.hd.am.div <- run.cces.perc[3,2:3] %>% as.numeric / run.cces.perc[1,2:3] %>% as.numeric
perc.hd.mi.div <- run.cces.perc[4,2:3] %>% as.numeric / run.cces.perc[1,2:3] %>% as.numeric


```


Table \ref{run.cces.perc} shows that `hd.ord` and `hot.deck` remain the fastest methods across both percentages of missingness, but the gap to `amelia` and `mice` narrows as the missingness increases. `amelia` improves from `r perc.hd.am.div %>% max %>% round(., digits = 1)` times slower than `hd.ord` for 20 percent missing data to `r perc.hd.am.div %>% min %>% round(., digits = 1)` times slower for 50 percent missing data. Similarly, `mice` speeds up from `r perc.hd.mi.div %>% max %>% round(., digits = 1)` to `r perc.hd.mi.div %>% min %>% round(., digits = 1)` times slower.



```{r Runtimes Increased Missingness MAR Table, results='asis', echo=FALSE}

stargazer(run.cces.perc,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes) by Percentage of Missingness. CCES Data",
          label = "run.cces.perc")

```


## Brief Evaluation of Shortcomings {#ordmiss-eval}

Given `hd.ord`'s inferior performance, we need to speculate about possible reasons. Perhaps the importance of the uneven distances between ordinal variable categories is over-emphasized in the literature, with the distances potentially being not as uneven as previously thought. The fact that `hd.ord` consistently performs worse when a second ordinal variable is added to the `polr` treatment seems to point in this direction. To investigate this possibility, I test the influence of `polr` on regression coefficient transformations with data from previous publications.

<!-- 
Background for this bit -- Jeff suggested this as part of his feedback:
	\begin{coi}
		\item Take research (Dataverse) from prominent research people: Michigan folks, Larry Bartels
		\item Take a glm regression (logit, probit, poisson model) with a RHS variable that is ordinal, perhaps scaled 1-7
		\item Throw away the outcome variable, put the ordinal variable as the DV
		\item Run polr and run linear regression on that
		\item What I'm looking for: How different are the regression coefficients (the RHS) between the two models? This difference shows how important the re-estimating of the ordinal spaces is
		\item If the results show no differences, this makes my speculation at the end, that the uneven spaces might not be that important after all, much stronger $\rightarrow$ and if the results show that there are actually big differences, this then excludes the this-isn't-that-important-after-all explanation as a possible reason why my analysis wasn't working $\rightarrow$ so it's a win-win situation no matter what the results show
	\end{coi}
-->

The first column of Table \ref{ordmiss-bartels-92} shows a replication of a linear model estimated by @bartels_1999_panel on the 1992 ANES data. Bartels regresses several explanatory variables on `Campaign Interest` (this estimation corresponds to the column "Panel" in Table 2 of the original publication). As we can see, the ordinal variable `Education` is part of the explanatory variables in the model. To test the transformations implemented by an ordered probit model for ordinal variables, I remove the current outcome variable (`Campaign Interest`) and replace it with `Education`. The resulting model is then estimated as a linear regression (column two) and an ordered probit regression (column three).


```{r lm and polr Differences on External Data, include=FALSE}

bart.92 <- read_sav("data/bartels/panel92p.sav")
# bart.96 <- read_sav("data/bartels/panel96p.sav")
# bart.96$V3b <- bart.96$V3^2

# all variables taken from data/bartels/var_descriptions.txt
bart.cols <- list(
  c("V4", "V3", "V5", "V6", "V7", "V13", "V16", "V17")
  # , c("V4", "V10", "V9", "V3", "V3b", "V8", "V6", "V12", "V18")
  )

bart.colnames <- list(
  c("Education", "Age", "Income", "Black", "Female", "Camp_int", "Part_strength", "Days_bef_elec")
  # , c("Education", "Rep_part", "Cons_ideol", "Age", "Age_squared", "Married", "Black", "Foll_pub_aff", "Clinton_morality")
  )

bart.dv.orig <- c("Camp_int"#, "Clinton_morality"
                  )
bart.dv.lm <- "Education"
dv.polr <- paste0(bart.dv.lm, ".fac")

bart.dfs <- list(bart.92#, bart.96
                 )
bart.dfs.sub <- list()
bart.evs.orig <- list()
bart.evs.new <- list()
bart.reg.res <- rep(list(list()), length(bart.dfs))

models <- c("orig", "lm", "polr")
years <- c(92#, 96
           )
mod.temp <- rep(list(list()), length(models))
names(mod.temp) <- models
bart.reg.res <- rep(list(mod.temp), length(bart.dfs))
names(bart.reg.res) <- years

for (n in 1:length(bart.dfs)){
  bart.dfs.sub <- subset(bart.dfs[[n]], subset = c(V1 == 1)) %>% .[bart.cols[[n]]]
  colnames(bart.dfs.sub) <- bart.colnames[[n]]
  bart.evs.orig <- colnames(bart.dfs.sub)[!colnames(bart.dfs.sub) %in% bart.dv.orig[n]]
  bart.dfs.sub[[dv.polr]] <- bart.dfs.sub[[bart.dv.lm]] %>% as.factor
  bart.evs.new <- bart.evs.orig[!bart.evs.orig %in% c(bart.dv.lm, dv.polr)]
  bart.reg.res[[n]][["orig"]] <- lm(paste(bart.dv.orig[[n]], paste(bart.evs.orig, collapse = " + "), sep = " ~ "),
                               data = bart.dfs.sub)
  bart.reg.res[[n]][["lm"]] <- lm(paste(bart.dv.lm, paste(bart.evs.new, collapse = " + "), sep = " ~ "),
                               data = bart.dfs.sub)
  bart.reg.res[[n]][["polr"]] <- polr(paste(dv.polr, paste(bart.evs.new, collapse = " + "), sep = " ~ "),
                                 data = bart.dfs.sub, Hess=TRUE)
  }

bart.92.lm <- bart.reg.res[["92"]][["lm"]] %>% 
  summary %>% 
  .$coefficients %>% 
  data.frame %>% 
  .[-1, 1:2]
bart.92.polr <- bart.reg.res[["92"]][["polr"]] %>% 
  summary %>% 
  .$coefficients %>%
  data.frame %>%
  .[1:nrow(bart.92.lm), 1:2]

xlab <- c("Age", "Income", "Black", "Female", "Partisan strength", "Days before election")

plots.92 <- list()
ov.perc <- c()
reps <- 100000

for (i in 1:nrow(bart.92.lm)){
  set.seed(126)
  lm.norm.92 <- rnorm(reps, mean = bart.92.lm[i,1], 
                      sd = bart.92.lm[i,2])
  polr.norm.92 <- rnorm(reps, mean = bart.92.polr[i,1], 
                        sd = bart.92.polr[i,2])
  
  # # to possibly add to plots
  # t.res <- t.test(lm.norm.92, polr.norm.92)
  # t.res %>% .$statistic
  # t.res %>% .$conf.int
  lm.92 <- cbind(lm.norm.92, rep("lm", reps)) %>% data.frame
  polr.92 <- cbind(polr.norm.92, rep("polr", reps)) %>% data.frame
  colnames(lm.92) <- colnames(polr.92) <- c("Coefficient", "Model")
  over.lap  <- list(lm.92$Coefficient %>% as.numeric, 
                    polr.92$Coefficient %>% as.numeric) %>% 
    overlap(.) %>%
    .$OV * 100
  ov.perc[i] <- round(over.lap, digits = 2)
  df.92 <- rbind(lm.92, polr.92)
  df.92$Coefficient <- df.92$Coefficient %>% as.numeric
  grob <- grobTree(textGrob(paste("Overlap:",  paste0(ov.perc[i], " %"), sep="\n"),
                            x=0.75,  y=0.75, hjust=0,
                            gp=gpar(col="red", fontsize=10)))
  if(i == 1){
    plots.92[[i]] <- ggplot(df.92, aes(x=Coefficient, fill=Model)) + 
      geom_density(alpha=0.2, aes(y=..density..), position="identity") + 
      xlab(xlab[i]) + 
      theme(axis.title.y=element_blank()) + 
      theme(legend.title=element_blank()) + 
      theme(legend.position = c(0.15, 0.75)) + 
      theme(plot.title = element_text(hjust = 0.5)) +
      annotation_custom(grob)
  } else{
    plots.92[[i]] <- ggplot(df.92, aes(x=Coefficient, fill=Model)) + 
      geom_density(alpha=0.2, aes(y=..density..), position="identity") + 
      xlab(xlab[i]) + 
      theme(axis.title.y=element_blank()) + 
      theme(legend.title=element_blank()) + 
      theme(legend.position = "none") + 
      theme(plot.title = element_text(hjust = 0.5)) +
      annotation_custom(grob)
  }
}

stargazer(bart.reg.res[["92"]][["orig"]],
          bart.reg.res[["92"]][["lm"]],
          bart.reg.res[["92"]][["polr"]],
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          dep.var.labels = c("Campaign Interest", "Education", "Education"),
          covariate.labels = c("Education", "Age", "Income", "Black", "Female", "Partisan strength", "Days before election"),
          title = "`lm` and `polr` Differences in 1992 ANES Data as Used by Bartels (1999)",
          no.space = TRUE,
          model.numbers = FALSE,
          star.char = c("", "", ""),
          label = "ordmiss-bartels-92",
          omit.table.layout = "n")

```


\begin{table}[!htbp] \centering 
  \caption{`lm` and `polr` Differences in 1992 ANES Data as Used by Bartels (1999)} 
  \label{ordmiss-bartels-92} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{1}{c}{Campaign Interest} & \multicolumn{1}{c}{Education} & \multicolumn{1}{c}{Education} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{\textit{OLS}} & \multicolumn{1}{c}{\textit{OLS}} & \multicolumn{1}{c}{\textit{ordered}} \\ 
 & \multicolumn{1}{c}{\textit{}} & \multicolumn{1}{c}{\textit{}} & \multicolumn{1}{c}{\textit{logistic}} \\ 
\hline \\[-1.8ex] 
 Education & 0.023^{} &  &  \\ 
  & (0.004) &  &  \\ 
  Age & 0.002^{} & -0.032^{} & -0.021^{} \\ 
  & (0.001) & (0.004) & (0.003) \\ 
  Income & 0.071^{} & 4.092^{} & 3.133^{} \\ 
  & (0.037) & (0.245) & (0.204) \\ 
  Black & -0.028 & -0.865^{} & -0.673^{} \\ 
  & (0.028) & (0.198) & (0.150) \\ 
  Female & -0.055^{} & -0.058 & -0.054 \\ 
  & (0.018) & (0.133) & (0.102) \\ 
  Partisan strength & 0.214^{} & 0.290 & 0.196 \\ 
  & (0.027) & (0.198) & (0.152) \\ 
  Days before election & -0.001^{} & 0.014^{} & 0.012^{} \\ 
  & (0.0005) & (0.004) & (0.003) \\ 
  Constant & 0.391^{} & -0.388 &  \\ 
  & (0.038) & (0.273) &  \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{1,359} & \multicolumn{1}{c}{1,359} & \multicolumn{1}{c}{1,359} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.114} & \multicolumn{1}{c}{0.248} &  \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.109} & \multicolumn{1}{c}{0.245} &  \\ 
Residual Std. Error & \multicolumn{1}{c}{0.331 (df = 1351)} & \multicolumn{1}{c}{2.401 (df = 1352)} &  \\ 
F Statistic & \multicolumn{1}{c}{24.750$^{}$ (df = 7; 1351)} & \multicolumn{1}{c}{74.507$^{}$ (df = 6; 1352)} &  \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}


To determine how statistically distinct the variables on the right-hand side in these models are (`Age`, `Income`, `Black`, `Female`, `Partisan strength`, `Days before election`), I simulate the posterior distribution of each $\bm{\beta}$ coefficient from both regressions in columns two and three as a normal distribution with mean $\bm{\hat{\beta}}$, standard error $SE(\bm{\hat{\beta}})$, and $n = 100,000$. I then plot the overlapping distributions of each linear regression coefficient with the corresponding ordered probit regression coefficient to assess the posterior percentage of overlay. The results are shown in Figure \ref{DensBart92}. 


```{r Density-Plots-Bartels-1992, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distributions of `lm` and `polr` Coefficients in 1992 ANES Data as Used by Bartels (1999)\\label{DensBart92}"}

grid.arrange(grobs = plots.92, ncol = 2, left = "Density")

```

```{r Bartels 1996 data, results='asis', echo=FALSE}

# stargazer(bart.reg.res[["96"]][["orig"]],
#           bart.reg.res[["96"]][["lm"]],
#           bart.reg.res[["96"]][["polr"]],
#           align = TRUE,
#           header = FALSE,
#           rownames = FALSE,
#           title = "lm and polr Differences in 1996 ANES Data as used by Bartels (1999)",
#           dep.var.labels = c("Clinton Morality", "Education", "Education"),
#           covariate.labels = c("Education", "Republican partisanship", "Conservative ideology", "Age", "Age squared", "Married", "Black", "Follow public affairs"),
#           no.space = TRUE,
#           model.numbers = FALSE,
#           label = "ordmiss-bartels-96")

```


We observe a small posterior percentage of overlay for the distributions of `Age` and `Income` (< 10 percent) and a large posterior percentage of overlay for the distributions of `Black`, `Female`, `Partisan strength`, and `Days before election` (> 40 percent). Since a large percentage indicates little difference in significance between the two sets of coefficients, these results appear provide evidence against the importance of re-estimating ordinal variable categories with an ordered probit model. It seems that uneven distances between ordinal variable categories might not actually be of crucial importance when it comes to missing data imputation.



## Conclusion {#ordmiss-conclusion}

I set out to improve multiple imputation results with ordinal variables by accounting for the unevenly spaced ordering contained in ordinal variables. I did so by adapting the multiple hot deck imputation function `hot.deck` to treat ordinal variables with an ordered probit model in order to estimate numerical thresholds from an assumed underlying latent continuous variable. The results clearly show diverging outcomes from the different algorithms, with each algorithm behaving differently under changing circumstances. 

`hd.ord` performs on par with some binary variables but overall worse than `amelia` and `mice` for data MAR with 5 variables with missing values. The results for the MNAR analyses paint a more mixed but overall unchanged picture. `hd.ord` performs somewhat better for binary variables but remains the least accurate method for interval and ordinal variables. Increasing the number of ordinal variables included in the `polr` treatment does not have a positive effect on the performance of `hd.ord`. In fact, `hd.ord` consistently performs slightly worse for both data sets for all mechanisms of missingness. Unlike `amelia` and `mice`, `hd.ord`'s performance also drastically worsens when the percentage of missingness in the data is increased to 50 percent.

On the positive side, `hd.ord` performs multiple imputation much more quickly: `amelia` and `mice` are at least `r (run.5var[3, "CCES"] / run.5var[1, "CCES"]) %>% round(., digits = 1)` and `r (run.5var[4, "CCES"] / run.5var[1, "CCES"]) %>% round(., digits = 1)` times slower than `hd.ord` for 20 percent of missing data, respectively. This speed gain is of dubious value, however. While it is necessary to iterate multiple imputation runs many times over for simulation purposes like this one, users likely will not do so, which greatly diminishes the computing time saved. Even if users do opt to run multiple imputation for 1,000 times, the differences in terms of absolute time for `amelia` are not so great as to be impractical. `amelia` took at most a little under `r run.5var[3, "CCES"] %>% round(., digits = 0)` minutes to compute 1,000 iterations of multiple imputation for 20 percent missing data, regardless of the data set in question. In absolute terms, this is not a lot of time for the general user to invest in data preparation. With a minimum of just over `r run.5var[4, "ANES"] %>% round(., digits = 0)` minutes, the same cannot be said for `mice`. Nonetheless, `hd.ord`'s speed gain over `amelia` cannot be considered enough reason to choose `hd.ord` over `amelia`, unless the data consists of exclusively binary variables where the differences between `hd.ord` and `amelia` are small.

Given all the above, the result of this quality comparison of major missing data solutions is a clear endorsement of `amelia`. It performs well for all types of variables in all stages of missingness and does so in a reasonably short amount of time. The combination of EM with bootstrapping clearly represents a great improvement in terms of speed over IP used in `mice`. While it offers a wealth of sophisticated options for specialized users, `amelia`'s default out-of-the-box settings are simple and intuitive for general users. On top of that, it is notable that `amelia` produces better results than `na.omit` when data is MNAR, i.e. it performs well in a setting it was not designed for. `amelia` thus represents the best `R` solution to problems of missing data for general users.

While `hd.ord` did not yield the desired improvements, my analysis provides in-depth insights into and corroborates the robustness of multiple imputation implementations like `amelia` and `mice` for a variety of variable types in survey settings.


<!--

What doesn't work

-- Increasing percentage of NAs == worse `hd.ord` performance
-- Ordinal and interval variables == worse `hd.ord` performance
-- High number of observations == reduced number of iterations (crashes and/or RAM maxing out)
-- High number of observations == makes `amelia` faster relative to `hd.ord`
-- 17 ANES education levels == increases needed number of iterations
-- 17 ANES education levels == causes `amelia` to stop on CO and Jeff
-- 10,000 iterations == results with 1,000 iterations are just as good
-- `ampute` with `bycases=FALSE` and `cont=FALSE` == worse `hd.ord` performance, good `na.omit` performance
-- `own.NA` == `na.omit` performs best (currently in appendix)
-- `own.NA.rows`== pretty much everything is zero, incl. `na.omit`; `mice` is awful (currently in appendix)
-- Running `hd.ord` with `method = p.draw` == worse `hd.ord` performance
-- Running `hd.ord` with `method = p.draw` == only works with only binary vars in the data
-- Increasing `sdCutoff` == only does something with only binary vars in the data
-- Only binary vars in data == no gain in `hd.ord` performance
-- amelia is very finicky when it comes to collinearity



What works

-- Results for binary variables == equal performance of `hd.ord`, `mice`, `amelia`
-- Increasing number of variables with NAs (all, not just binary) == better `hd.ord` performance
-- 1000 observations in data sets == increases `amelia` running time
-- MNAR == MAR in terms of `hd.ord` performance
-- Multiple ordinal variables == same performance as with one ordinal variable

?? mice.pdf: 6.2. Sensitivity analysis under MNAR

-->
