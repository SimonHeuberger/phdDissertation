---
# AU dissertation fields
title: "WHAT PEOPLE THINK: ADVANCES IN PUBLIC OPINION MEASUREMENT USING ORDINAL VARIABLES"
author: "Simon Heuberger"
degreeyear: "2021"
degree: "Doctor of Philosophy"
chair: "Professor Jeff Gill"
secondreader: "Professor Ryan T. Moore"
thirdreader: "Professor Elizabeth Suhay"
fourthreader: "Professor R. Michael Alvarez"
fifthreader: "Professor Elizabeth Malloy"
degreefield: "Political Science"
abstract: |
  Surveys are a central part of political science. Without surveys, we would not know what people think about political issues. Survey experiments also enable us to test how people react to given treatments. Surveys and survey experiments are only as good as the analytical techniques we as researchers use, though. This applies particularly to how we use and measure variables. For ordinal variables, some of our current measurements and techniques are insufficient. Ordinal variables consist of ordered categories where the spacing between each category is uneven and not known. One example is education, one of the most important predictors of political behavior. The distances between education categories such as "Elementary School", "Some High School", and "High School Graduate" are not evenly spread. Current practice nonetheless often does not take this information into account. This could misrepresent the data and potentially distort results. My dissertation develops two methods to address this and applies them in original survey research. Chapter \ref{ordblock} develops a new method to improve the use of ordinal variables in blocking in survey experiments. Preliminary evidence suggests that the re-estimation of ordinal variable categories with an ordered probit approach might matter, but the results are mixed. Chapter \ref{ordmiss} develops a new method to treat missing survey data with ordinal variables. The results show that the method performs worse than existing software, despite expectations to the contrary. Chapter \ref{framing} applies both methods in an online survey experiment that tests morality and self-interest in political framing. The results confirm the negative findings from the previous chapters but also show tentative evidence for the importance of morality in issue-opposing frames.
acknowledgements: |
  Throughout the writing of this dissertation I have received a great deal of support and assistance. First and foremost, I want to thank my supervisor and chair Jeff Gill. It is safe to say that this dissertation would not exist without him. He has been my guide in the world of political methodology and has set me on a path I would not have taken without him. In addition to his invaluable and unwavering support throughout this project, Jeff also trusted me with the replications for Political Analysis. Working for PA has provided me with contact to scholars around the world, which has opened countless doors to job opportunities. It was a pleasure and a true honor to work with Jeff all these years.\newline
  I am also extremely grateful for all the other members on my committee. Ryan Moore has accompanied me on my graduate journey from the beginning as my teacher, supervisor, and committee member. He introduced me to `R` six years ago, and I have never looked back. He has been a fountain of knowledge in all things statistics and coding throughout. Without Liz Suhay, the framing chapter would be a shadow of what it has become. Her passion for emotions and clear eye for detail has been a huge support in designing the experiment and developing the questionnaire. I met Mike Alvarez through my work for PA several years ago. He has been an inspiration on replication work, and I am honored that he accepted my invitation to join the committee. Finally, Betty Malloy very kindly agreed to serve as the external reader at very short notice during tumultuous Covid times. I am extremely grateful for all their contributions and support. I consider myself very lucky to have been given the chance to work with such outstanding scholars.\newline
  On a personal level, thanks are endless. To my friend Gabe: You have been a hero during the dissertation stage. I could not and would not have done this without you. When things got tough, you were there. Your `R` proficiency leaves a little to be desired but hey, no one is perfect. I missed you terribly during these Covid times and hope to see you again soon. To my friend Hauke: We have known each other for many years, and I am blessed by each one. I will never forget the emotinal and practical support you have given me all this time. I am honored to be Linus' godfather. To many, many more years of friendship to come. Lastly, to my family Angelika, Ludwig, Katherina, Kilian, Lorenz, and Michela: You guys are my everything. No more and no less. I don't know what I would do without any of you. Life is good because of you.\newline
  Last but not least: Eternal gratitude for love and devotion that outshines humankind to Fendi and Muesli, the two greatest creatures on the planet and the best things I have ever done. And of course to Waldemar Burgo.
# End of AU dissertation fields
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  eagledown::dissertation_pdf: 
    latex_engine: xelatex
#  eagledown::dissertation_gitbook: default
#  eagledown::dissertation_word: default
#  eagledown::dissertation_epub: default
bibliography: sources/all-together.bib
csl: sources/apa.csl
---

```{r include_packages, include = FALSE}
# This chunk ensures that all required packages for the AU dissertation template are installed and loaded
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(stargazer))
  install.packages("stargazer", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
  install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(eagledown))
  devtools::install_github("SimonHeuberger/eagledown")
if(!require(dplyr))
  install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(gridExtra))
  install.packages("gridExtra", repos = "http://cran.rstudio.com")
if(!require(blockTools))
  install.packages("blockTools", repos = "http://cran.rstudio.com")
if(!require(randomizr))
  install.packages("randomizr", repos = "http://cran.rstudio.com")
if(!require(forcats))
  install.packages("forcats", repos = "http://cran.rstudio.com")
if(!require(magrittr))
  install.packages("magrittr", repos = "http://cran.rstudio.com")
if(!require(mice))
  install.packages("mice", repos = "http://cran.rstudio.com")
if(!require(data.table))
  install.packages("data.table", repos = "http://cran.rstudio.com")
if(!require(haven))
  install.packages("haven", repos = "http://cran.rstudio.com")
if(!require(MASS))
  install.packages("MASS", repos = "http://cran.rstudio.com")
if(!require(overlapping))
  install.packages("overlapping", repos = "http://cran.rstudio.com")
if(!require(lattice))
  install.packages("lattice", repos = "http://cran.rstudio.com")
if(!require(grid))
  install.packages("grid", repos = "http://cran.rstudio.com")
if(!require(tidyverse))
  install.packages("tidyverse", repos = "http://cran.rstudio.com")
if(!require(caret))
  install.packages("caret", repos = "http://cran.rstudio.com")
if(!require(broom))
  install.packages("broom", repos = "http://cran.rstudio.com")
if(!require(reshape2))
  install.packages("reshape2", repos = "http://cran.rstudio.com")
library(devtools)
library(stargazer)
library(ggplot2)
library(plyr)
library(dplyr)
require(gridExtra)
library(blockTools)
library(randomizr)
library(forcats)
library(magrittr)
library(mice)
library(data.table)
library(haven)
library(MASS)
library(overlapping)
library(lattice)
library(grid)
library(tidyverse)
library(caret)
library(broom)
library(reshape2)
options(scipen=999) # turn off scientific notation (e.g. e-04)

```


<!--chapter:end:00-thesis.Rmd-->

# INTRODUCTION {#intro}

News outlets, political campaigns, and survey institutes spend millions of dollars every year in the attempt to uncover what people think. The exact nature of public opinion is highly sought after information. It is not hard to see why: Public opinion is central to the functioning of democracy. It consists of the desires and wants of people. Their collective views and opinions determine who gets elected, what policies are enacted, and how society is structured. We need to know what people want, how they think, how they feel, what they support, and what they oppose because these wants and desires directly inform policy making and political decisions. We do so by conducting surveys. Surveys are a central part of public opinion. Without surveys, we would not know what people think. They collect data from a population or a sample population to gain information and insights into political issues. Surveys are ubiquitous in today's media environment. Research institutes like the Pew Research Center conduct dozens of surveys on numerous topics every year. In the run-up to the 2020 presidential election, the number of opinion polls conducted in swing states easily reached triple digits.

In political science, surveys are often used to analyze political attitudes. To establish causal effects, researchers use survey experiments. Like surveys, survey experiments collect background information on people's demographics. Unlike surveys, however, they also include attempts to uncover treatment effects on public opinion by testing one or several hypotheses. Similar to a laboratory setting, this involves exposing respondents to differing conditions in order to determine the effect of the intervention in question. In medical research, a treatment may take the form of a pill. In a survey context, a treatment can take the form of particular types of questions, different question wordings, or exposure to a particular type of content, among many others. Survey experiments are one of the most powerful methodological tools in political science. By combining experimental design that provides clear causal inference with the flexibility of the survey context as a site for attitude research, survey experiments can be used in almost any field to study almost any question. 

Surveys and survey experiments are only as good as the analytical techniques we as researchers use, though. This applies particularly to how we phrase and measure the questions we ask respondents. If a question is worded confusingly, for instance with a double negative, we cannot gain much insight from it. If we provide unsuitable response options, for example in multiple choice questions, we might obtain inaccurate information. Take for instance a potential question inquiring after support for the legalization of marijuana. Say this question only offered the response choices "Yes, I support legalization" and "No, I don't support legalization". Are these good choices? Most likely not. Restricting the range of possible answers to a binary choice denies respondents the chance to voice more complex opinions. Some people might support legalization a little bit. Others might be highly enthusiastic about it. Yet others again might rigidly oppose it, while a fourth group is ambivalent about the subject. How we word questions and how we measure responses matters. This does not end at questionnaire design. How we use the collected information for subsequent analysis is just as, if not more, important. The statistical techniques and methods we use to analyze survey and survey experiment data crucially influence the results we obtain. This applies particularly to different types of variables. While some variables are comparatively easy to wrangle, such as numerical ones, others provide a greater challenge. For one particular type, namely ordinal variables, I consider some of our current analytical techniques to be insufficient.

Ordinal variables are part of the larger framework of categorical variables. Categorical variables represent types of data which are commonly divided into three groups: nominal, interval, and ordinal. Nominal variables are categorical variables with two or more categories that are not intrinsically ordered. Examples include gender (Female, Male, Transgender etc.), race (African-American, White, Hispanic etc.), and party ID (Democrat, Republican, Independent) where the categories cannot be ordered sensibly into highest or lowest. Interval variables are ordered categorical variables with evenly spaced values. Examples include income (\$20,000, \$40,000, \$60,000, \$80,000 etc.), where the distance between \$20,000 and \$40,000 is the same as the distance between \$60,000 and \$80,000. Ordinal variables are ordered categorical variables where the spacing between values is not the same. Examples include education (Elementary School, Some High School, High School Graduate etc.) where the distance between "Elementary School" and "Some High School" is likely different than the distance between "Some High School" and "High School Graduate". Each subsequent category has quantitatively more education than the previous one, but the exact measure of the distances between the categories is not known.  

Current practice often does not take ordinal variable information into account. One such technique involves turning ordinal categories into categorical indicators. Researchers here create a new binary variable for an education category, for instance "High School Graduate". While it removes the issue of uneven distances, this approach ignores the ordered nature contained in ordinal variables. Education categories are treated like nominal variables, i.e. we would wrongly assume that the ordering of education categories is arbitrary. Ordinal variables are also often made numerical for analytic purposes. This is problematic because of their unevenly spaced categories. If we turn the education categories "Elementary School", "Some High School", and "High School Graduate" into the numerical values 1, 2, and 3, we wrongly assume that the distances between the education categories correspond to these evenly spaced values. Do the numbers 1 to 3 really represent the distances between these categories? Perhaps the true spacing between some of the categories is so narrow they should not even be separate categories at all. We cannot answer this by making an arbitrary assumption that may not be justified by the data. Important information might be lost for one of the major predictors of political opinion, which could lead to distortion [@obrien_1981_using]. To truly use the ordinal nature of a variable, we need to use its inherent unevenly spaced order to make a more underlying description of the data possible [@agresti_2010_analysis]. To fill this gap, I propose an ordered probit approach that estimates an ordinal variable's underlying latent continuous structure. I use this approach to develop two methods: The first method improves the use of ordinal variables in blocking in survey experiments. The second method provides a new way to treat missing survey data with ordinal variables. I subsequently apply both methods in an online survey experiment on political framing.

Chapter \ref{ordblock} outlines the blocking method. Survey experiments depend on balance to enable causal estimates. In order to identify potential causal effects, all treatment groups in a survey experiment need to be balanced on the potential outcomes. We get there by making all treatment groups look the same in terms of covariates. While this can be achieved with randomization, this typically leads to problems for small samples. Blocking, i.e. arranging participants in groups that are equal in terms of participants' covariates and using random allocation within these groups, can alleviate such worries. I use an ordered probit approach to estimate an assumed underlying latent continuous structure underneath ordinal variables, which results in a new set of data-driven variable categories that can then be used for blocking. This avoids arbitrary numerical conversion and fully utilizes the ordinal information provided in the unevenly spaced variable categories. I test this method by blocking external survey data, conducting variance tests, and running a placebo regression. Findings are mixed, with some results supporting the notion that the re-estimation of ordinal variable categories with an ordered probit approach matters and others negating it.

Chapter \ref{ordmiss} describes a new method to treat missing survey data with ordinal variables. Missing data are ubiquitous in survey research [@allison_2002_missing;@raghunathan_2016_missing]. This poses a big problem for researchers because data can typically not be analyzed with statistical software if they contain missing values [@little_2002_statistical;@molenberghs_2007_missing]. Scholars have developed several ways to treat missing data, among them multiple imputation, which accounts for and incorporates uncertainty around the estimated imputations through repeated draws [@andridge_2010_review; @graham_1999_performance; @schafer_2002_missing; @white_2011_multiple]. I develop a method to impute discrete missing data specifically for the specific circumstances of ordinal variables. This method is based on multiple hot deck imputation and uses the same ordered probit approach as in chapter \ref{ordblock}. I apply a scaled solution with newly estimated numerical thresholds from an assumed underlying latent continuous variable to measure the distances between the categories and calculate affinity scores. I test this method by imputing artificially inserted missing values in external survey data and comparing its performance to common imputation techniques. The results show that the method performs worse than existing techniques, despite expectations to the contrary. 

Chapter \ref{framing} applies both methods in an online survey experiment on political framing. Framing is the practice of presenting an issue to affect the way people see it [@chong_framing_2007]. It reorganizes existing information already present in people's minds and attempts to direct people's attention towards particular considerations [@druckman_2003_framing]. While numerous experiments have shown that frames can have substantial influence on people's opinions, we still don't know what aspects cause this influence. I provide an avenue of clarification by testing the influence of morality and self-interest in framing in direct juxtaposition. The findings show tentative evidence for the importance of morality in issue-opposing frames.









<!--chapter:end:01-introduction.Rmd-->

# BLOCKING IN SURVEY EXPERIMENTS WITH A NEW METHOD TO MEASURE ORDINAL VARIABLES {#ordblock}

## Introduction {#ordblock-intro}

<!--
```{r include=FALSE}
drop.zero <- function (df, digits = 4, p, s, pad.char = NA, ...){
    library(dplyr)
    ldots <- list(...)
    if (length(ldots) > 0) {
        if (!is.null(ldots[["prefix"]]) & missing(p)) 
            p <- ldots[["prefix"]]
        if (!is.null(ldots[["suffix"]]) & missing(s)) 
            s <- ldots[["suffix"]]
    }
    x <- dplyr::select_if(df, is.numeric)
    for(i in 1:ncol(x)){
      x[,i] <- round(as.numeric(x[,i]), digits)
      x[,i] <- sprintf(paste0("%.", digits, "f"), x[,i])
      x[,i] <- gsub("^0(?=\\.)|(?<=-)0", "", x[,i], perl = TRUE)
    }
    df[, colnames(x)] <- x
    return(df)
}

drop.zero.num <- function(num, digits = 3){
  if(!is.numeric(num)){
    num <- as.numeric(num)
  }
  num <- round(num, digits)
  num <- sprintf(paste0("%.", digits, "f"), num)
  num <- gsub("^0(?=\\.)|(?<=-)0", "", num, perl = TRUE)
  return(num)
}

```
-->

Survey experiments collect background information and attempt to uncover treatment effects on public opinion and political attitudes. In order to identify such potential effects, the treatment groups need to be balanced on the potential outcomes. This can be achieved through random assignment of participants to treatment groups. Randomization, i.e. flipping a fair coin to decide which treatment group a participant is assigned to, probabilistically results in balance based on the Law of Large Numbers [@urdan_statistics_2010]. For small samples, however, it can lead to serious imbalance. This can leave experimental results in statistically murky waters [@imai_quantitative_2018;@king_designing_1994;@fox_applied_2015]. In survey experiments, the overall sample size is often split across several treatment groups, which can exacerbate the problem. @chong_framing_2007, for instance, split 869 participants in a framing experiment on urban growth over 17 treatment groups, which leads to an average of just over 50 participants per group. Randomization is unlikely to lead to comparable treatment groups of this size. Researchers need to employ statistical methods to obtain balanced groups here. Blocking, i.e. arranging participants in groups that are equal in terms of participants' covariates and using random allocation within these groups, can alleviate such worries. 

Blocking depends on covariates. In political science, many covariates with high predictive power are categorical variables, i.e. variables where the data can be divided into groups (e.g. race). Others include interval (ordered and evenly spaced, e.g. SAT score) and ordinal (ordered and unevenly spaced, e.g. education) variables. Ordinal variables are ordered variables where the spacing between the values is not the same.

To block, these variables are often made numerical, e.g. by assigning the numbers 1-3 to the variable categories. This is acceptable for interval variables as the evenly spaced numbers correspond to the evenly spaced categories. For ordinal variables, however, this can be problematic. Take for instance the example of education: Each subsequent category has quantitatively more education than the previous, but the exact measure of the distances between the categories is unclear. An arbitrary evenly spaced string of numbers does not correspond to these unevenly spaced ordinal categories and may misrepresent the data. Do evenly spaced numbers really represent the distances between the categories? Perhaps the true spacing between some of the categories is so narrow they should not even be separate categories at all. 

I propose an ordered probit threshold approach to circumvent this problem: This approach estimates an assumed underlying latent continuous structure underneath ordinal variables whose data-driven categories can then be used for blocking. The following sections provide a background on survey experiments and blocking, describe the key aspects of ordinal variables, and outline my proposed ordered probit approach. I then demonstrate the effect of this approach with external survey data. The results are mixed. Some findings support the notion that the re-estimation of ordinal variable categories with an ordered probit approach matters, while others negate it. 



## Theory {#ordblock-theory}

### Preliminary Notations on Survey Experiments {#ordblock-theory-experiments}

The simplest of survey experiments has two potential outcomes for participants $i$, $y_{1i}$ and $y_{0i}$, with 1 denoting the treatment and 0 referring to the control. Consider a simplified version of a famous survey experiment by @tversky_framing_1981, where researchers want to test the effect of the mortality format on participants' choices.\label{death} They provide participants with the following scenario:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent Imagine that the US is preparing for the outbreak of an unusual Asian disease. A program to combat the disease has been proposed. Assume that the exact scientific estimates of the consequences of the program are as follows...
\end{adjustwidth}

Participants in the control group receive the program description in survival format:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent If the program is adopted, 200 out of 600 people will live.
\end{adjustwidth}

Participants in the treatment group receive the program description in mortality format:

\vspace{0.3cm}
\begin{adjustwidth}{50pt}{50pt}
\ssp
\noindent If the program is adopted, 400 out of 600 people will die.
\end{adjustwidth}

@tversky_framing_1981 use this experiment to demonstrate the importance of framing. Support for the program is much higher among respondents who received the survival format, even though the success rate of the program is identical in both formats. Framing the program in a positive light thus dramatically increases support by connecting the program to people's aversion of death and affirmation of life. While these findings stem from an experiment conducted in the 1970s, it is not a big leap to imagine a similar outcome in today's Covid-19 world. There is little reason to assume human behavior has changed to such a degree over the past decades that we would no longer be affected by framing today. If we conducted this experiment now, it would be quite possible to once more find sizable differences between these two groups.

Let $\bm{T}=0$ denote the control group and $\bm{T}=1$ denote the treatment group. After being shown one of the two groups, all participants are asked whether they support or oppose the program. The treatment effect for each individual participant $i$ is given by $y_{1i} - y_{0i}$. A comparison of the groups' average support reveals the Average Treatment Effect (ATE) across all participants, $\mathbf{E}[\delta] = \mathbf{E}[y_{1i} - y_{0i}]$. A central characteristic of such a comparison is the fundamental problem of causal inference [@holland_1986_statistics;@rubin_1974_estimating]: We are unable to observe both potential outcomes for the same participant at once. In our case, we cannot observe how much participant $i$ supports the program if given the survival format whilst also observing how much the same participant $i$ would have supported the program if given the mortality format. Since the true average treatment effect is unobservable, we need to use statistical means to assess the unobservable counterfactuals. If $\bm{Y}$ and $\bm{T}$ are independent, we can substitute $\mathbf{E}[\bm{Y_t}|\bm{T}=t]$ for $\mathbf{E}[\bm{Y_t}]$, thereby replacing an unobservable entity with something we can estimate from the data. If this independence is given, we can use the participants who received the mortality format (treatment) to estimate what would have happened to the participants who received the survival format (control) if the survival participants had received the mortality format. The potential outcome of the control group then mirrors what would have happened in the case of treatment, and vice versa. The process of randomization helps justify this independence and allows comparability. Another means to achieve comparability is the process of blocking. 


### Randomization {#ordblock-theory-randomization}

Randomization is equivalent to flipping a fair coin for each participant to be assigned to treatment or control. This chance procedure gives each participant the same probability of being assigned to either group (or groups, in case of multiple treatment groups) [@lachin_1988_properties]. Randomization increases covariate balance as the number of participants, $n$, increases [@imai_2009_essential]. The larger a researcher's sample, the better the resulting balance from randomization in expectation. Probabilistically, randomization enables the comparison of the average treatment effect to be unbiased, which allows the researcher to attribute any treatment effects to the treatment [@king_a-politically_2007]. 

While randomization thus guarantees balance as the sample size reaches infinity, it often does not do so in the naturally finite sample sizes researchers actually work with. With huge samples, the Law of Large Numbers requires that treatment groups selected through randomization will be balanced. With small samples, however, it is possible to get unlucky and end up with unbalanced groups [@imai_2008_misunderstandings]. Blocking can help achieve balance in such scenarios [@epstein_2002_rules].


### Blocking {#ordblock-theory-blocking}

Identical levels in terms of covariates across treatment groups represent the key aspect in experimental studies. In randomization, this is achieved by random chance. In blocking, this is achieved by combining covariate information about the participants with randomization. Specifically, participants are blocked into treatment groups that are similar to one another in terms of their covariates before treatment is assigned. Their similarity is estimated with the Mahalanobis or Euclidian distance. The Mahalanobis distance (MD) is a multivariate distance metric which measures the distance between two vectors (or between a point and a distribution). For two random vectors $x_i$ and $y_i$ with $i = [1,\ldots,n]$ of the same distribution, the MD is defined as

\begin{align}
MD_{xy} = \sqrt{(x_i - y_i)' S^{-1} (x_i - y_i)},
\end{align}

where $\bm{S}$ denotes the covariance matrix. If the covariance matrix is a diagonal identity matrix, the resulting distance measure becomes the Euclidian distance (ED), 

\begin{align}
ED_{xy} = \sqrt{\sum_{i=1}^N \frac{(x_i - y_i)^2}{s^2}},
\end{align}

with $s_i$ denoting the standard deviation of $x_i$ and $y_i$. The MD accounts for covariances, whereas the ED assumes equal variances and zero covariances. The ED can thus be argued to represent a special case of the MD.

Blocking is better suited to achieving balance in finite samples than randomization, as it "directly controls the estimation error due to differing levels of observed covariates in the treatment and control groups" [@moore_2012_multivariate, p. 463]. This is particularly relevant with small samples and a high number of treatment groups, as the overall number of participants needs to be divided up. Figure \ref{BoxLawLarNum} shows this visually. The following steps outline the estimation process behind the figure: 

\vspace{0.3cm}
\begin{adjustwidth*}{+0.5cm}{+0.5cm}
\begin{enumerate}
\item \noindent Randomly sample a discrete variable $v$ with levels 1 to 5.
\item Randomly assign the sample to a specified number of treatment groups. 
\item Block the sample separately into the same number of treatment groups. 
\item Take the mean of $v$ for each randomly assigned treatment group. 
\item Take the mean of $v$ for each blocked treatment group. 
\item Estimate the distances between the means of the randomized groups.
\item Estimate the distances between the means of the blocked groups.
\item Repeat steps 1 to 4 100 times.
\end{enumerate}
\end{adjustwidth*}
\vspace{0.3cm}

We follow these steps repeatedly for sample sizes up to 1,000 for 2, 3, 5, and 10 treatment groups. Figure \ref{BoxLawLarNum} shows the resulting distributions of maximum mean distances. Blocking outperforms randomization in every scenario. The difference between the two methods is smallest for large samples and a small number of treatment groups. 

```{r Law of Large Numbers Simulations, eval=FALSE, include=FALSE}

### THE CODE THAT CREATES THE SIMULATIONS FOR THE BLOCKING PLOTS IS IN scripts/lln//lln_testing.Rmd. IT TAKES 3-4 DAYS TO RUN THIS CODE, SO THERE IS NO POINT HAVING IT HERE ###

```

```{r Law of Large Numbers Plotting Code Boxplots, include=FALSE}

### I LOAD THE CREATED SIMULATIONS HERE TO CREATE THE PLOTS ###

all_blocked_2 <- read.csv("data/blocking/all_blocked_2.csv")
all_blocked_3 <- read.csv("data/blocking/all_blocked_3.csv")
all_blocked_5 <- read.csv("data/blocking/all_blocked_5.csv")
all_blocked_10 <- read.csv("data/blocking/all_blocked_10.csv")
all_means_variances_2 <- read.csv("data/blocking/all_means_variances_2.csv")
all_means_variances_3 <- read.csv("data/blocking/all_means_variances_3.csv")
all_means_variances_5 <- read.csv("data/blocking/all_means_variances_5.csv")
all_means_variances_10 <- read.csv("data/blocking/all_means_variances_10.csv")

# There are 100 NAs each in "all_means_variances_3", "all_means_variances_5", and "all_means_variances_10"
# They are always for the first respective sampled number: 9, 15, 30
# They're in control for _3 and _5, and in treatment5 for _10
# I don't know why those are happening, but I'm not starting with the first sampled numbers anyway, and it doesn't make any difference for the overall simulations, so I am removing those
all_means_variances_3 <- subset(all_means_variances_3, subset = (sampled_numbers != 9))
all_means_variances_5 <- subset(all_means_variances_5, subset = (sampled_numbers != 15))
all_means_variances_10 <- subset(all_means_variances_10, subset = (sampled_numbers != 30))

# I want the plot legend to read "randomized" instead of "rand"
all_means_variances_2$label <- fct_recode(all_means_variances_2$label, "randomized" = "rand")
all_means_variances_3$label <- fct_recode(all_means_variances_3$label, "randomized" = "rand")
all_means_variances_5$label <- fct_recode(all_means_variances_5$label, "randomized" = "rand")
all_means_variances_10$label <- fct_recode(all_means_variances_10$label, "randomized" = "rand")

together <- list(all_blocked_2, all_means_variances_2, all_blocked_3, all_means_variances_3, all_blocked_5, all_means_variances_5, all_blocked_10, all_means_variances_10) # collect all dfs in a list to loop over
couple <- list() # empty list

for(i in 1:(length(together))){
   couple[[i]] <- subset(together[[i]], select = c(sampled_numbers, diff, label))
   couple[[i]]$sampled_numbers <- as.factor(couple[[i]]$sampled_numbers)
} # subset for 3 columns and turn sampled_numbers into factor

sims_2 <- rbind(couple[[1]],couple[[2]]) # combine blocked and rand for each # of treatment groups
sims_3 <- rbind(couple[[3]],couple[[4]])
sims_5 <- rbind(couple[[5]],couple[[6]])
sims_10 <- rbind(couple[[7]],couple[[8]])

selection <- c(0, 0.1, 0.2, 0.3, 0.5, 1) # the quantiles I want

quantile(as.numeric(levels(sims_2$sampled_numbers)), selection) # quantiles for 2 groups
levels(sims_2$sampled_numbers) # levels for 2 groups
sims_2_range <- subset(sims_2, subset = sampled_numbers %in% c(14, 102, 206, 302, 502, 998)) # hand-select samples for range
sims_2_one <- subset(sims_2, subset = sampled_numbers == as.numeric(levels(sims_2$sampled_numbers)[2])) # select second level for 'intro' plot

quantile(as.numeric(levels(sims_3$sampled_numbers)), selection)
levels(sims_3$sampled_numbers)
sims_3_range <- subset(sims_3, subset = sampled_numbers %in% c(18, 108, 207, 306, 504, 999))
sims_3_one <- subset(sims_3, subset = sampled_numbers == as.numeric(levels(sims_3$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_5$sampled_numbers)), selection)
levels(sims_5$sampled_numbers)
sims_5_range <- subset(sims_5, subset = sampled_numbers %in% c(25, 115, 215, 305, 505, 995))
sims_5_one <- subset(sims_5, subset = sampled_numbers == as.numeric(levels(sims_5$sampled_numbers)[2]))

quantile(as.numeric(levels(sims_10$sampled_numbers)), selection)
levels(sims_10$sampled_numbers)
sims_10_range <- subset(sims_10, subset = sampled_numbers %in% c(40, 130, 220, 300, 500, 1000))
sims_10_one <- subset(sims_10, subset = sampled_numbers == as.numeric(levels(sims_10$sampled_numbers)[2]))

xlab <- "Sample Sizes"
ylab <- "Max. Distances Between Treatment Groups"

plot_first <- ggplot(sims_2_one, aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.15, 0.65)) # first plot outside of the loop because of the legend

sims_plots <- list(sims_2_range, sims_3_one, sims_3_range, sims_5_one, sims_5_range, sims_10_one, sims_10_range) # list of all subsets for plotting
plots <- list()
for(i in 1:(length(sims_plots))){
   plots[[i]]  <- ggplot(sims_plots[[i]], aes(x=sampled_numbers, y=diff)) + geom_boxplot(aes(fill=label)) + theme(axis.title=element_blank()) + guides(fill=FALSE)
  } # create plot for each data subset

```


```{r Boxplot-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distances Between Treatment Group Means in Randomized and Blocked Data. Increasing Sample Size for 2 (Top Row), 3 (Second Row), 5 (Third Row), and 10 Treatment Groups (Bottom Row). Leftmost Pair on the Right Panel Is the Same as the Pair on the Left Panel\\label{BoxLawLarNum}"}

grid.arrange(plot_first, plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]], plots[[7]], ncol = 2, nrow = 4, bottom=xlab, left=ylab)

```


For $n = 998$ and 2 treatment groups, the largest distance between randomized treatment groups is `r round(max(all_means_variances_2$diff[all_means_variances_2$sampled_numbers == 998]), digits=3)`, while the largest distance between blocked treatment groups is `r round(max(all_blocked_2$diff[all_blocked_2$sampled_numbers == 998]), digits=3)`. For small samples and a large number of treatment groups, the difference between the two methods increases. For $n = 40$ and 10 treatment groups, the largest distance between randomized treatment groups is `r round(max(all_means_variances_10$diff[all_means_variances_10$sampled_numbers == 40]), digits=3)`, while the largest distance between blocked treatment groups is `r round(max(all_blocked_10$diff[all_blocked_10$sampled_numbers == 40]), digits=3)`. 

Figure \ref{HistLawLarNum} shows the count distributions of these imbalances. For 2 treatment groups (top left plot), almost all blocked treatment groups have a maximum distance of zero. Most randomized groups also have a maximum distance of zero, but it is a narrow majority. Almost 50 percent of randomized groups have distances greater than zero, though still lower than one. As the number of treatment groups increases, so do the distances between the treatment groups. Nonetheless, the vast majority of blocked groups still show a maximum distance of zero. Even for 10 treatment groups, more than 60 percent of the blocked groups are not distant from each other at all. This does not hold true for the randomized groups. For 3 treatment groups, the majority of distances are now above zero. For 5 groups, the majority of distances exceed 0.25. For 10 treatment groups, more than 60 percent of groups show a distance larger than 0.5.


```{r Law of Large Numbers Plotting Code Histograms, include=FALSE}

ylab <- "Count"
xlab <- "Max. Distances Between Treatment Groups"

plot_first_more <- ggplot(sims_2, aes(x=diff, fill=label)) + geom_histogram(alpha=0.4, aes(y=..count..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + theme(legend.title=element_blank()) + theme(legend.position = c(0.6, 0.75)) + scale_fill_manual(values=c("red", "blue")) # first plot outside of the loop because of the legend

sims_plots_more <- list(sims_3, sims_5, sims_10) # list of all data sets (minus for 2 groups for plotting)

plots_more <- list()
for(i in 1:(length(sims_plots_more))){
   plots_more[[i]] <- ggplot(sims_plots_more[[i]], aes(x=diff, fill=label)) + geom_histogram(alpha=0.4, aes(y=..count..), position="identity", binwidth = 0.2) + theme(axis.title=element_blank()) + guides(fill=FALSE) + scale_fill_manual(values=c("red", "blue")) # create plot for each data set
}
```

```{r Hist-Law-Large-Numbers, echo=FALSE, fig.width=10, fig.height=4, fig.cap="Distribution of Treatment Group Differences in Randomized and Blocked Data for 2 (Top Left), 3 (Top Right), 5 (Bottom Left), and 10 Treatment Groups (Bottom Right). Overlapping Area Shown in Dark Purple.\\label{HistLawLarNum}"}

grid.arrange(plot_first_more, plots_more[[1]], plots_more[[2]], plots_more[[3]], ncol = 2, nrow = 2, bottom = xlab, left = ylab)

```


#### Blocking on the Go {#ordblock-theory-blocking-onthego}

Survey experiments can utilize blocking in two ways: with all data present and 'on the go'. The above data represents a simplified example of the former. All respondent covariate data have been collected before respondents are blocked into treatment groups, i.e. covariate data are known for all respondents before assignment to treatment. Blocking 'on the go' involves blocking respondents as they arrive for treatment at differing times. This is the case, for instance, for online survey experiments, where participants complete the survey at differing times, i.e. they 'trickle in' for treatment assignment as the experiment progresses. 'Traditional' blocking can not be used here, since it relies on covariate information about the entire sample, which is not available. Instead, we need to block sequentially, or 'on the go', as the experiment progresses. I use this method in the online survey experiment in chapter \ref{framing}.

Sequential blocking in political science is based on covariate-adaptive randomization, which varies probabilities based on knowledge about previous participants and the current participant [@chow_2007_adaptive]. Traditional covariate-adaptive approaches, such as the biased coin design [@efron_1971_forcing] and minimization [@pocock_1975_sequential], assign the incoming participant to the treatment group with the fewest participants with identical covariate information. For discrete covariates, for instance, this takes the form of assigning all participants except the first one, $q$, for covariate $c$ with value $d$ to a treatment group $g$ with probability

\begin{align}
\text{prob}\left(g* = g\right) = \left(1 - \frac{q_{cdg}}{\sum^G_{g = 1} q_{cdg}}\right) \left(\sum_{g = 1}^G\left(1 - \frac{q_{cdg}}{\sum^G_{g = 1} q_{cdg}}\right)\right)^{-1}.
\end{align}

This works for discrete covariates as the number of possible covariate levels is finite. For continuous covariates, the number of possible covariate levels rises exponentially, which results in rare identical participants as they are unlikely to look the same. Blocking on continuous covariates is not possible with these traditional approaches [@markaryan_2010_exact;@rosenberger_2002_randomization;@eisele_1995_biased]. @moore_blocking_2013 develop a method to do so by exploiting relationships between the current participant's covariate profile and those of all previously assigned participants. They define the similarity between participants with the MD between participants.

<!--
$q$ and $r$ with covariate vectors $\bm{x}_q$ and $\bm{x}_r$, 

\begin{align}
MD_{qr} = \sqrt{(\bm{x}_q - \bm{x}_r)' S^{-1} (\bm{x}_q - \bm{x}_r)}.
\end{align}

Recall that $S$ denotes the covariance matrix. 
-->

To aggregate pairwise similarity, they implement the mean, median, and trimmed mean of the pairwise MDs between the current participant and the participants in each treatment condition: Participants are indexed with treatment condition $t$ using $r \in \{1,...,R\}$. For each condition $t$, they estimate an average MD between the current participant, $q$, and the participants previously assigned. If the distance in terms of MD for the incoming participant is 2 in the control and 5 in the treatment condition, the incoming participant looks more similar to the control condition. To set the probability of assignment, @moore_blocking_2013 calculate the mean MDs for each incoming participant, $q$, for all treatment conditions, $t$, and sort the treatment conditions by these averages. Randomization is biased towards conditions with high scores. For each value of $k$ with $k \in \{2,3,...,6\}$, the condition with the highest average MD is then assigned a probability $k$ times larger than all other assignment probabilities. 

Blocking is thus possible when all covariate information is known at the time of assignment and when it 'trickles in' over time. Covariate information, however, is only one side of the coin. Researchers also need to take into consideration the characteristics of the variable to block on. Not all types of variables can and should be used the same way for blocking. Specifically, the current use of ordinal variables as blocking variables is somewhat problematic.



### Ordinal Variables {#ordblock-theory-ordinal}

Ordinal variables matter in surveys. One of the most important ordinal variables in political science surveys is education. It is widely established that education represents one of the major driving forces behind public opinion and political behavior, such as turnout or donations, in the U.S. [@dawood_campaign_2015; @fiorina_disconnect_2009; @leighley_who_2014; @abramowitz_disappearing_2010; @druckman_how_2013; @fiorina_culture_2011; @king_polarization_1997]. Ordinal variables are part of the larger framework of categorical variables. Categorical variables represent types of data which are commonly divided into three groups: nominal, interval, and ordinal variables. Nominal variables are categorical variables with two or more categories that are not intrinsically ordered. Examples include gender (Female, Male, Transgender etc.), race (African-American, White, Hispanic etc.), and party ID (Democrat, Republican, Independent) where the categories cannot be ordered sensibly into highest or lowest. Interval variables are ordered categorical variables with evenly spaced values. Examples include income (\$20,000, \$40,000, \$60,000, \$80,000 etc.), where the distance between \$20,000 and \$40,000 is the same as the distance between \$60,000 and \$80,000. Ordinal variables are ordered categorical variables where the spacing between values is not the same. Examples include education (Elementary School, Some High School, High School Graduate etc.) where the distance between "Elementary School" and "Some High School" is likely different than the distance between "Some High School" and "High School Graduate". Each subsequent category has quantitatively more education than the previous one, but the exact measure of the distances between the categories is unclear.  


For statistical analysis, the categories of nominal variables are often turned into binary variables. This manipulation does not impose any unnatural ordering onto the variable and thus does not require any theoretical assumptions. Interval variables are often made numerical, which is statistically sound. It makes sense to assign numerical values such as 1, 2, 3, and 4 to income categories of \$20,000, \$40,000, \$60,000, and \$80,000 as the distance between each of these categories is identical for any adjacent pair. This translates perfectly into the numerical values with identical distances, i.e. the distance between \$20,000 and \$40,000 is the same as the distance between 1 and 2. Ordinal variables are often treated as nominal variables by creating a new binary variable for one education category, for instance "High School Graduate". This approach ignores the ordered nature contained in ordinal variables and we would wrongly assume that the ordering of education categories is arbitrary. Ordinal variables are also often made numerical for analytic purposes. This is problematic because of their unevenly spaced categories. If the education categories "Elementary School", "Some High School", and "High School Graduate" were turned into the numerical values 1, 2, and 3, we would wrongly assume that the distances between the education categories correspond to these evenly spaced values. Do the numbers 1 to 3 really represent the distances between these categories? Perhaps the true spacing between some of the categories is so narrow they should not even be separate categories at all. We cannot answer this by making an arbitrary assumption that is not justified by the data. Alternatively, if "Elementary School", "Some High School", and "High School Graduate" were turned into three separate dummy variables, we would wrongly assume that there is no ordering to these values. In both cases, important information would be lost, which could lead to distortion [@obrien_1981_using]. Unfortunately, these two approaches are commonly applied to ordinal variables in the social sciences. @liddell_2018_analyzing for instance report that all articles published in 2016 in the Journal of Personality and Social Psychology (JPSP), Psychological Science (PS), and the Journal of Experimental Psychology that mention the term 'Likert' analyze ordinal data with a metric or binary model. Doing so can cause inversions of effects as well as Type I and Type II errors [@lalla_2017_fundamental;@torra_2006_regression]. To truly use the ordinal nature of a variable, we need to use both its quantitative and its inherent unevenly spaced ordered aspects to make a more underlying description of the data possible [@agresti_2010_analysis]. To fill this gap, I propose an ordered probit model that estimates the latent continuous structure underlying ordinal variables.



### Ordered Probit Approach {#ordblock-theory-op}

Many approaches in the literature on the analysis of ordinal variables incorporate the distribution of the variable categories [@agresti_1996_introduction]. The most promising suggestions focus on natural extensions of probit and logit models [@winship_1984_regression] by assigning scores to be estimated from the data [@agresti_1990_categorical] and quantifying each non-quantitative variable according to the empirical distributions of the variable, assuming the presence of a continuous underlying variable for each ordinal indicator [@lucadamoa_2014_scaling]. In fact, @agresti_2010_analysis states "that the type of ordinal method used is not that crucial" but that the "results may be quite different, however, from those obtained using methods that treat all the variables as nominal" (p. 3). The same applies to methods which treat ordinal variables as interval [@gertheiss_2008_penalized]. This suggests that a probit or logit model is suitable to uncover the latent continuous variable underlying an ordinal variable, thereby using the ordinal information provided and respecting uneven distances. In the literature, this approach is focused exclusively on the analysis of ordinal variables as a response variable. I propose an ordered probit model that applies to ordinal variables as predictors. The workflow of the model is shown in Figure \ref{op-workflow}.

\vspace{0.2cm}

\ssp

\begin{figure}
\centering
\begin{tikzpicture}
    \node [block2rw] (model) {\scriptsize{Linear Model: Ordinal Variable $\sim$ Predictors}};
    \node [block2rw, below =1cm of model] (train) {\scriptsize{Train model on data}};
    \node [block2rw, below =1cm of train] (cutoff) {\scriptsize{Estimate cutoff thresholds between categories}};
    \node [block2rw, below =1cm of cutoff] (bin) {\scriptsize{Bin cases according to linear predictors}};
    \node [block2rw, below =1cm of bin] (cats) {\scriptsize{Use re-estimated categories}};
	\coordinate[below=0cm of model] (modelc);
	\coordinate[above=0cm of train] (trainac);
	\coordinate[below=0cm of train] (trainbc);
	\coordinate[above=0cm of cutoff] (cutoffac);
	\coordinate[below=0cm of cutoff] (cutoffbc);
	\coordinate[above=0cm of bin] (binac);
	\coordinate[below=0cm of bin] (binbc);
	\coordinate[above=0cm of cats] (catsc);
	\path [line] (modelc) -- (trainac);
	\path [line] (trainbc) -- (cutoffac);
	\path [line] (cutoffbc) -- (binac);
	\path [line] (binbc) -- (catsc);
\end{tikzpicture}
\caption{Ordered Probit Workflow} 
\label{op-workflow}
\end{figure}

\dsp

\vspace{-0.4cm}

We need to estimate a linear combination of meaningful covariates as predictors and an ordinal variable as the dependent variable. We then train this model on externally and internally valid data to estimate cutoff thresholds between the ordinal categories and bin data cases according to the linear predictors. The binned cases determine which variable categories make sense, given the underlying latent continuous variable. We then replace the original categories with these re-estimated categories and conduct the statistical analysis of interest. 

Notationally, let there be $\bm{X}$, an $n \times k$ matrix of explanatory variables. Let further $\bm{Y}$ be observed on the ordered categories $\bm{Y}_i \in [1,\ldots,k]$, for $i=1,\ldots n$, and let $\bm{Y}$ be assumed to be produced by the unobserved latent continuous variable $\bm{Y^{cont}}$. $\bm{Y^{cont}}$ is continuous on $\mathfrak{R}$ from $-\infty$ to $\infty$. The 'response mechanism' for the $r^{th}$ category is $Y=r \Longleftrightarrow \xi_{r-1} < Y^{cont} < \xi_r$. This requires there to be thresholds on $R$:
$Y^{cont}_i: \; \xi_0 \underset{a=1}{\longleftarrow\!\longrightarrow} \xi_1 \underset{a=2}{\longleftarrow\!\longrightarrow} \xi_2 \underset{a=3}{\longleftarrow\!\longrightarrow} \xi_3\ldots \xi_{A-1} \underset{a=A}{\longleftarrow\!\longrightarrow} \xi_A$. The vector of (unseen) utilities across individuals in the sample, $\bm{Y^{cont}}$, is determined by a linear model of explanatory variables, $Y^{cont} = X \beta + \mu$, where $\bm{\beta} =[\beta_1,\beta_2,\ldots,\beta_p]$ does not depend on the $\xi_j$ and $\mu \sim F_{\mu}$. For the observed vector $\bm{Y}$,

\begin{align}
p(Y \leq r|X) &= p(Y^{cont} \leq \xi_r) = p(X \beta + \mu \leq \xi_r) \nonumber\\
&= p(\mu \leq \xi_r+ X \beta) = F_{\mu}(\xi_r + X \beta)
\end{align}
            
is called the cumulative model because $p(Y \leq \xi_r|X) = p(Y=1|X) + p(Y=2|X) + \ldots + p(Y=r|X)$. A logistic distributional assumption on the errors produces the ordered logit specification

\begin{align}
F_{\mu}(\xi_r - X' \beta) = P(Y \leq r|X) = [1+\exp(-\xi_r-X' \beta)]^{-1}. 
\end{align}

The likelihood function is 

\begin{align}
L(\beta, \xi|X,Y) = \prod_{i=1}^{n}\prod_{j=1}^{A-1}\left[\Lambda(\xi_j + X_i' \beta) - \Lambda(\xi_{j-1} + X_i' \beta) \right]^{z_{ij}}
\end{align}

with $\bm{\Lambda}$ denoting the logistic distribution and where $z_{ij}=1$ if the $i^\text{th}$ case is in the $j^\text{th}$ category and $z_{ij}=0$ otherwise. The thresholds on $\mathfrak{R}$ partition the variable into regions corresponding to the ordinal categories. The linear model, $\bm{Y^{cont}}$, bins the observations between these thresholds according to the linear predictors. In `R`, the ordered probit model can be implemented with the `polr` function from the `MASS` package [@ripley_2020_package].






## Data {#ordblock-data}

I use the American National Election Studies (ANES) to implement the proposed ordered probit method. The ANES are the oldest continuous collection of national surveys on electoral behavior and attitudes in the US and are conducted before and after every US presidential and Congressional election by the University of Michigan and Stanford University. Data have been collected since 1948 in the attempt to understand voter behavior and candidate choice, among many others. The list of questions has been continually expanded and refined over the years. All ANES data are publicly available and the ANES are frequently used for high-profile publications [see for instance @jackman_2018_does; @leighley_who_2014]. I apply the proposed ordered probit method to the 2016 ANES data and train a specified regression model on these data. This estimates the thresholds between each existing ANES education category and bins all observations according to the linear predictors to determine the education categories that make sense, based on the underlying latent continuous variable. This results in two sets of education categories: ANES and ordered probit (OP). The data are then blocked into three treatment groups to simulate a survey experiment environment. This process is conducted twice: once with the ANES categories, and once with the OP categories. To simulate sequential blocking, the order of observations is assumed to represent the sequential order of arrival for treatment. The estimation of group means and variances together with statistical tests reveals the suitability of this approach. Finally, I block the ANES data once more into two treatment groups and conduct a placebo regression to test model fitness further.




## Results {#ordblock-results}

Recall that we need to estimate a linear combination of meaningful covariates as predictors and an ordinal variable as the dependent variable. This model needs to be trained on externally and internally valid data. The ANES data have been shown to fulfill these criteria [@krupnikov_2014_cross-sample;@malhotra_2007_effect]. I train the following model on these data, using standard demographics in political science as predictors for all observations $i$ with $i = [1,\ldots,n]$: 

\begin{align}
\text{Education}_i \sim & \,\beta_0 + \beta_1 \text{Gender}_i + \beta_2 \text{Race}_i + \beta_3 \text{Age}_i + \beta_4 \text{Income}_i \,+ \nonumber \\
& \beta_5 \text{Occupation}_i + \beta_6 \text{Party ID}_i + \epsilon_i
\end{align}

This estimates cutoff thresholds between the categories and bins data cases according to the linear predictors. The cutoff coefficients between each of the education categories are shown in Table \ref{education-categories}.


```{r Education Thresholds Table, include=FALSE}
op.model.thresholds <- read.csv("data/blocking/thresholds.csv")
colnames(op.model.thresholds) <- c("Thresholds", "Coefficients", "Standard Errors", "t-values")
stargazer(op.model.thresholds, 
          summary = FALSE, 
          rownames = FALSE, 
          header=FALSE, 
          align = TRUE, 
          title = "Ordered Probit Threshold Estimates", 
          label = "education-categories")
```

\begin{table}[!htbp] \centering 
  \caption{Ordered Probit Threshold Estimates} 
  \label{education-categories} 
\begin{tabular}{r@{}lr@{}lr@{}lr@{}l} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{2}{c}{Thresholds} & \multicolumn{2}{c}{Coefficients} & \multicolumn{2}{c}{Standard Errors} & \multicolumn{2}{c}{t-values} \\ 
\hline \\[-1.8ex] 
Up to 1st $\mid$ & \,1st-4th & -7.&869 & 1.&024 & -7.&681 \\ 
1st-4th $\mid$ & \,5th-6th & -7.&146 & 0.&717 & -9.&965 \\ 
5th-6th $\mid$ & \,7th-8th & -5.&379 & 0.&326 & -16.&515 \\ 
7th-8th $\mid$ & \,9th & -4.&671 & 0.&253 & -18.&472 \\ 
9th $\mid$ & \,10th & -3.&920 & 0.&206 & -19.&070 \\ 
10th $\mid$ & \,11th & -3.&468 & 0.&188 & -18.&489 \\ 
11th $\mid$ & \,12th & -2.&984 & 0.&174 & -17.&100 \\ 
12th $\mid$ & \,HS grad & -2.&511 & 0.&166 & -15.&116 \\ 
HS grad $\mid$ & \,Some college & -0.&711 & 0.&154 & -4.&607 \\ 
Some college $\mid$ & \,Associate & 0.&384 & 0.&154 & 2.&500 \\ 
Associate $\mid$ & \,Bachelor's & 1.&045 & 0.&154 & 6.&766 \\ 
Bachelor's $\mid$ & \,Master's & 2.&478 & 0.&160 & 15.&538 \\ 
Master's $\mid$ & \,Professional & 4.&099 & 0.&177 & 23.&144 \\ 
Professional $\mid$ & \,Doctorate & 4.&838 & 0.&197 & 24.&589 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}


The ordered probit model uses the ordinal information with unevenly spaced distances provided and bins observations according to the estimated threshold coefficients, which in turn determines what education categories make sense given the underlying latent continuous variable. Figure \ref{DensEducLP} shows the distribution of this variable. As we can see, no cases fall lower than "12th|HS grad" or higher than "Master's|Professional", which results in a new set of model-estimated OP education categories that fit the data. We thus have two sets of education categories: the original ANES categories, and the model-estimated OP categories.


```{r Education Categories Linear Predictor Plotting Code, include=FALSE}

ord.list <- readRDS("data/anes/ord_list.rds")
lp.values <- data.frame(ord.list$plr.out$lp)
cut.points <- ord.list$int.df[, c(1,2)] %>% 
  filter(., Values >= min(lp.values) & Values < max(lp.values))
cut.points.plot <- rbind(ord.list$int.df[8, c(1,2)], cut.points, ord.list$int.df[13, c(1,2)])

```

```{r Density-Education-Categories-Linear-Predictors, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Education Linear Predictor Data Cases. Vertical Lines Indicate Threshold Coefficients. No Cases Fall Lower than '12th|HS grad' or Higher than 'Master's|Professional'.\\label{DensEducLP}"}

ggplot(lp.values, aes(x=ord.list.plr.out.lp)) + 
  geom_density() +
  xlim(c(plyr::round_any(cut.points.plot[1,2],1), 
         plyr::round_any(cut.points.plot[6,2],.5, f = ceiling))) + 
  geom_vline(xintercept = cut.points.plot$Values, linetype="solid", color = "red") + 
  annotate(geom = "text", x = cut.points.plot$Values, y = 0.2, label = cut.points.plot$Intercepts, 
           vjust = -1, color = "red", size = 3, angle = 90) + 
  xlab("Linear Predictor Data Cases") + ylab("Density")

```

```{r Education Categories Plotting Code, include=FALSE}

op.model.data <- readRDS("data/anes/anes_education.rds")
plot.orig <- ggplot(op.model.data, aes(x = education)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = "darkred") + theme(axis.title=element_blank(), axis.text.x = element_text(angle = 45))
plot.new <- ggplot(op.model.data, aes(x = education.new)) + geom_bar(aes(y = (..count..)/sum(..count..)), fill = "darkblue") + theme(axis.title=element_blank(), axis.text.x = element_text(angle = 45))

```

```{r Barplot-Education-Categories, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Education Categories. Original ANES Categories on the Left, Ordered Probit Estimated Categories on the Right. Categories Below 'High School Graduate' and Above 'Master's' Are Gone. OP Categories Can Now Be Used For Blocking.\\label{BarEducCat}"}

grid.arrange(plot.orig, plot.new, ncol = 2, nrow = 1, bottom = "Education", left = "Percentages")

```


Figure \ref{BarEducCat} shows the distributions of both sets. All categories 'below' "High School Graduate" and 'above' "Master's" are gone in the OP set because they do not fit the data. We can now use these estimated education categories as the basis for blocking. Assigning numerical values to the new categories is now justifiable because they are based on data-driven estimations. This allows us to block on numerical values with the MD, which would not be permitted on theoretical grounds without empirical justification. The following sections demonstrate how the newly estimated categories affect blocking and regression results. 




### Blocking Differences {#ordblock-data-blockdiff}


```{r Block-ANES-OP-Categories, include=FALSE}

#### This block is only needed for list.dfs, which I need here. Everything else is used for stuff in the appendix ####

df.an <- readRDS("data/blocking/df_an.RDS")
df.op <- readRDS("data/blocking/df_op.RDS")

for(i in 1:(df.an$education %>% unique %>% length)){
  df.an[df.an$education == i, "education"] <- op.model.data$education %>% levels %>% .[i]
}

df.an$education <- factor(df.an$education, levels = op.model.data$education %>% levels)

for(i in 1:(df.op$education %>% unique %>% length)){
  df.op[df.op$education == i, "education"] <- op.model.data$education.new %>% levels %>% .[i]
}

df.op$education <- factor(df.op$education, levels = op.model.data$education.new %>% levels)
assigned.an <- readRDS("data/blocking/assigned_an.RDS")
assigned.op <- readRDS("data/blocking/assigned_op.RDS")

an.t1 <- df.an[df.an$id %in% assigned.an$assg[[1]][,1],] %>% subset(., select=-c(id, education))
an.t2 <- df.an[df.an$id %in% assigned.an$assg[[1]][,2],] %>% subset(., select=-c(id, education))
an.t3 <- df.an[df.an$id %in% assigned.an$assg[[1]][,3],] %>% subset(., select=-c(id, education))
# an.t4 <- df.an[df.an$id %in% assigned.an$assg[[1]][,4],] %>% subset(., select=-c(id, education))
# an.t5 <- df.an[df.an$id %in% assigned.an$assg[[1]][,5],] %>% subset(., select=-c(id, education))

op.t1 <- df.op[df.op$id %in% assigned.op$assg[[1]][,1],] %>% subset(., select=-c(id, education))
op.t2 <- df.op[df.op$id %in% assigned.op$assg[[1]][,2],] %>% subset(., select=-c(id, education))
op.t3 <- df.op[df.op$id %in% assigned.op$assg[[1]][,3],] %>% subset(., select=-c(id, education))
# op.t4 <- df.op[df.op$id %in% assigned.op$assg[[1]][,4],] %>% subset(., select=-c(id, education))
# op.t5 <- df.op[df.op$id %in% assigned.op$assg[[1]][,5],] %>% subset(., select=-c(id, education))


### props of factor columns, except education ###

list.props <- list()
list.dfs <- list(an.t1, an.t2, an.t3,
                 op.t1, op.t2, op.t3)
# list.dfs <- list(an.t1, an.t2, an.t3, an.t4, an.t5,
#                  op.t1, op.t2, op.t3, op.t4, op.t5)
n.tr <- 3
# n.tr <- 5
an <- c()
op <- c()

for(w in 1:n.tr){
  an[w] <- paste0("ANES", w)
  op[w] <- paste0("OP", w)
}

names(list.dfs) <- c(an, op)
is.not.num <- function(x) !is.numeric(x)
tmp <- dplyr::select_if(an.t1, is.not.num)

for(x in 1:ncol(tmp)){
  list.props[[x]] <- data.frame(matrix(NA, levels(tmp[, x]) %>% length, length(list.dfs)))
  rownames(list.props[[x]]) <- levels(tmp[,x])
  colnames(list.props[[x]]) <- names(list.dfs)
}

names(list.props) <- colnames(tmp)

for(i in 1:ncol(tmp)){
  for(y in 1:length(list.dfs)){
    tmp.fac <- dplyr::select_if(list.dfs[[y]], is.not.num)
    list.props[[i]][,y] <- prop.table(table(tmp.fac[,i]))
  }
}

df.props <- plyr::ldply(list.props, data.frame, .id = NULL) 
lev.names <- sapply(tmp, levels) %>% unlist
lev.names["pid4"] <- "Something else"
rownames(df.props) <- lev.names
df.props <- round(df.props, digits = 3) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3")]
# df.props <- drop.zero(df.props, digits = 3) %>%
#   .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]


### means of numeric columns ###

list.means <- list()
tmp2 <- dplyr::select_if(op.t1, is.numeric)

for(x in 1:ncol(tmp2)){
  list.means[[x]] <- data.frame(matrix(NA, 1, length(list.dfs)))
  colnames(list.means[[x]]) <- names(list.dfs)
}

names(list.means) <- colnames(tmp2)

for(i in 1:ncol(tmp2)){
  for(y in 1:length(list.dfs)){
    tmp.means <- dplyr::select_if(list.dfs[[y]], is.numeric)
    list.means[[i]][,y] <- mean(tmp.means[,i]) %>% round(., digits = 3)
  }
}

df.means <- plyr::ldply(list.means, data.frame, .id = NULL) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3")]
# df.means <- plyr::ldply(list.means, data.frame, .id = NULL) %>%
#   .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]

rownames(df.means) <- colnames(tmp2) %>%
  gsub("\\.", " ", .) %>%
  tools::toTitleCase(.)


### now education ###

an.ed.t1 <- df.an[df.an$id %in% assigned.an$assg[[1]][,1],] %>% subset(., select=c(education))
an.ed.t2 <- df.an[df.an$id %in% assigned.an$assg[[1]][,2],] %>% subset(., select=c(education))
an.ed.t3 <- df.an[df.an$id %in% assigned.an$assg[[1]][,3],] %>% subset(., select=c(education))
# an.ed.t4 <- df.an[df.an$id %in% assigned.an$assg[[1]][,4],] %>% subset(., select=c(education))
# an.ed.t5 <- df.an[df.an$id %in% assigned.an$assg[[1]][,5],] %>% subset(., select=c(education))

list.eds.an <- list(an.ed.t1, an.ed.t2, an.ed.t3)
# list.eds.an <- list(an.ed.t1, an.ed.t2, an.ed.t3, an.ed.t4, an.ed.t5)

df.ed.an <- data.frame(matrix(NA, levels(an.ed.t1[,1]) %>% length, length(list.eds.an)))
rownames(df.ed.an) <- levels(an.ed.t1[,1])
colnames(df.ed.an) <- an

for(i in 1:ncol(an.ed.t1)){
  for(y in 1:length(list.eds.an)){
    df.ed.an[,y] <- prop.table(table(list.eds.an[[y]][,i]))
  }
}

df.ed.an <- round(df.ed.an, digits = 3)

op.ed.t1 <- df.op[df.op$id %in% assigned.op$assg[[1]][,1],] %>% subset(., select=c(education))
op.ed.t2 <- df.op[df.op$id %in% assigned.op$assg[[1]][,2],] %>% subset(., select=c(education))
op.ed.t3 <- df.op[df.op$id %in% assigned.op$assg[[1]][,3],] %>% subset(., select=c(education))
# op.ed.t4 <- df.op[df.op$id %in% assigned.op$assg[[1]][,4],] %>% subset(., select=c(education))
# op.ed.t5 <- df.op[df.op$id %in% assigned.op$assg[[1]][,5],] %>% subset(., select=c(education))

list.eds.op <- list(op.ed.t1, op.ed.t2, op.ed.t3)
# list.eds.op <- list(op.ed.t1, op.ed.t2, op.ed.t3, op.ed.t4, op.ed.t5)

df.ed.op <- data.frame(matrix(NA, levels(op.ed.t1[,1]) %>% length, length(list.eds.op)))
rownames(df.ed.op) <- levels(op.ed.t1[,1])
colnames(df.ed.op) <- op

for(i in 1:ncol(op.ed.t1)){
  for(y in 1:length(list.eds.op)){
    df.ed.op[,y] <- prop.table(table(list.eds.op[[y]][,i]))
  }
}

df.ed.op <- round(df.ed.op, digits = 3)
top <- data.frame(matrix(NA, 8, length(list.eds.op)))
colnames(top) <- colnames(df.ed.op)
rownames(top) <- rownames(df.ed.an)[1:8]
bottom <- data.frame(matrix(NA, 2, length(list.eds.op)))
colnames(bottom) <- colnames(top)
rownames(bottom) <- rownames(df.ed.an)[14:15]
df.ed.all <-rbind(top, df.ed.op, bottom) %>% cbind(df.ed.an, .) %>%
  .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3")]
# df.ed.all <-rbind(top, df.ed.op, bottom) %>% cbind(df.ed.an, .) %>%
#   .[c("ANES1", "OP1", "ANES2", "OP2", "ANES3", "OP3", "ANES4", "OP4", "ANES5", "OP5")]

### combine and print (and manually adjust) factors, numerics, education ###

df.all <- rbind(df.props, df.means) # I decided not to include ed.all because I don't want to have to explain the differing numbers (which might well be meaningless anyway)
rownames(df.all)[17] <- "Employed" # sounds better than "Working"

```

```{r Variance-In-Treatment-Groups, include=FALSE}

##### compare treatment groups in AN with treatment groups in OP #####

list.dfs.var <- list.dfs # so I don't mess with the output from above

# add a treatment group column (ANES 1, OP 1 etc.) to each data frame
# result is a list of length 6
# each cat only has one unique value (e.g. ANES 1)
for(x in 1:(length(list.dfs)/2)){
  list.dfs.var[[x]]$cat <- rep(paste0("ANES", x), nrow(list.dfs.var[[x]])) %>% as.factor
  list.dfs.var[[x+3]]$cat <- rep(paste0("OP", x), nrow(list.dfs.var[[x+3]])) %>% as.factor
}

list.dfs.t <- list()

# combine ANES and OP data for each treatment group
# result is a list of length 3
# cat now has two unique values respective of treatment group (e.g. OP 1, ANES 1)
for(x in 1:(length(list.dfs)/2)){
  list.dfs.t[[x]] <- c(list.dfs.var[x], list.dfs.var[x+3]) %>%
    ldply(., data.frame) %>% subset(., select =-c(.id))
}

num.res <- rep(list(list()), 2)
names(num.res) <- c("aov.sum", "aov.tukey")
list.dfs.num.aov <- rep(list(num.res), 3)
names(list.dfs.num.aov) <- c("T1", "T2", "T3")

for(t in 1:length(list.dfs.num.aov)){
  for(w in 1:ncol(dplyr::select_if(list.dfs.t[[1]], is.numeric))){
    aa <- dplyr::select_if(list.dfs.t[[t]], is.numeric) # select only numeric columns
    bb <- cbind(aa, list.dfs.t[[t]]$cat) # combine numeric columns with cat column
    colnames(bb) <- c(colnames(aa), "cat")
    aov.out <- aov(bb[,w] ~ bb$cat) # aov regression of each numeric column on cat
    list.dfs.num.aov[[t]]$aov.sum[[w]] <- summary(aov.out)
    names(list.dfs.num.aov[[t]]$aov.sum)[[w]] <- colnames(bb)[w]
    list.dfs.num.aov[[t]]$aov.tukey[[w]] <- TukeyHSD(aov.out)
    names(list.dfs.num.aov[[t]]$aov.tukey)[[w]] <- colnames(bb)[w]
    # num.aov.sum[[w]] <- summary(aov.out)
    # names(num.aov.sum)[[w]] <- paste0(colnames(bb)[w], ".sum")
    # num.aov.tukey[[w]] <- TukeyHSD(aov.out)
    # names(num.aov.tukey)[[w]] <- paste0(colnames(bb)[w], ".tukey")
  }
}

fac.res <- rep(list(list()), 2)
names(fac.res) <- c("glm.sum", "glm.anov")
list.dfs.fac.aov <- rep(list(fac.res), 3)
treats <- c("T1", "T2", "T3")
names(list.dfs.fac.aov) <- treats

for(t in 1:length(list.dfs.num.aov)){
  for(w in 1:(ncol(dplyr::select_if(list.dfs.t[[1]], is.not.num))-1)){
    my.dat <- dplyr::select_if(list.dfs.t[[t]], is.not.num)
    my.mod <- glm(my.dat[,w] ~ my.dat$cat, family = "binomial")
    list.dfs.fac.aov[[t]]$glm.sum[[w]] <- summary(my.mod)
    names(list.dfs.fac.aov[[t]]$glm.sum)[[w]] <- colnames(my.dat)[w]
    list.dfs.fac.aov[[t]]$glm.anov[[w]] <- anova(my.mod, test = "Chisq")
    names(list.dfs.fac.aov[[t]]$glm.anov)[[w]] <- colnames(my.dat)[w]
  }
}

# These commented out lines are just for me as an overview how all the lists are structured and how to get the numbers as a df
# Lists of length 3, named T1 to T3
# list.dfs.num.aov %>% length
# list.dfs.num.aov %>% names
# list.dfs.fac.aov %>% length
# list.dfs.fac.aov %>% names
# 
# Lists of length 2, named aov.sum, aov.tukey and glm.sum, glm.anov
# list.dfs.num.aov[["T1"]] %>% length
# list.dfs.num.aov[["T1"]] %>% names
# list.dfs.fac.aov[["T1"]] %>% length
# list.dfs.fac.aov[["T1"]] %>% names
# 
# Lists of lengths 2 and 8, named after variables (age, feel.trump and gender, race, income, occupation, pid, pres.approv, min.wage, country.track)
# list.dfs.num.aov[["T1"]][["aov.sum"]] %>% length
# list.dfs.num.aov[["T1"]][["aov.sum"]] %>% names
# list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% length
# list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% names
# 
# Classes and names of each output
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]] %>% class # summary.aov, listof
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]] %>% names # NULL
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]] %>% class # summary.glm
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]] %>% names # lots of useful names
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]] %>% class # TukeyHSD, multicomp
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]] %>% names # bb$cat
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]] %>% class # anova, data.frame
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]] %>% names # lots of names
#
# How to transform each output into a data frame
# list.dfs.num.aov[["T1"]][["aov.sum"]][["age"]][[1]] %>% data.frame
# list.dfs.fac.aov[["T1"]][["glm.sum"]][["gender"]]$coefficients %>% data.frame
# list.dfs.num.aov[["T1"]][["aov.tukey"]][["age"]]$`bb$cat` %>% data.frame
# list.dfs.fac.aov[["T1"]][["glm.anov"]][["gender"]][2,] %>% data.frame


# Store the output of each method for all treatment groups, so 4 data frames where each df contains all groups
aov.names <- list.dfs.num.aov[["T1"]][["aov.sum"]] %>% names # could have also used aov.tukey here
aov.length <- list.dfs.num.aov[["T1"]][["aov.sum"]] %>% length
glm.names <- list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% names # could have also used glm.anov here
glm.length <- list.dfs.fac.aov[["T1"]][["glm.sum"]] %>% length # could have also used glm.anov here

list.aov.sum <- list.aov.tukey <- rep(list(list()), length(aov.names))
list.glm.sum <- list.glm.anov <- rep(list(list()), length(glm.names))

for(i in 1:length(treats)){
  tmp.aov.sum <- list.dfs.num.aov[[i]][["aov.sum"]]
  tmp.aov.tukey <- list.dfs.num.aov[[i]][["aov.tukey"]]
  for(x in 1:aov.length){ # this loop does tmp.aov.sum and tmp.aov.tukey
    tmp.sum <- tmp.aov.sum[[names(tmp.aov.sum)[[x]]]][[1]] %>% data.frame
    rownames(tmp.sum) <- c(paste0(names(tmp.aov.sum)[[x]], treats[i]),
                           paste0(names(tmp.aov.sum)[[x]], treats[i], "Resid"))
    list.aov.sum[[x]][[i]] <- tmp.sum
    tmp.tukey <- tmp.aov.tukey[[names(tmp.aov.tukey)[[x]]]]$`bb$cat` %>% data.frame
    rownames(tmp.tukey) <- paste0(names(tmp.aov.tukey)[[x]], treats[i], rownames(tmp.tukey))
    list.aov.tukey[[x]][[i]] <- tmp.tukey
  }
  tmp.glm.sum <- list.dfs.fac.aov[[i]][["glm.sum"]]
  tmp.glm.anov <- list.dfs.fac.aov[[i]][["glm.anov"]]
  for(y in 1:glm.length){ # this loop does tmp.glm.sum and tmp.glm.anov
    tmp.sum <- tmp.glm.sum[[names(tmp.glm.sum)[[y]]]]$coefficients %>% data.frame
    rownames(tmp.sum) <- c(paste0(names(tmp.glm.sum)[[y]], treats[i], "Int"),
                           paste0(names(tmp.glm.sum)[[y]], treats[i]))
    list.glm.sum[[y]][[i]] <- tmp.sum
    tmp.anov <- tmp.glm.anov[[names(tmp.glm.anov)[[y]]]][2,] %>% data.frame
    rownames(tmp.anov) <- paste0(names(tmp.glm.anov)[[y]], treats[i])
    list.glm.anov[[y]][[i]] <- tmp.anov
  }
}

df.aov.sum <- unlist(list.aov.sum, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.aov.sum <- cbind(df.aov.sum[, 1:3] , round(df.aov.sum[, 4:5], digits = 3))
df.aov.tukey <- unlist(list.aov.tukey, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.glm.sum <- unlist(list.glm.sum, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.glm.anov <- unlist(list.glm.anov, recursive = FALSE) %>% do.call("rbind", .) %>% round(., digits = 3)
df.glm.anov <- cbind(df.glm.anov[, c(1,3,4)], round(df.glm.anov[, c(2, 5)], digits = 3)) %>% .[,c(1,4,2,3,5)]

colnames(df.aov.sum) <- c("Df", "Sum.Sq", "Mean.Sq", "F-value", "p-value")
colnames(df.aov.tukey) <- c("Diff.Means", "CI Lower", "CI Upper", "Adj. p-value")
colnames(df.glm.sum) <- c("Estimate", "Std.Error", "z-value", "p-value")
colnames(df.glm.anov) <- c("Df", "Deviance", "Resid.Df", "Residual Deviance", "Pr.Chi")

# this one is below
tab.aov.sum <- stargazer(df.aov.sum, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Summary of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "aov-sum")
at <- gsub("{c}", "{l}", tab.aov.sum, fixed = TRUE) %>%
  gsub("age", "", ., fixed = TRUE) %>%
  gsub("feel.trump", "", ., fixed = TRUE) %>%
  gsub("Resid", " Residuals", ., fixed = TRUE)
cat(at)

# this one is commented out below
tab.aov.tukey <- stargazer(df.aov.tukey, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Tukey's 'Honest Significant Difference' Test of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "aov-tukey")
bt <- gsub("{c}", "{l}", tab.aov.tukey, fixed = TRUE) %>%
  gsub("age", "", ., fixed = TRUE) %>%
  gsub("feel.trump", "", ., fixed = TRUE) %>%
  gsub("OP", " OP", ., fixed = TRUE)
cat(bt)

# this one is below
tab.glm.sum <- stargazer(df.glm.sum, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Summary of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "glm-sum")
ct <- gsub("{c}", "{l}", tab.glm.sum, fixed = TRUE) %>%
  gsub("gender", "", ., fixed = TRUE) %>%
  gsub("race", "", ., fixed = TRUE) %>%
  gsub("income", "", ., fixed = TRUE) %>%
  gsub("occupation", "", ., fixed = TRUE) %>%
  gsub("pid", "", ., fixed = TRUE) %>%
  gsub("pres.approv", "", ., fixed = TRUE) %>%
  gsub("min.wage", "", ., fixed = TRUE) %>%
  gsub("country.track", "", ., fixed = TRUE) %>%
  gsub("Int", " Intercept", ., fixed = TRUE) %>%
  gsub("\\multicolumn{1}{l}{", "", ., fixed = TRUE)
cat(ct)

# this one is commented out in the appendix
tab.glm.anov <- stargazer(df.glm.anov, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "ANOVA Chisq Test of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group",
                 label = "glm-anov")
dt <- gsub("{c}", "{l}", tab.glm.anov, fixed = TRUE) %>%
  gsub("gender", "", ., fixed = TRUE) %>%
  gsub("race", "", ., fixed = TRUE) %>%
  gsub("income", "", ., fixed = TRUE) %>%
  gsub("occupation", "", ., fixed = TRUE) %>%
  gsub("pid", "", ., fixed = TRUE) %>%
  gsub("pres.approv", "", ., fixed = TRUE) %>%
  gsub("min.wage", "", ., fixed = TRUE) %>%
  gsub("country.track", "", ., fixed = TRUE)
cat(dt)

# To align ct along dots: Straighten the cat output out. Copy only the actual rows into a Latex doc. Replace all } with nothing. Replace all . with .&
# Copy rows back into R. Add \multicolumn{2}{l} for the column headings. Add variable title rows. Replace extracolsep etc. with r@{}l for each numerical column.
# Add longtable stuff


```


We block the ANES on education into three treatment groups, once based on the original ANES education categories, and once based on the newly estimated OP education categories. We overall do not observe large differences between the ANES and OP variable proportions/means in each treatment group with the naked eye (see appendix Figures \ref{ANBlock1} to \ref{ANBlock3} for details).

To dig deeper, I run an Analysis of Variance (ANOVA) test on the numerical variables `Age` and `Feel Trump`. An ANOVA regression provides information about levels of variability within a regression model and forms a basis for tests of significance. As the name implies, ANOVA analyzes the variance in the data to look for differences by calculating the the sums of squares to measure the distances of each data point to the mean. The ratio of the sums of squares forms the F-statistic, which is used to gauge statistical significance. 

I form a combined data set of all observations for each education set for each treatment group. This results in three data sets, i.e. one per treatment group. The first data set contains all observations from the ANES set that were assigned to treatment group 1 and all observations from the OP set that were assigned to treatment group 1. The second data set contains all ANES and OP observations that were assigned to treatment group 2. The third data set contains all ANES and OP observations that were assigned to treatment group 3. Each data set contains the column `Education Set` which denotes the education set each observation belongs to. For each data set, I run an ANOVA regression of the numerical variables `Age` and `Feel Trump` on `Education Set` to test whether the choice of education set has a significant effect. Table \ref{aov-sum} shows a summary of the results. Almost none of the variable intercepts show statistical significance. An exception here is treatment group 2, which shows a $p$-value of `r df.aov.sum["feel.trumpT2", "p-value"]` for `Feel Trump`. Overall, however, the differences between the treatment group means of the numerical variables `Age` and `Feel Trump` are not statistically significant for this sample size.


 \begin{table}[!htbp] \centering  
 \caption{Summary of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}   
 \label{aov-sum}
 \begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{l}{} & \multicolumn{1}{l}{Df} & \multicolumn{1}{l}{Sum.Sq} & \multicolumn{1}{l}{Mean.Sq} & \multicolumn{1}{l}{F-value} & \multicolumn{1}{l}{p-value} \\ 
 \hline \\[-1.8ex] 
\multicolumn{1}{l}{\textbf{Age}} & & & & & \\ 
 \multicolumn{1}{l}{T1} & 1 & 0.107 & 0.107 & 0.000 & 0.985 \\
 \multicolumn{1}{l}{T1 Residuals} & 2,098 & 641,917.600 & 305.966 & & \\
 \multicolumn{1}{l}{T2} & 1 & 414.519 & 414.519 & 1.340 & 0.247 \\ 
 \multicolumn{1}{l}{T2 Residuals} & 2,098 & 648,946.600 & 309.317 & & \\
 \multicolumn{1}{l}{T3} & 1 & 427.954 & 427.954 & 1.401 & 0.237 \\
 \multicolumn{1}{l}{T3 Residuals} & 2,098 & 641,038.400 & 305.547 & & \\ 
 & & & & & \\
\multicolumn{1}{l}{\textbf{Feel Trump}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & 1,164.808 & 1,164.808 & 0.979 & 0.323 \\ 
 \multicolumn{1}{l}{T1 Residuals} & 2,098 & 2,496,492.000 & 1,189.939 & & \\
 \multicolumn{1}{l}{T2} & 1 & 7,054.667 & 7,054.667 & 5.699 & 0.017 \\ 
 \multicolumn{1}{l}{T2 Residuals} & 2,098 & 2,597,204.000 & 1,237.943 & & \\
 \multicolumn{1}{l}{T3} & 1 & 2,486.298 & 2,486.298 & 2.023 & 0.155 \\ 
 \multicolumn{1}{l}{T3 Residuals} & 2,098 & 2,577,984.000 & 1,228.782 & & \\ 
 \hline \\[-1.8ex]
 \end{tabular} 
 \end{table}


This leaves the non-numerical factor variables (e.g. `Gender`, `Race` etc.). Since an ANOVA test is not possible for these variables, I conduct a binomial GLM regression here. The results are shown in Table \ref{glm-sum}. Contrary to the numerical variables, a large number of the $p$-values for the variable intercepts are highly significant. This includes all intercepts of `Race`, `Income`, `Occupation`, `Party ID`, `Min. Wage`, and `Country Track`. The only exceptions are `Gender` and `Pres. Approval`, where none of the intercepts in any treatment group (`Gender`) and only the intercepts in treatment groups 1 and 5 (`Pres. Approval`)  reach statistical significance. The large number of statistically distinct categories indicates that a differentiation between education categories may be important for non-numerical factor variables. 


\ssp

\footnotesize

\begin{longtable}{lr@{}lr@{}lr@{}lr@{}l}  
\caption{Summary of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}    
\label{glm-sum}  
\\[-1.8ex]\hline  
\hline \\[-1.8ex]  
  & \multicolumn{2}{l}{Estimate} & \multicolumn{2}{l}{Std.Error} & \multicolumn{2}{l}{z-value} & \multicolumn{2}{l}{p-value} \\  
\hline \\[-1.8ex]  
\textbf{Gender} & & & & \\  
 T1 Intercept & 0.&091 & 0.&062 & 1.&481 & 0.&139 \\ 
 T1 & 0.&038 & 0.&087 & 0.&437 & 0.&662 \\ 
 T2 Intercept & 0.&126 & 0.&062 & 2.&035 & 0.&042 \\
 T2 & -0.&004 & 0.&087 & -0.&044 & 0.&965 \\
 T3 Intercept & 0.&134 & 0.&062 & 2.&159 & 0.&031 \\
 T3 & -0.&034 & 0.&087 & -0.&393 & 0.&694 \\ 
 & & & & \\  
\textbf{Race} & & & & \\  
 T1 Intercept & -1.&061 & 0.&071 & -15.&024 & 0.&000 \\
 T1 & -0.&035 & 0.&100 & -0.&351 & 0.&726 \\ 
 T2 Intercept & -1.&153 & 0.&072 & -15.&952 & 0.&000 \\
 T2 & 0.&041 & 0.&102 & 0.&407 & 0.&684 \\
 T3 Intercept & -1.&206 & 0.&073 & -16.&452 & 0.&000 \\
 T3 & -0.&005 & 0.&104 & -0.&052 & 0.&959 \\
   & & & & \\  
\textbf{Income} & & & & \\  
 T1 Intercept & 1.&334 & 0.&076 & 17.&557 & 0.&000 \\
 T1 & -0.&006 & 0.&107 & -0.&054 & 0.&957 \\
 T2 Intercept & 1.&227 & 0.&074 & 16.&649 & 0.&000 \\ 
 T2 & 0.&038 & 0.&105 & 0.&367 & 0.&714 \\ 
 T3 Intercept & 1.&398 & 0.&077 & 18.&058 & 0.&000 \\ 
 T3 & -0.&036 & 0.&109 & -0.&327 & 0.&744 \\
 & & & & \\  
\textbf{Employment} & & & & \\  
 T1 Intercept & -0.&421 & 0.&063 & -6.&678 & 0.&000 \\
 T1 & -0.&246 & 0.&091 & -2.&713 & 0.&007 \\
 T2 Intercept & -0.&567 & 0.&064 & -8.&831 & 0.&000 \\
 T2 & 0.&193 & 0.&090 & 2.&152 & 0.&031 \\
 T3 Intercept & -0.&588 & 0.&064 & -9.&126 & 0.&000 \\ 
 T3 & 0.&049 & 0.&091 & 0.&545 & 0.&586 \\ 
 & & & & \\  
  \textbf{Party ID} & & & & \\  
 T1 Intercept & 0.&530 & 0.&064 & 8.&297 & 0.&000 \\
 T1 & 0.&125 & 0.&091 & 1.&367 & 0.&172 \\
 T2 Intercept & 0.&655 & 0.&065 & 10.&065 & 0.&000 \\
 T2 & -0.&125 & 0.&091 & -1.&367 & 0.&172 \\
 T3 Intercept & 0.&609 & 0.&065 & 9.&421 & 0.&000 \\ 
 T3 & -0.&000 & 0.&091 & -0.&000 & 1.&000 \\ 
 & & & & \\  
\textbf{President} & & & & \\  
 T1 Intercept & -0.&210 & 0.&062 & -3.&388 & 0.&001 \\ 
 T1 & 0.&058 & 0.&088 & 0.&657 & 0.&511 \\ 
 T2 Intercept & -0.&008 & 0.&062 & -0.&123 & 0.&902 \\ 
 T2 & -0.&134 & 0.&087 & -1.&528 & 0.&126 \\ 
 T3 Intercept & -0.&103 & 0.&062 & -1.&666 & 0.&096 \\
 T3 & 0.&076 & 0.&087 & 0.&873 & 0.&383 \\ 
 & & & & \\  
\textbf{Minimum Wage} & & & & \\  
 T1 Intercept & -0.&613 & 0.&065 & -9.&480 & 0.&000 \\ 
 T1 & 0.&042 & 0.&091 & 0.&456 & 0.&649 \\ 
 T2 Intercept & -0.&465 & 0.&063 & -7.&340 & 0.&000 \\
 T2 & -0.&106 & 0.&090 & -1.&173 & 0.&241 \\
 T3 Intercept & -0.&685 & 0.&065 & -10.&472 & 0.&000 \\ 
 T3 & 0.&068 & 0.&092 & 0.&736 & 0.&462 \\
 & & & & \\  
\textbf{Country} & & & & \\  
 T1 Intercept & 0.&954 & 0.&069 & 13.&850 & 0.&000 \\ 
 T1 & -0.&005 & 0.&097 & -0.&049 & 0.&961 \\ 
 T2 Intercept & 1.&111 & 0.&071 & 15.&544 & 0.&000 \\ 
 T2 & -0.&055 & 0.&100 & -0.&552 & 0.&581 \\ 
 T3 Intercept & 0.&992 & 0.&069 & 14.&282 & 0.&000 \\
 T3 & 0.&059 & 0.&099 & 0.&593 & 0.&553 \\ 
 \hline \\[-1.8ex]
 \end{longtable} 

\dsp

\normalsize


Overall, the evidence is mixed. While blocking on two different sets of education categories does not result in significantly differing means for the numerical variables, it does yield statistically significant differences between the factor variable proportions.


<!--
To investigate this further, I estimate Tukey's 'Honest Significant Difference' test, also know as Tukey's test, for these ANOVA results. Tukey's HSD test compares pairs of means based on a studentized range distribution. It is similar to a $t$-test but corrects for the probability of making false discoveries (i.e. type I errors). It is mathematically formulated as

\vspace{-1cm}
$$q_t = \frac{M_I - M_J}{SE},$$

with $M_I$ and $M_J$ denoting two means (with $M_I$ being the larger one) and $SE$ the standard error of the sum of the means. The resulting value $q_t$ is compared to the critical value of the distribution. Table \ref{aov-tukey} displays the results. Each row shows the comparison of OP and ANES means per treatment group. The adjusted $p$-values confirm the results from Table \ref{aov-sum}. With the exception of treatment group 3, the differences between the treatment group means of `Age` and `Feel Trump` are not statistically significant, i.e. the two blocking mechanisms overall do not result in significantly differing means for the numerical variables.


\begin{table}[!htbp] \centering    
\caption{Tukey's `Honest Significant Difference' Test of ANOVA Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}    
\label{aov-tukey}  
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }  
\\[-1.8ex]\hline  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Diff.Means} & \multicolumn{1}{l}{CI Lower} & \multicolumn{1}{l}{CI Upper} & \multicolumn{1}{l}{Adj. p-value} \\  
\hline \\[-1.8ex]  
\multicolumn{1}{l}{\textbf{Age}} & & & & \\  
\multicolumn{1}{l}{T1 OP1-ANES1} & \multicolumn{1}{l}{.410} & \multicolumn{1}{l}{-1.523} & \multicolumn{1}{l}{2.342} & \multicolumn{1}{l}{.678} \\  
\multicolumn{1}{l}{T2 OP2-ANES2} & \multicolumn{1}{l}{.590} & \multicolumn{1}{l}{-1.353} & \multicolumn{1}{l}{2.534} & \multicolumn{1}{l}{.551} \\  
\multicolumn{1}{l}{T3 OP3-ANES3} & \multicolumn{1}{l}{-1.654} & \multicolumn{1}{l}{-3.591} & \multicolumn{1}{l}{.283} & \multicolumn{1}{l}{.094} \\  
\multicolumn{1}{l}{T4 OP4-ANES4} & \multicolumn{1}{l}{.071} & \multicolumn{1}{l}{-1.869} & \multicolumn{1}{l}{2.011} & \multicolumn{1}{l}{.942} \\  
\multicolumn{1}{l}{T5 OP5-ANES5} & \multicolumn{1}{l}{.583} & \multicolumn{1}{l}{-1.345} & \multicolumn{1}{l}{2.510} & \multicolumn{1}{l}{.553} \\  
 & & & & \\
\multicolumn{1}{l}{\textbf{Feel Trump}} & & & & \\  
\multicolumn{1}{l}{T1 OP1-ANES1} & \multicolumn{1}{l}{-.317} & \multicolumn{1}{l}{-4.091} & \multicolumn{1}{l}{3.456} & \multicolumn{1}{l}{.869} \\  
\multicolumn{1}{l}{T2 OP2-ANES2} & \multicolumn{1}{l}{-.884} & \multicolumn{1}{l}{-4.798} & \multicolumn{1}{l}{3.030} & \multicolumn{1}{l}{.658} \\  
\multicolumn{1}{l}{T3 OP3-ANES3} & \multicolumn{1}{l}{-4.338} & \multicolumn{1}{l}{-8.194} & \multicolumn{1}{l}{-.482} & \multicolumn{1}{l}{.027} \\  
\multicolumn{1}{l}{T4 OP4-ANES4} & \multicolumn{1}{l}{2.817} & \multicolumn{1}{l}{-1.049} & \multicolumn{1}{l}{6.684} & \multicolumn{1}{l}{.153} \\  
\multicolumn{1}{l}{T5 OP5-ANES5} & \multicolumn{1}{l}{2.722} & \multicolumn{1}{l}{-1.158} & \multicolumn{1}{l}{6.602} & \multicolumn{1}{l}{.169} \\  
\hline \\[-1.8ex]  
\end{tabular}  
\end{table} 
-->





### Placebo Regression {#ordblock-data-plac}

To further test the usefulness of the ordered probit categories, I conduct a placebo regression. Placebo tests are most commonly used for difference-in-differences estimators and are falsification tests to analyze whether an effect exists that should not exist [@bertrand_2004_much; @mills_2009_palgrave; @rothstein_2010_teacher]. The placebo treatment should be unrelated to the model or method being studied [@hartman_2018_equivalence; @rosenbaum_2002_observational; @mora_2019_alternative]. In the effort to conduct an analysis that is separate from the previous section, I block the 2016 ANES data anew on education, this time into two treatment groups. As before, this is done separately for the original ANES and the OP education categories. We then model the following OLS regression on the feeling thermometer towards Donald Trump as the Republican presidential candidate for all observations $i$ with $i = [1,\ldots,n]$:

\begin{align}
\text{Feel Trump}_i \sim & \,\beta_0 + \beta_1 \text{Group}_i + \beta_2 \text{Democrat}_i + \beta_3 \text{Republican}_i + \beta_4 \text{Income}_i \,+ \nonumber \\ 
& \beta_5 \text{Male}_i + \beta_6 \text{White}_i + \beta_7 \text{Black}_i + \beta_8 \text{Hispanic}_i.
\end{align}

The \texttt{Group} variable indicates the two differing sets of education categories and is a placebo treatment, i.e. I did not conduct an experiment and no actual treatment was administered. Instead, I created the \texttt{Group} variable, randomly 'assigned' observations to the two treatment groups, and then used this variable as an explanatory variable. The treatment is thus completely artificial, which means the difference between both treatment groups should be zero, i.e. there should be no significant differences between the \texttt{Group} regression coefficients for the two education sets. The null hypothesis thus assumes that there is no effect. To test this, each blocking/regression process for each set of categories is repeated 1,000 times. The distribution of the placebo treatment indicator (\texttt{Group}) is visualized in Figure \ref{DensPlacTreat}. 


```{r Placebo Treatment Plotting Code, include=FALSE}

repeats <- 1000
list.list.gt2.coeff <- readRDS(paste("data/blocking/list_list_test_ols_trump_gt2_coeff_all_obs_", repeats, "_runs.rds", sep = ""))

gt2.orig <- unlist(list.list.gt2.coeff[[1]], recursive = FALSE)
gt2.new <- unlist(list.list.gt2.coeff[[2]], recursive = FALSE)

gt2.orig.df <- data.frame(cbind(gt2.orig, rep("ANES", length(gt2.orig))))
gt2.new.df <- data.frame(cbind(gt2.new, rep("OP", length(gt2.new))))
colnames(gt2.orig.df) <- colnames(gt2.new.df) <- c("coefficient", "categories")
gt2.df <- rbind(gt2.orig.df, gt2.new.df)
gt2.df$coefficient <- as.numeric(as.character(gt2.df$coefficient))

```


```{r Density-Plot-Placebo-Treatment, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distribution of Placebo Treatment Coefficients by Education Set. Overlapping Area Shown in Dark Purple.\\label{DensPlacTreat}"}

ggplot(gt2.df, aes(x=coefficient, fill=categories)) + geom_density(alpha=0.4, aes(y=..density..), position="identity") + xlab("Regression Coefficients for Placebo Treatment Group") + ylab("Density") + theme(legend.title=element_blank()) + theme(legend.position = c(0.85, 0.75)) + theme(plot.title = element_text(hjust = 0.5))+ scale_fill_manual(values=c("red", "blue"))

```


Both distributions center around zero, as is the statistical expectation. Upon closer inspection, however, the ordered probit categories are closer to the true value of zero than the ANES categories on both mean (`r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, mean)[2], digits = 3))` v. `r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, mean)[1], digits = 3))`) and median (`r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, median)[2], digits = 3))` v. `r unname(round(tapply(gt2.df$coefficient, gt2.df$categories, median)[1], digits = 3))`). This indicates slightly superior performance by the ordered probit categories, as they more closely approach the true value when used in a regression.





## Conclusion {#ordblock-conclusion}

I set out to improve the use of ordinal variables to block respondents into treatment groups in survey experiments. Survey experiments depend on balance of covariates between treatment groups to allow the estimation of causal effects. Randomization ensures balance for large samples but becomes problematic for small samples. Blocking greatly alleviates this problem. 

Blocking naturally depends on covariates. One of the most important covariates in political science is education. Out of convenience, education is often converted to a numerical variable for regressions in practice. Due to the special nature of education as an ordinal variable, such an approach is potentially problematic as ordinal variable categories are not evenly spaced. The distance between the education categories "Elementary School" and "Some High School" is very likely not the same as the distance between "Some High School" and "High School Graduate". Converting these three categories to the numerical values 1, 2, and 3, however, assumes evenly spaced distances. This could lead to a misrepresentation of the data.

As an alternative, I proposed an ordered probit approach whereby we estimate the latent underlying continuous variable underneath education. This estimates cutoff thresholds between the education categories, bins observations according to linear model predictors, and results in a new set of education categories that fit the data and represent the latent underlying continuous variable with its unevenly spaced distances. I applied this approach to the 2016 ANES data, resulting in two sets of education categories: the original ANES categories, and the newly estimated OP categories. I subsequently blocked both sets into three treatment groups and analyzed the group means and variances. While the numerical variables do not show statistically distinct intercepts, most of the factor variables do. This might indicate that the re-estimation of education categories is meaningful, but the evidence is unclear. I also ran a placebo regression 1,000 times to conduct a falsification test. This shows distributions of the placebo treatment variable around the statistically expected zero but also reveals the OP variable to be closer to zero than the ANES variable for both mean and median. This might indicate slightly superior performance by the OP method. 

In all, the results are unclear. Some findings support the notion that the re-estimation of ordinal variable categories with an ordered probit approach matters, while others negate it. The next chapter will use the OP approach to address multiple imputation with ordinal variables.






<!--chapter:end:02-ordinal-blocking.Rmd-->

# QUALITY COMPARISON OF MAJOR MISSING DATA SOLUTIONS WITH A PROPOSED NEW METHOD FOR ORDINAL VARIABLES {#ordmiss}

## Introduction {#ordmiss-intro}

```{r include=FALSE}
# function to make all diff columns absolute values
abs.diff <- function(df){
  df$diff <- df$diff %>% abs
  return(df)
}

# function to add a plus/minus sign for the diff numeric columns in data
addPlus <- function (df, digits = 4){
  x <- dplyr::select_if(df, is.numeric)
  for(i in 1:ncol(x)){
    x[,i] <- round(as.numeric(x[,i]), digits)
    x[,i] <- sprintf(paste0("%.", digits, "f"), x[,i])
    # x[,i] <- gsub("^0(?=\\.)|(?<=-)0", "", x[,i], perl = TRUE) # took it out because Jeff wants leading zeros ...
  }
  df[, colnames(x)] <- x
  
  for(i in 1:nrow(df)){
    if(grepl("-", df$diff[i]) == FALSE){
      df$diff[i] <- paste0("+", df$diff[i])
    }else{
      df$diff[i] <- paste0("-", df$diff[i])
    }
  }
  return(df)
}

```

```{r Levels Overall, include=FALSE}
levs <- c("amelia", "hot.deck", "hd.ord", "mice", "na.omit", "true")
col.names <- c("Method", "Variable", "ANES", "CCES")
# col.names <- c("Method", "Variable", "ANES", "CCES", "Framing")
```


Missing data are ubiquitous in survey research [@allison_2002_missing;@raghunathan_2016_missing]. Respondents frequently refuse to answer questions, select "Don't Know" as a response option, or drop out during the response collection process [@honaker_2010_what]. Missingness in data sets poses a big problem since such data cannot be appropriately analyzed with statistical software without pre-processing [@little_2002_statistical;@molenberghs_2007_missing]. Scholars have developed several ways to treat missing data. These can be roughly categorized into deletion, single imputation, and multiple imputation. 

Deletion, also called case-wise deletion, list-wise deletion or complete case analysis, simply removes all observations with missing values from the sample. Single imputation concerns the replacement of missing values with substitute estimates such as the mean, regression coefficients, or values from randomly drawn 'similar' respondents in the data. Multiple imputation estimates missing values from conditional distributions and subsequently averages the results into a single parameter of inference [@rubin_1976_inference;@king_2001_analyzing;@fay_1996_alternative].

Multiple imputation has become the state of the art in missing data management since it accounts for and incorporates uncertainty around the estimated imputations through repeated draws [@andridge_2010_review; @graham_1999_performance; @schafer_2002_missing; @white_2011_multiple]. This is missing from single imputation techniques which treat the single estimated replacement value as a de-facto data entry on par with observed values. Uncertainty is not reflected in the imputed values, which leads to biased standard errors and confidence intervals [@kroh_2006_taking;@gill_2013_bayesian]. Similarly, list-wise deletion has been shown to induce bias with political data [@bodner_2008_what;@collins_2001_comparison;@pigott_review_2001;@rees_1997_methods;@reilly_1993_data].

However, parametric multiple imputation as applied by the most popular imputation packages in `R` is not necessarily always the most suitable method for all types of variables. For discrete data, multiple hot deck imputation, a combination of the single imputation method hot decking and multiple imputation, proves more precise as it avoids the common multiple imputation technique of imputing discrete data on a continuous scale [@gill_2012_have]. The latter turns discrete variables into continuous variables which changes their nature and can result in non-observable and biased imputation values with artificially smaller standard errors [@fuller_2005_deck; @kim_2004_finite; @kim_2004_fractional]. Multiple hot deck imputation on the other hand preserves the integrity of discrete data, does not change the size of standard errors, and produces more accurate imputations. It estimates affinity scores for each missing value to measure how similar a respondent with a missing value is to another respondent across all variables except the missing one. 

However, multiple hot deck imputation does not account for ordinal variables as its affinity score algorithm assumes even distances between categories in discrete data. This assumption is not warranted for ordinal variables. I propose a method designed to impute discrete missing data specifically from ordinal variables. Because of the success of multiple hot deck imputation in its applicability to missing data with discrete variables with a small number of categories [@gill_2012_have], this method is based on multiple hot deck imputation and adapted to account for the specific circumstances of ordinal variables. Based on the ordered probit model approach described in section \ref{ordblock-theory-op}, it applies a scaled solution with newly estimated numerical thresholds from an assumed underlying latent continuous variable to measure the distances between the categories and calculate affinity scores.

<!--
The following is an outline of the remainder of this chapter: Sections \ref{ordmiss-theory-mechanisms} to \ref{ordmiss-theory-singimpute} will discuss the theory behind missing data mechanisms and provide an overview of list-wise deletion as well as the most common single imputation methods. Section \ref{ordmiss-theory-multimpute} explains the basics of multiple imputation and outlines major `R` packages that implement it. These include `Amelia`, `mice`, `hot.deck` (applying multiple hot deck imputation), and my self-penned multiple hot deck imputation function for ordinal variables, `hd.ord`. Since single imputation is widely condemned as a general imputation method, my focus lies on multiple imputation. Section \ref{ordmiss-data} describes the survey data these packages/functions are tested on: a selection of the 2016 ANES and a subset of the 2016 CCES data. Each sample is complete and randomly amputed to insert missing data. Each of the four `R` packages/functions is applied to each data set and tested for accuracy and speed for different types of missingness and variables. Section \ref{ordmiss-results} shows the results and assesses the usefulness of the proposed approach. Finally, section \ref{ordmiss-conclusion} features concluding remarks.
-->


## Theory {#ordmiss-theory}

### Missing Data Mechanisms {#ordmiss-theory-mechanisms}

Let there be $\bm{X}$, an $n \times v$ matrix with data on $n$ respondents for $v$ variables. Let there also be the response indicator $\bm{R}$ as an $n \times v$ matrix with values of 0 or 1. Let their respective elements be denoted by $x_{ij}$ and $r_{ij}$, with $i = 1, ..., n$ and $j = 1, ..., v$. If $x_{ij}$ is observed, $r_{ij} = 1$. If $x_{ij}$ is missing, $r_{ij} = 0$. All elements where $r_{ij} = 0$ make up the missing data, $\bm{X^{miss}}$. All elements where $r_{ij} = 1$ make up the observed data, $\bm{X^{obs}}$. Together, $\bm{X^{obs}}$ and $\bm{X^{miss}}$ form the complete data $\bm{X}$. Missing data can then generally be described by $\text{prob}(\bm{R} = 0 | \bm{X^{obs}}, \bm{X^{miss}}, \bm{\theta})$, i.e. the probability of missing data depends on the observed data, the missing data, and a vector of unknown parameters. Depending on the mechanism by which data is missing, this expression can be further simplified. Data can be missing by three basic mechanisms: It can be missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). Researchers need to make assumptions about how the data came to be missing. 

The general missing data expression can be simplified under the MCAR assumption to $\text{prob}(\bm{R} = 0 | \bm{\theta})$, i.e. the generic probability of missing data, independent of the data themselves and only dependent on $\bm{\theta}$. A MCAR assumption in political science surveys is rare and requires justification as missing data most often occur systematically. Respondents are known to withhold sensitive data, for instance in the attempt to hide private information (income, sexual orientation) or out of fear of political or social repercussions in the community (union membership, support for polarizing political candidates) [@groves_survey_2009]. Such information is often not refused randomly but occurs only in specific subsets of respondents. Answers criticizing the authorities, for instance, tend to be missing in surveys in non-democratic states, while the state-loyal responses are present. 

<!--
The simplest and easiest case is missing completely at random (MCAR). Under the MCAR assumption, there is no process guiding the missingness; it is inserted truly at random. 
In statistical terms, this means both the unobserved and the missing data independently from each other form a random sample of the population and have the same underlying distribution. 
-->

It is more commonly assumed among researchers that data are MAR. MAR means missing data are related to the observed but not the unobserved data. In practical terms, this for instance means missing data on income can be related to observed data on education or occupation. Here, the missing data are not a random sample of the entire data. MAR transforms the general missing data expression to $\text{prob}(\bm{R} = 0 | \bm{X^{obs}}, \bm{\theta})$, i.e. the chances of missing data depend on the observed data and $\bm{\theta}$.

Finally, data can be MNAR.^[Data MNAR is also sometimes called 'non-ignorable' [see for instance @gill_2012_have and Allison, 2002].] This is the case when missing data are related to unknown and/or unobserved parameters. Continuing the example of missing data on income, under MNAR we do not observe data on education or occupation that can be used to fill the missing income data slot. In the case of data MNAR, the general missing data expression remains unchanged, $\text{prob}(\bm{R} = 0 | \bm{X^{obs}}, \bm{X^{miss}}, \bm{\theta})$, i.e. the missingness of data depends on the observed data, the missing data, and $\bm{\theta}$.

As mentioned above, the assessment of the missing data mechanism underlying any respective data comes from researchers and their understanding of the data generating process. The statistical methods to address missing data can be broadly categorized into deletion, imputation, and multiple imputation.




### List-Wise Deletion {#ordmiss-theory-delete}

One of the most common methods of handling missing data in quantitative political science is list-wise deletion. This involves the removal of any incomplete observations, thereby reducing the sample size. The resulting sample is then ready for analysis. 

List-wise deletion is not biased with data MCAR as it removes a random sample of the population, but it reduces the degrees of freedom and might hinder sub-group analysis [@allison_2002_missing; @king_2001_analyzing; @little_2002_statistical; @schafer_2002_missing]. When data are MAR or MNAR, list-wise deletion is always biased, since the observed data is tilted towards respondents with characteristics that make them more likely to respond. Whether this bias is trivial or substantial depends on the research and circumstances in question [@collins_2001_comparison; @diggle_1994_informative; @glynn_1993_multiple; @graham_1997_analysis; @robins_1998_semiparametric].

While the bias inserted by list-wise deletion in each individual data analysis may not necessarily be drastic, studies have shown that it can be so severe as to alter substantive conclusions [@brown_1994_efficacy; @graham_1996_maximizing; @honaker_2010_what; @wothke_2000_longitudinal]. Even if that were not or only rarely the case and most data were MCAR, reducing the sample size is generally never a recommended approach as, among other aspects, standard errors from regression models are inflated. As @king_2001_analyzing put it, the result of list-wise deletion "is a loss of valuable information at best and severe selection bias at worst" (p. 49). In `R`, list-wise deletion can be implemented with the base function `na.omit`. 




### Single Imputation {#ordmiss-theory-singimpute}

Single imputation means replacing missing data with substituted values, i.e. the structural opposite of deletion. Imputation requires a predictive distribution, based on the observed data, from which value substitutions are picked. Single imputation, regardless of its exact nature, is not recommended to impute missing data since, similar to deletion, it downward biases standard errors and confidence intervals [@honaker_2010_what]. Crucially, uncertainty is not reflected in the imputed values [@little_2002_statistical]. The following is a mere selection of the most common single imputation methods and makes no claim of completeness. Since single imputation is widely condemned as a general imputation method and as my focus lies on multiple imputation, they are also brief.


#### Mean {#ordmiss-theory-impute-mean}

Mean imputation, sometimes also called unconditional mean imputation, involves replacing missing values within cells with the mean of the observed values, so $\bm{X^{miss}} = \bm{\overline{X^{obs}}}$. While it does not change the mean of the sample, this method distorts the empirical distribution of $\bm{X}$, which in turn produces biased estimates of any non-linear quantities such as variances and covariances [@haitovsky_1968_missing]. It is also bound to be inaccurate in most cases, since few values generally fall exactly on the mean, and can be non-sensical for discrete variables [@efron_1994_missing]. Mean imputation can be done in many ways in `R`, for instance with the `impute` function in the `Hmisc` package or by setting `method = "mean"` in the `mice` function in the `mice` package. 

<!--
It is also very simple to execute in base `R`, e.g. for a sample data frame `df` and the numerical column `x`: $\text{df} \leftarrow \text{transform(df, x = ifelse(is.na(x), mean(x, na.rm=TRUE), x))}$.
-->


#### Regression {#ordmiss-theory-impute-regress}

Imputation by regression, sometimes also called conditional mean imputation, imputes missing values conditional on observed values. Researchers predict observed variable values based on other variables, while the fitted values from the regression model are then used to impute variable values where they are missing. Let there be an explanatory variable $x$ in a multiple regression model. Assume that $x$ contains missing values, $x^{miss}$, and observed values, $x^{obs}$. We regress $x^{obs}$ on the other explanatory variables and use the estimated equation to generate predicted values for $x^{miss}$, $x^{pred}$. $x^{pred}$ then replace $x^{miss}$, thus completing the data set. While more accurate than mean imputation, particularly for large samples with data MCAR, regression imputation nonetheless suffers from the same flaw that accompanies all single imputation approaches: Uncertainty is not reflected in the imputed values [@horton_2007_much].

Differing variations of imputation by regression exist in `R`, such as the `aregImpute` function in the `Hmisc` package, which performs additive regression, and setting `method = "norm.predict"` in the `mice` function to conduct linear regression. The `predict` function in base `R` also applies linear regression imputation.


#### Hot Decking {#ordmiss-theory-impute-hd}

Hot deck imputation was developed in the 1970s and replaces missing values with values from similar respondents in the sample [@ernst_1978_weighting; @ford_1983_overview]. It is called 'hot decking' as a reference to taking draws from a deck of matching computer punch-cards. The deck was 'hot' since it was currently being processed, as opposed to pre-collected or 'cold' data [@andridge_2010_review; @little_2002_statistical]. In the most general version, researchers select all respondents that are 'similar' to a respondent with missing data and randomly draw one of those respondents (with replacement) to fill in the missing value. The respondent with the initially missing value is termed the \textit{recipient}, while the 'similar' respondent is called the \textit{donor}. Variations of the method include hot decking within adjustment cells, by nearest neighbor, and sequentially ordered by a covariate [@cox_1980_weighted; @david_1986_alternative; @kaiser_1983_effectiveness; @kalton_1981_efficient; @rockwell_1975_investigation].

Contrary to mean or regression imputation, hot deck imputation preserves the integrity of the data, i.e. only actually observed values are used to fill in missing slots [@bailar_1997_comparison]. In both other single imputation methods, it is possible and sometimes even likely that missing values are replaced by values not found amongst the observed values. Contrary to regression imputation, hot decking also does not require a fitted model and is thus less vulnerable to model misspecification. However, hot decking does necessitate the existence of at least some donors for a respondent at every variable value that is missing. With a lot of missing data and few 'similar' matches, the accuracy of hot decking greatly decreases [@young_2011_survey]. Hot decking works best for discrete data as continuous data are very unlikely to be matched or 'similar', though the definition of what might constitute a 'similar' respondent is somewhat subjective [@marker_2002_large-scale]. As is the case with all single imputation methods, uncertainty is not reflected in the imputed values. Selecting the initial sample of 'similar' respondents and the subsequent random sampling from that subsampling are treated as factual responses, which leads to smaller standard errors and confidence intervals than statistically valid [@little_2002_statistical].

To my knowledge, there is currently no `R` package that applies single hot deck imputation. Nonetheless, variations of hot decking are still in use by some government statistics agencies such as the National Center for Education Statistics (for parts of the Current Population Survey) or the U.S. Bureau of the Census [@census_2002_current; @education-statistics_2002_nces].




### Multiple Imputation {#ordmiss-theory-multimpute}

Multiple imputation was invented by Rubin in the 1970s to account for the absence of uncertainty in single imputation methods and allow more accurate standard error estimates. It fills missing values with a predictive model that includes observed data and prior knowledge [@honaker_2010_what]. Over the time of its development, it has become the dominant sophisticated strategy for handling missing data [@dempster_1977_maximum; @glynn_1993_multiple; @heitjan_1991_ignorability; @little_2002_statistical; @rubin_1976_inference; @rubin_1986_multiple; @rubin_1987_multiple; @rubin_1996_multiple]. Multiple imputation involves three general steps:

\vspace{0.3cm}
\begin{adjustwidth*}{+0.5cm}{+0.5cm}
\ssp
\begin{enumerate}
\item \noindent Impute data with missing values $m$ times. This results in $i$ complete
 \begin{sloppypar}\hspace{0.5cm} data sets (with $i = 1, ..., m$).\end{sloppypar}
\item Analyze each of the $i$ complete data sets.
\item Combine the results from each of the $i$ analyses into one collective result.
\end{enumerate}
\end{adjustwidth*}
\vspace{0.3cm}

\ssp

\begin{figure}
\centering
\begin{tikzpicture}
    \node [block3] (ds) {\scriptsize{Data set with missing values}};
    \node [block2r, above right=1.2cm and 1.8cm of ds] (imp1) {\scriptsize{Imputed data set 1}};
    \node [block2r, above right=-0.4cm and 1.8cm of ds] (imp2) {\scriptsize{Imputed data set 2}};
    \node [block2r, below right=1.2cm and 1.8cm of ds] (impi) {\scriptsize{Imputed data set $i$}};
	\coordinate[right=0cm of ds] (dsc);
	\coordinate[left=0cm of imp1] (imp1lc);	
	\coordinate[left=0cm of imp2] (imp2lc);	
	\coordinate[left=0cm of impi] (impilc);	
	\path [line] (dsc) -- (imp1lc);
	\path [line] (dsc) -- (imp2lc);
	\path (imp2) --node[auto=false]{\Large{\vdots}} (impi);
	\path [line] (dsc) -- (impilc);
    \node [block2r, right=1cm of imp1] (results1) {\scriptsize{Results 1}};
    \node [block2r, right=1cm of imp2] (results2) {\scriptsize{Results 2}};
    \node [block2r, right=1cm of impi] (resultsi) {\scriptsize{Results $i$}};
	\coordinate[right=0cm of imp1] (imp1rc);
	\coordinate[right=0cm of imp2] (imp2rc);
	\coordinate[right=0cm of impi] (impirc);
	\coordinate[left=0cm of results1] (results1lc);	
	\coordinate[left=0cm of results2] (results2lc);	
	\coordinate[left=0cm of resultsi] (resultsilc);	
	\path [line] (imp1rc) -- (results1lc);
	\path [line] (imp2rc) -- (results2lc);
	\path (results2) --node[auto=false]{\Large{\vdots}} (resultsi);
	\path [line] (impirc) -- (resultsilc);
    \node [block3, below right=1.2cm and 1.8cm of results1] (results) {\scriptsize{Combined result}};
	\coordinate[left=0cm of results] (resultsc);
	\coordinate[right=0cm of results1] (results1rc);	
	\coordinate[right=0cm of results2] (results2rc);	
	\coordinate[right=0cm of resultsi] (resultsirc);	
	\path [line] (results1rc) -- (resultsc);
	\path [line] (results2rc) -- (resultsc);
	\path [line] (resultsirc) -- (resultsc);
\end{tikzpicture}
\caption{Multiple Imputation Workflow} 
\label{mi-workflow}
\end{figure}

\dsp

Figure \ref{mi-workflow} provides a graphical overview of this workflow. Each missing value is imputed $m$ times from a conditional distribution using other present values for the respective value to create $i$ imputed complete data sets. The chosen statistical analysis $\bm{\tau}$, for instance a regression model, is applied to each of these $i$ data sets, resulting in $\bm{\tau_{i}}$, with $i = 1, ..., m$. Averaging $\bm{\tau_{i}}$ then gives us the single estimate, $\bm{\overline{\tau}}$. Together, this is expressed as: 

\begin{align}
\overline{\tau} = \frac{1}{m}\sum\limits_{i=1}^{m} \tau_{i}.
\end{align}


Following @rubin_1987_multiple, the total variance of $\bm{\overline{\tau}}$, $\bm{Var_T}$ consists of the mean variance of $\bm{\tau_i}$ within each data set $i$, $\bm{\overline{Var_W}}$, and the sample variance of $\bm{\tau}$ across both data sets, $\bm{Var_A}$:

\begin{align}
\overline{Var_W} &= \frac{1}{m} \sum\limits_{i=1}^{m} SE(\tau_i)^2\\
Var_A &= \sum\limits_{i=1}^{m} \frac{(\tau_{i} - \overline{\tau})^2}{m -1}\\
Var_T &= \overline{Var_W} + Var_A.
\end{align}

Multiplied by a factor correcting for small numbers of $m$ (as $m < \infty$), $\bm{Var_T}$ is adjusted to 

\begin{align}
Var_T = \overline{Var_W} + Var_A (1 + \frac{1}{m}).
\end{align}


Each imputed complete data set is identical to all other imputed complete data sets, with the exception of the imputed value. The imputed values for a missing value differ with each imputation of $\bm{M}$ in order to reflect uncertainty levels. The 'multiple' part of the imputation is a crucial aspect here since each imputation run will produce slightly different parameter estimates. Imputing multiple times and then averaging the results creates variability to adjust the standard errors upward, with the latter step taking the same form as an ANOVA calculation [@kroh_2006_taking]. This deliberate random variation included in a deterministic multiple imputation run removes the overconfidence from single imputation, where the standard error estimates are too low [@schafer_2002_missing]. In a case where the utilized multiple imputation model predicts missing values well, variation across the imputed values is small. In other cases, variation may be larger, depending on the level of certainty we have about the missing value. Multiple imputation has been shown to produce consistent, asymptotically efficient and normal estimates for a variety of data MAR [@allison_2002_missing]. It is also advised for data MCAR in order to retain degrees of freedom.

Choosing $m$, the number of imputations, is somewhat subjective. Originally, $m = 5$ was considered sufficient based on efficiency calculations [@rubin_1987_multiple] and is still the default in most software packages. More recent discussions stress the need for an increase of $m$ in order to estimate more nuanced standard errors. Various approaches continue to coexist, such as focusing on the parameter with the largest fraction of missing information [@kroh_2006_taking] or starting with $m = 5$ and gradually increasing it in subsequent runs [@raghunathan_2016_missing]. The most common current practice appears to be to set $m$ to the percentage of missing data, i.e. if 20 percent of data are missing, $m = 20$ [@bodner_2008_what; @white_2011_multiple]. 

There are numerous ways to implement multiple imputation. Up until the late 1990s, this required considerable statistical knowledge and sophisticated methodological skills [see @honaker_2010_what for an overview]. The use of multiple imputation was thus limited to a rather specialized audience of statisticians and methodologists. Since then, numerous `R` packages have emerged to facilitate user-friendliness. The by far most popular packages are `mice` and `Amelia`. Since its inception in 2001, `mice` has been downloaded nearly 3 million times from CRAN at the time of writing. `Amelia` was created in 1998 and has been downloaded over 600,000 times. They are both considered among the very best implementations of multiple imputation [@horton_2007_much]. Any improvement in multiple imputation thus needs to be measured against them. `hot.deck`, the method by @gill_2012_have upon which my proposed method of multiple hot deck imputation with ordinal variables, `hd.ord`, is based, follows this approach and demonstrates improved results when compared to `Amelia`. I extend this with `hd.ord` and also include `mice` as a further benchmark of performance.

The following sections do not cover the full list of functions available in each package, as this would go far beyond the scope of this chapter and fills articles of its own [e.g. @buuren_2007_multiple]. Instead, I will focus on the packages' core underlying mechanisms and their major functions to perform imputation, which are named after their package namesakes: `mice`, `amelia`, `hot.deck`, and `hd.ord`.^[For the remainder of this chapter and to avoid confusion, all names will refer to the functions unless explicitly stated otherwise.] I extend the focus on simplicity and user-friendliness further by running these major imputation functions with their default settings. Survey analysts usually do not possess the statistical expertise that enable them to dive deeply into distribution or chain properties. The vast majority of users can be assumed to use imputation functions with their default settings. If a package only proved superior over others by setting specific and highly technical function arguments, this would defeat the purpose of making multiple imputation the missing data approach for the masses. I apply only one very minor exception to the default settings by setting the number of imputations to the percentage of missingness instead of the default five.





#### `mice`: Multivariate Imputation by Chained Equations {#ordmiss-theory-multimpute-mice}

The `R` package `mice` was released in 2001 [@buuren_2000_multiple]. It stands for Multiple Imputation by Chained Equations (MICE), which means imputing incomplete multivariate data by full conditional specification [@buuren_2007_multiple; @buuren_2011_mice], a version of the imputation-posterior (IP) [@king_2001_analyzing]. Full conditional specification refers to imputation on a variable-by-variable basis, i.e. a set of conditional densities is used to impute data for each individual missing value. This approach does not require the specification of a multivariate distribution for the missing data, which separates it from competing methods like joint modeling [@schafer_1997_analysis]. The initial release of `mice` featured predictor selection, passive imputation, and automatic pooling. Subsequent releases included functionality for imputing multilevel data, post-processing imputed values, specialized pooling, stable imputation of categorical data, and model selection, among many others. Imputation by chained equations is extensively used across domains [see @buuren_2011_mice for a list of over 20 applied fields]. 

Chained equations are based on the Gibbs sampler, a randomized Markov chain Monte Carlo algorithm to estimate a sequence of observations from a specified multivariate probability distribution [@gelman_2013_bayesian; @gill_2014_bayesian]. In essence, chained equations fill in missing values through an iterative repetition of univariate procedures that are chained together -- hence the name for the procedure. As the term univariate signifies, specification happens at the variable level, i.e. each chained equation specifies the imputation model separately for each column of the data. Following deliberations by @rubin_1987_multiple and @buuren_2011_mice, imputation by chained equations takes the missing data generating process into account and maintains data relations as well as the uncertainty about these relations. With these conditions satisfied, the imputation model results in statistically valid and factual imputations. This has been proven empirically under various circumstances, for instance for regression models [@giorgi_2008_performance; @horton_2001_multiple; @horton_2007_much], continuous data [@yu_2007_evaluation], missing predictor variables [@moons_2006_using], large surveys [@schunk_2008_markov], and addressing issues of convergence [@brand_1999_development; @buuren_2006_fully; @drechsler_2008_does].

Continuing the notation from section \ref{ordmiss-theory-mechanisms} and incorporating @buuren_2011_mice, let there be $\bm{X}$, an $n \times v$ matrix with data on $n$ respondents for $v$ variables, that is formed of missing, $\bm{X^{miss}}$, and observed data, $\bm{X^{obs}}$. As before, let there also be a vector of unknown parameters $\bm{\theta}$. Now let $\bm{X}$ further be a random sample from the $z$-variate multivariate distribution, $\bm{Z}(\bm{X} | \bm{\theta})$, with $\bm{\theta}$ accounting for the multivariate distribution of $\bm{X}$. The proverbial pot of gold here is how to estimate the multivariate distribution of $\bm{\theta}$. Under the chained equations model, we estimate a posterior distribution of $\bm{\theta}$ by sampling repeatedly from conditional distributions, i.e.

\begin{align}
Z(X_1 &| X_{-1}, \theta_1) \nonumber\\
&\vdots \nonumber\\
Z(X_z &| X_{-z}, \theta_z).
\end{align}

Any iteration $n$ of chained equations is then a Gibbs sampler that sequentially draws

\begin{align}
\theta_1^{*(n)} &\sim Z(\theta_1 | X_1^{obs}, X_2^{(n-1)}, ..., X_z^{(n-1)}) \nonumber\\
X_1^{*(n)} &\sim Z(X_1 | X_1^{obs}, X_2^{(n-1)}, ..., X_z^{(n-1)}, \theta_1^{*(n)}) \nonumber\\
&\vdots \nonumber\\
\theta_z^{*(n)} &\sim Z(\theta_z | X_z^{obs}, X_1^{(n)}, ..., X_{z-1}^{(n)}) \nonumber\\
X_z^{*(n)} &\sim Z(X_z | X_z^{obs}, X_1^{(n)}, ..., X_z^{(n)}, \theta_z^{*(n)}),
\end{align}

with the chain starting from a random draw from observed marginal distributions and $\bm{X_i^{(n)}} = (\bm{X_i^{obs}}, \bm{X_i^{*(n)}})$ being the $i$th imputed variable at iteration $n$. Note that immediately preceding imputations, $\bm{X_i^{*(n-1)}}$, do not affect $\bm{X_i^{*(n)}}$ directly but only through connections with other variables. 

Figure \ref{mice-func} shows the package's main imputation function, `mice`, with all its arguments. As stated above, I will use `mice` with its default settings to ensure simplicity and user-friendliness.

\vspace{0.5cm}

\begin{figure}[!htbp] 
  \centering
  \includegraphics{figures/mice.png}
  \caption{The \texttt{mice} Function}
  \label{mice-func}
\end{figure}

\vspace{-0.5cm}

The majority of arguments are not of importance to general users. Arguments like `predictorMatrix`, which specifies the set of predictors to be used for each target column, and `blocks`, which provides the option to manually put variables into imputation blocks, require too much statistical knowledge to be of use to non-specialists. Other arguments do not affect the basic workings of the function. This applies for instance to `printFlag`, which sets the console printing preference, `seed`, which is used to offset the random number generator, and `data.init`, which specifies a data frame to be used to initialize imputations before the start of the iterative process. 

The only important arguments for general users are `data`, `m`, and `defaultMethod`. Only `data` requires user input. As mentioned above, `m` should also be set to the percentage of missing data. The argument `defaultMethod` does not require user input but is crucial for insights into the default workings of `mice`. Its options `pmm`, `logreg`, `polyreg`, and `polr` refer to the default imputation methods that are implemented depending on the type of variable in question. The argument `pmm` (predictive mean matching) is used for numerical data, `logreg` (logistic regression imputation) for binary and factor data with two levels, `polyreg` (polytomous regression imputation) for factor data with more than two unordered levels, and `polr` (proportional odds model) for factor data with more than two ordered levels. Note that `mice` thus distinguishes between ordered and unordered as well as the number of factor levels, but does not specifically incorporate ordinal variables, which feature ordered but unevenly spaced levels.


<!--
`data`
A data frame or a matrix containing the incomplete data. Missing values are coded as NA.
`m = 5`
Number of multiple imputations. The default is m=5.
`method = NULL`
Can be either a single string, or a vector of strings with length length(blocks), specifying the imputation method to be used for each column in data. If specified as a single string, the same method will be used for all blocks. The default imputation method (when no argument is specified) depends on the measurement level of the target column, as regulated by the defaultMethod argument. Columns that need not be imputed have the empty method "". See details.
`predictorMatrix`
A numerical matrix of length(blocks) rows and ncol(data) columns, containing 0/1 data specifying the set of predictors to be used for each target column. Each row corresponds to a variable block, i.e., a set of variables to be imputed. A value of 1 means that the column variable is used as a predictor for the target block (in the rows). By default, the predictorMatrix is a square matrix of ncol(data) rows and columns with all 1's, except for the diagonal. Note: For two-level imputation models (which have "2l" in their names) other codes (e.g, 2 or -2) are also allowed.
`where = NULL`
A data frame or matrix with logicals of the same dimensions as data indicating where in the data the imputations should be created. The default, where = is.na(data), specifies that the missing data should be imputed. The where argument may be used to overimpute observed data, or to skip imputations for selected missing values.
`blocks`
List of vectors with variable names per block. List elements may be named to identify blocks. Variables within a block are imputed by a multivariate imputation method (see method argument). By default each variable is placed into its own block, which is effectively fully conditional specification (FCS) by univariate models (variable-by-variable imputation). Only variables whose names appear in blocks are imputed. The relevant columns in the where matrix are set to FALSE of variables that are not block members. A variable may appear in multiple blocks. In that case, it is effectively re-imputed each time that it is visited.
`visitSequence = NULL`
A vector of block names of arbitrary length, specifying the sequence of blocks that are imputed during one iteration of the Gibbs sampler. A block is a collection of variables. All variables that are members of the same block are imputed when the block is visited. A variable that is a member of multiple blocks is re-imputed within the same iteration. The default visitSequence = "roman" visits the blocks (left to right) in the order in which they appear in blocks. One may also use one of the following keywords: "arabic" (right to left), "monotone" (ordered low to high proportion of missing data) and "revmonotone" (reverse of monotone).
`formulas`
A named list of formula's, or expressions that can be converted into formula's by as.formula. List elements correspond to blocks. The block to which the list element applies is identified by its name, so list names must correspond to block names. The formulas argument is an alternative to the predictorMatrix argument that allows for more flexibility in specifying imputation models, e.g., for specifying interaction terms.
`blots = NULL`
A named list of alist's that can be used to pass down arguments to lower level imputation function. The entries of element blots[[blockname]] are passed down to the function called for block blockname.
`post = NULL`
A vector of strings with length ncol(data) specifying expressions as strings. Each string is parsed and executed within the sampler() function to post-process imputed values during the iterations. The default is a vector of empty strings, indicating no post-processing.
`defaultMethod = c("pmm", "logreg", "polyreg", "polr")`
A vector of length 4 containing the default imputation methods for 1) numerical data, 2) factor data with 2 levels, 3) factor data with > 2 unordered levels, and 4) factor data with > 2 ordered levels. By default, the method uses pmm, predictive mean matching (numerical data) logreg, logistic regression imputation (binary data, factor with 2 levels) polyreg, polytomous regression imputation for unordered categorical data (factor > 2 levels) polr, proportional odds model for (ordered, > 2 levels).
`maxit = 5`
A scalar giving the number of iterations. The default is 5.
`printFlag = TRUE`
If TRUE, mice will print history on console. Use print=FALSE for silent computation.
`seed = NA`
An integer that is used as argument by the set.seed() for offsetting the random number generator. Default is to leave the random number generator alone.
`data.init = NULL`
A data frame of the same size and type as data, without missing data, used to initialize imputations before the start of the iterative process. The default NULL implies that starting imputation are created by a simple random draw from the data. Note that specification of data.init will start all m Gibbs sampling streams from the same imputation.
-->





#### `Amelia`: A Program for Missing Data {#ordmiss-theory-multimpute-amelia}

The `R` package `Amelia` was originally released in 1998 [@honaker_1998_amelia]. A second version, `Amelia II`, was released in 2010 [@honaker_2012_amelia]. Contrary to `mice`, which is based on IP, both versions of `Amelia` are based on the expectation-maximization (EM) algorithm [@dempster_1977_maximum; @gelman_2013_bayesian; @jackman_2000_estimation; @mclachlan_1997_algorithm; @tanner_1996_tools]. In EM, deterministic calculations of posterior means replace random draws from the entire posterior. This translates into running regressions to estimate the regression coefficient, imputing a missing value with a predicted value, re-estimating the regression coefficient, and repeating the process until convergence [@king_2001_analyzing]. While the iterations and parameters thus represent an entire density in IP, they are single maximum posterior values in EM. This makes EM comparatively much faster in finding the maximum of the likelihood function. On its own, however, EM is unsuitable for multiple imputation as it does not provide the rest of the distribution. 

The `Amelia` package circumvents this issue with expectation-maximization importance sampling (EMi) [@dempster_1977_maximum; @rubin_1987_multiple], which combines EM with the iterative simulation approach of importance sampling. This proved unsuitable for large data sets, however, as it led to high running times and system crashes. The `Amelia II` package addresses this by mixing EM with bootstrapping [@efron_1994_missing; @lahlrl_2003_impact; @rubin_1994_missing; @shao_1996_bootstrap], allowing the imputation of more variables for more observations more quickly.

`Amelia II` is based on the assumption that the complete data ($\bm{X^{obs}}$ and $\bm{X^{miss}}$) are multivariate normal (MVN), $\bm{X} \sim \bm{N_v}(\mu, \sum)$, with mean vector $\mu$ and covariance matrix $\sum$. The MVN model has been proven to work for a variety of variable types [@ezzati-rice_1995_simulation; @graham_1999_performance; @rubin_1986_multiple; @schafer_1997_analysis]. Continuing the notation from section \ref{ordmiss-theory-mechanisms} and incorporating @honaker_2010_what, let there be a vector of unknown parameters $\bm{\theta}$, with $\bm{\theta} = (\mu, \sum)$. Let there further be our missingness matrix $\bm{R}$ and the likelihood of $\bm{X^{obs}}$, $\text{prob}(\bm{X^{obs}}, \bm{R} | \bm{\theta})$. `Amelia II` is explicitly set up for the MAR assumption of missing data, $\text{prob}(\bm{R} = 0 | \bm{X^{obs}}, \bm{\theta})$. Under this assumption, the likelihood can be transformed as

\begin{align}
\text{prob}(X^{obs}, R | \theta) = \text{prob}(R | X^{obs}) \text{prob}(X^{obs} | \theta).
\end{align}

Since the missing mechanism is MAR, we are only interested in the inference on complete data parameters, thus the likelihood becomes

\begin{align}
L(\theta | X^{obs}) \propto \text{prob}(X^{obs} | \theta)
\end{align}

which further translates into

\begin{align}
\text{prob}(X^{obs} | \theta) = \int \text{prob}(X | \theta) x X^{miss}
\end{align}

under the law of iterated expectations. This results in the posterior

\begin{align}
\text{prob}(\theta | X^{obs}) \propto \text{prob}(X^{obs} | \theta) = \int \text{prob}(X | \theta) x X^{miss}.
\end{align}

Taking draws from this posterior is computationally intensive since the contents of $\mu$ and $\sum$ increase exponentially as the number of variables increases -- this is the perennial crux of multiple imputation, particularly for large data sets with many variables. `Amelia II` solves this through a combination of EM and bootstrapping. This process bootstraps the data to simulate estimation uncertainty for each posterior draw, runs the EM algorithm to find the mode of the posterior bootstrapped data, and then imputes by drawing from $\bm{X^{miss}}$ conditional on $\bm{X^{obs}}$ and the respective draws of $\bm{\theta}$. The latter is a linear regression with parameters that can be estimated from $\bm{\theta}$. This bootstrapped EM approach is faster than IP as Markov chains do not need to be assessed for convergence and an improvement over EMi since the variance matrix of $\mu$ and $\sum$ does not need to be calculated, allowing the algorithm to handle larger data sets.

Figure \ref{amelia-func} shows the package's main imputation function, `amelia`, with all its arguments. As stated above, I will use `amelia` with its default settings to ensure simplicity and user-friendliness.

\vspace{0.5cm}

\begin{figure}[!htbp] 
  \centering
  \includegraphics{figures/amelia.png}
  \caption{The \texttt{amelia} Function}
  \label{amelia-func}
\end{figure}

\vspace{-0.5cm}

As with `mice`, the majority of arguments are not of importance to general users. Specifications such as `splinetime`, which allows the control of cubic smoothing splines of time, and `lags`, which indicates columns in the data that should have their lags included in the imputation model, will only be used in very particular situations by a small minority of users. Other arguments likewise are not crucial to the basic workings of the function, such as `p2s`	to control console printing and `parallel` to identify any type of parallel operation to be used. 

The only argument that requires user input is `x`, which needs to be data with missing values that can be in a variety of formats. `m`, identical to `mice`, should be adjusted to reflect the percentage of missingness in the data. Three other arguments are important since they arguably comprise the core of `amelia`'s underlying imputation mechanism: `tolerance`, `autopri`, and `boot.type`. `tolerance`	sets the convergence threshold for the EM algorithm. `autopri`	allows the EM chain to increase the empirical prior if the path strays into an non-positive definite covariance matrix. `boot.type` offers the option to turn off the non-parametric bootstrap that is applied by default. 

General multiple imputation research treats independent ordinal variables as continuous variables. `amelia` supports this and treats ordinal variables as continuous variables as a default. This means missing ordinal variables are imputed on a continuous scale, rather than preserved as the factual levels present in the observed data. However, the `ords` argument allows users to 'disable' continuous ordinal imputation. In this case, ordinal variables are still imputed on a continuous scale, but these imputations are then scaled and used as the probability of success in a binomial distribution. The draw from this binomial distribution is then transformed into one of the ordinal levels present in the observed data by rounding. While `amelia` thus does incorporate ordinal variables to some extent, the rounding process changes the nature of ordinal variables to continuous variables. None of its features address or reflect the spacing between the ordinal variable categories. 


<!--
`x`
either a matrix, data.frame, a object of class "amelia", or an object of class "molist". The first two will call the default S3 method. The third a convenient way to perform more imputations with the same parameters. The fourth will impute based on the settings from moPrep and any additional arguments.
`m`	
the number of imputed data sets to create.
`p2s`	
an integer value taking either 0 for no screen output, 1 for normal screen printing of iteration numbers, and 2 for detailed screen output. See "Details" for specifics on output when p2s=2.
`frontend`	
a logical value used internally for the GUI.
`idvars`	
a vector of column numbers or column names that indicates identification variables. These will be dropped from the analysis but copied into the imputed data sets.
`ts`	
column number or variable name indicating the variable identifying time in time series data.
`cs`	
column number or variable name indicating the cross section variable.
`polytime`	
integer between 0 and 3 indicating what power of polynomial should be included in the imputation model to account for the effects of time. A setting of 0 would indicate constant levels, 1 would indicate linear time effects, 2 would indicate squared effects, and 3 would indicate cubic time effects.
`intercs`	
a logical variable indicating if the time effects of polytime should vary across the cross-section.
`lags`
a vector of numbers or names indicating columns in the data that should have their lags included in the imputation model.
`leads`	
a vector of numbers or names indicating columns in the data that should have their leads (future values) included in the imputation model.
`startvals`	
starting values, 0 for the parameter matrix from list-wise deletion, 1 for an identity matrix.
`tolerance`	
the convergence threshold for the EM algorithm.
`logs`
a vector of column numbers or column names that refer to variables that require log-linear transformation.
`sqrts`	
a vector of numbers or names indicating columns in the data that should be transformed by a sqaure root function. Data in this column cannot be less than zero.
`lgstc`	
a vector of numbers or names indicating columns in the data that should be transformed by a logistic function for proportional data. Data in this column must be between 0 and 1.
`noms`
a vector of numbers or names indicating columns in the data that are nominal variables.
`ords`
a vector of numbers or names indicating columns in the data that should be treated as ordinal variables.
`incheck`	
a logical indicating whether or not the inputs to the function should be checked before running amelia. This should only be set to FALSE if you are extremely confident that your settings are non-problematic and you are trying to save computational time.
`collect`	
a logical value indicating whether or not the garbage collection frequency should be increased during the imputation model. Only set this to TRUE if you are experiencing memory issues as it can significantly slow down the imputation process.
`arglist`	
an object of class "ameliaArgs" from a previous run of Amelia. Including this object will use the arguments from that run.
`empri`	
number indicating level of the empirical (or ridge) prior. This prior shrinks the covariances of the data, but keeps the means and variances the same for problems of high missingness, small N's or large correlations among the variables. Should be kept small, perhaps 0.5 to 1 percent of the rows of the data; a reasonable upper bound is around 10 percent of the rows of the data.
`priors`	
a four or five column matrix containing the priors for either individual missing observations or variable-wide missing values. See "Details" for more information.
`autopri`	
allows the EM chain to increase the empirical prior if the path strays into an non-positive definite covariance matrix, up to a maximum empirical prior of the value of this argument times $n$, the number of observations. Must be between 0 and 1, and at zero this turns off this feature.
`emburn`	
a numerical vector of length 2, where emburn[1] is a the minimum EM chain length and emburn[2] is the maximum EM chain length. These are ignored if they are less than 1.
`bounds`	
a three column matrix to hold logical bounds on the imputations. Each row of the matrix should be of the form c(column.number, lower.bound,upper.bound) See Details below.
`max.resample`	
an integer that specifies how many times Amelia should redraw the imputed values when trying to meet the logical constraints of bounds. After this value, imputed values are set to the bounds.
`overimp`	
a two-column matrix describing which cells are to be overimputed. Each row of the matrix should be a c(row, column) pair. Each of these cells will be treated as missing and replaced with draws from the imputation model.
`boot.type`	
choice of bootstrap, currently restricted to either "ordinary" for the usual non-parametric bootstrap and "none" for no bootstrap.
`parallel`	
the type of parallel operation to be used (if any). If missing, the default is taken from the option "amelia.parallel" (and if that is not set, "no").
`ncpus`	
integer: the number of processes to be used in parallel operation: typically one would choose the number of available CPUs.
`cl`
an optional parallel or snow cluster for use if parallel = "snow". If not supplied, a cluster on the local machine is created for the duration of the amelia call.
-->







#### `hot.deck`: Multiple Hot Deck Imputation {#ordmiss-theory-multimpute-hdnorm}

`hot.deck` is an `R` package released in 2012 [@gill_2012_have]. It combines a variation of non-parametric hot decking (see section \ref{ordmiss-theory-singimpute}) with multiple imputation and aims to fill gaps where parametric multiple imputation, i.e. the approach used in `mice` and `amelia`, falls short [@fuller_2005_deck; @kim_2004_finite; @kim_2004_fractional; @reilly_1993_data]. Like hot decking, `hot.deck` uses draws of actual observable values (\textit{donors}) to fill missing values (\textit{recipients}). In order to account for uncertainty around the drawn values, `hot.deck` iterates these draws over $m$ imputations and pools the results. 

The main proposed advantage of `hot.deck` lies in its applicability to missing data with discrete variables with a small number of categories. Approaches like the one used in `amelia`, for instance, by default impute discrete data on a continuous scale. This changes the nature of discrete variables and practically turns them into continuous variables. This can result in non-observable, biased, and sometimes even non-sensical imputation values with artificially smaller standard errors. The proposed `amelia` solution of rounding continuous imputations is problematic as well: Let imputation 1 of a binary variable between 0 and 1 be 0.4. Let further imputation 2 of the binary variable be 0.6. With rounding, these imputations become 0 and 1, when they are in fact 0.4 and 0.6. Rounding thus by definition introduces at least some level of bias. The problem is exacerbated for ordinal variables, where the spacing between the discrete variable categories is unknown, since it arbitrarily reduces or lengthens distances between the categories. This is not the case in `hot.deck` as it preserves the integrity of discrete data, does not change the size of standard errors, and produces more accurate imputations. `hot.deck` also does not require assumptions of a MVN distribution that are required by `amelia`.

Following @gill_2012_have, `hot.deck` estimates affinity scores, $\bm{\alpha}$, for each missing value to measure how similar a respondent with a missing value, the recipient $c$, is to another respondent, the potential donor $o$, across all variables except the missing one. Each score is bounded by 0 and 1. The total set of affinity scores is denoted by $\bm{\alpha_{co}}$. For each respondent, let there be vector $(p, v)$, with $p$ being the dependent variable and $v$ a vector of discrete explanatory variables of length $k$. If recipient $c$ has $q_c$ missing values in $v_c$, then the potential donor vector, $v_o$, has between 0 and $k-q_c$ exact matches with $c$. Let $w_{co}$ be the number of variables where $c$ and $o$ have non-identical values. This leaves $k-q_c -w_{co}$ as the number of variables where they have identical values. Scaled by the highest number of possible matches $(k-q_c)$, this value forms the affinity score

\begin{align}
\alpha_{co} = \frac{k-q_c-w_{co}}{k-q_c}
\end{align}

for each missing value recipient $c$. When the number of identical matches decreases, so does $\bm{\alpha_{co}}$. While this might work well for binary variables, it poses a problem for discrete variables with many levels, as the probability to find identical matches decreases. To account for this, `hot.deck` treats potential donors $o$ for the $h$th variable in $v_{o[h]}$ that are 'close' differently than potential donors $o$ that are further away. 'Close' is defined as $v_{o[h]}$ and $v_{c[h]}$ being in the same concentric standard deviation from $\overline{h}$, the mean of variable $h$. Values outside of this range are penalized while values within this range are counted as matches. All donors with the highest affinity scores, i.e. all matches, form the best imputation cell $\bm{B}$. Since all values of $v_{c[h]}$ in $\bm{B}$ are part of the same distribution of independent and identically distributed (iid) random variables, which satisfies the MCAR requirement, we can use random draws from $\bm{B}$ to impute the missing value. As with the other multiple imputation approaches, this process is then repeated $m$ times for each missing value to account for imputation uncertainty, following the logic displayed in Figure \ref{mi-workflow}.

Figure \ref{hot.deck-func} shows the package's main imputation function, `hot.deck`, with all its arguments. As before, I will use `hot.deck` with its default settings. 

\vspace{0.5cm}

\begin{figure}[!htbp] 
  \centering
  \includegraphics{figures/hot.deck.png}
  \caption{The \texttt{hot.deck} Function}
  \label{hot.deck-func}
\end{figure}

\vspace{-0.5cm}

Like `mice` and `amelia`, `hot.deck` only requires user input for `data`. `m` should once more be set to the percentage of missingness. Specialized arguments such as `optimStep` and `optimStop`, which can be tweaked to optimize standard deviation cutoff parameters, as well as `weightedAffinity`, which indicates whether a correlation-weighted affinity score should be used, do not apply to general users.

`method` and `cutoff` form the core of `hot.deck`. The default setting of `best.cell` in the `method` argument implements multiple hot deck imputation. The alternative, `p.draw`, on the other hand, merely conducts random probabilistic draws. `cutoff` allows users to specify which variables the algorithm should treat as discrete. By default, any variable up to and including 10 unique values is considered discrete. This thus includes the majority of political science survey measures, with the sensible exceptions of variables like age or for instance widely spread assessments of income levels.

Overall, `hot.deck` is a specialized function to improve the application of multiple imputation for discrete data and has been shown to do so for highly granular discrete data [@gill_2012_have]. Moreover, political science survey research relies on highly discrete measures. What is missing from `hot.deck`, however, is the incorporation of ordinal variables as a special form of discrete data. I thus identify this gap as a leverage point to improve the use of ordinal variables in the imputation of missing data. To do so, I adapt `hot.deck` to form `hd.ord`, a function specifically designed to utilize the ordered but unevenly spaced information contained in ordinal variables.






<!--
`data`
A data frame or matrix with missing values to be imputed using multiple hot deck imputation.
`m`	
Number of imputed data sets required.
`method`	
Method used to draw donors based on affinity either best.cell (the default) or p.draw for probabilistic draw
`cutoff`	
A numerical scalar such that any variable with fewer than cutoff unique non-missing values will be considered discrete and necessarily imputed with hot deck imputation.
`sdCutoff`	
Number of standard deviations between observations such that observations fewer than sdCutoff standard deviations away from each other are considered sufficiently close to be a match, otherwise they are considered too far away to be a match.
`optimizeSD`	
Logical indicating whether the sdCutoff parameter should be optimized such that the smallest possible value is chosen that produces no thin cells from which to draw donors. Thin cells are those where the number of donors is less than m.
`optimStep`	
The size of the steps in the optimization if optimizeSD is TRUE.
`optimStop`	
The value at which optimization should stop if it has not already found a value that produces no thin cells. If this value is reached and thin cells still exist, a warning will be returned, though the routine will continue using optimStop as sdCutoff.
`weightedAffinity`	
Logical indicating whether a correlation-weighted affinity score should be used.
`impContinuous`
Character string indicating how continuous missing data should be imputed. Valid options are HD (the default) in which case hot-deck imputation will be used, or mice in which case multiple imputation by chained equations will be used.
`IDvars`
A character vector of variable names not to be used in the imputation, but to be included in the final imputed data sets.
-->





#### `hd.ord`: Multiple Hot Deck Imputation with Ordinal Variables {#ordmiss-theory-multimpute-hdord}

`hd.ord` is a self-penned `R` function designed specifically to implement multiple hot deck imputation with ordinal variables. It is an extension of `hot.deck` and fully utilizes the unevenly spaced yet ordered information contained in ordinal variables. As described in section \ref{ordblock-theory-op}, ordinal variables matter in political science surveys because a key variable in such surveys is ordinal: education. The importance of the spacing between education values is best demonstrated with a simplified example shown in Table \ref{ordmiss-ordspace}.


\begin{table}[!htbp] 
  \centering
  \caption{Illustrative Data}
  \label{ordmiss-ordspace}
  \begin{tabular}{lccccc}
  \bottomrule 
  \midrule
  Respondent & Age & Party ID & Education & Income & Gender\\
  \hline
  A & 25 & Republican & High School Graduate & \$30-40,000 & Male \\
  B & 40 & NA & Some High School &  \$20-30,000 & Female\\
  C & 30 & Democrat & Bachelor's Degree &  \$50-60,000 & Female\\
  \bottomrule 
  \end{tabular}
\end{table}

Respondent B shows missing data for party ID. To impute a fill-in value, we look at how close respondents A and C are to B in terms of age, education, income, and gender. C is closer to B in terms of age and they share the same gender. A is closer to B on education and income. `hot.deck` measures these distances and estimates affinity scores for respondents A and C. The affinity scores measure how close A and C are to B on all variables except the missing one, i.e. party ID. B then receives the party ID fill-in value from whichever respondent has the higher score. The algorithm building the affinity score is based on evenly spaced sequential numerical values, e.g. 1, 2, 3 etc. to represent the distances between the variable categories. This makes sense for age, income, and gender, but not for education, since education is an ordinal variable. Applying `hot.deck` to such a numerical representation might thus misrepresent the data.

To avoid this, `hd.ord` uses the ordered probit approach described in section \ref{ordblock-theory-op}. It applies `polr` from the `MASS` package to any specified number of ordinal variables in the data to estimate the underlying latent continuous variable. This estimates cutoff thresholds between the ordinal categories and bins data cases according to the linear predictors. The binned cases determine which variable categories make sense, given the underlying latent continuous variable. This can result in a reduction of education categories if the categories are too finely thinned out. `hd.ord` estimates the mid-cutpoints between each of these newly estimated categories based on the `polr` results. We then replace the ordinal variable categories with the newly estimated numerical mid-cutpoints in the data. Finally, these values are scaled and used to assess distances to calculate affinity scores.


```{r Illustrative Data Code, include=FALSE}

load("functions/OPMord.Rdata")
load("functions/OPMcut.Rdata")
ill.data <- readRDS("data/anes/anes_1000.rds")

ill.data <- ill.data[,!names(ill.data) %in% c("Liberal", "Conservative", "Single")]
dv <- "Educ"
dplyr::rename(ill.data, 
              Democrat = Dem,
              Income = Inc)

all.evs <- colnames(ill.data)[-which(colnames(ill.data) == dv)]                         
add.nas.columns <- c("Democrat", "Male", "Interest", "Income", "Age")                        
no.nas <- ill.data[,!names(ill.data) %in% add.nas.columns]                                  
yes.nas <- ill.data[,names(ill.data) %in% add.nas.columns]
prop <- .2

set.seed(123)
data.amp <- cbind(no.nas, ampute(yes.nas, prop = prop, mech = "MAR")$amp)
OPMord.full <- OPMord(data.amp, dv = dv, evs = all.evs)
OPMcut.dat <- OPMcut(data = OPMord.full$data.full.nas, dv = dv, OPMordOut = OPMord.full)

OPMord.full$int.df$Intercepts <- OPMord.full$int.df$Intercepts %>% as.character 
OPMord.full$int.df$Intercepts <- c("Less Than High School|Some High School", "Some High School|High School Graduate", "High School Graduate|Some College", "Some College|Bachelor's Degree", "Bachelor's Degree|Master's Degree")
OPMord.full$int.df$Values <- round(OPMord.full$int.df$Values, digits = 3)
ill.data.int.df <- OPMord.full$int.df[,1:2]
ill.data.int.df.tab <- ill.data.int.df
colnames(ill.data.int.df.tab) <- c("Thresholds", "Coefficients")

ill.data.cutp <- data.frame(cbind(c("Less Than High School", "Some High School", "High School Graduate", "Some College", "Bachelor's Degree", "Master's Degree"), OPMcut.dat$Educ %>% unique %>% round(., digits = 3) %>% sort))
ill.data.cutp$X2 <- ill.data.cutp$X2 %>% as.character %>% as.numeric
# ill.data.cutp <- ill.data.cutp %>% round(digits = 3)
colnames(ill.data.cutp) <- c("origed", "midc")
ill.data.cutp.tab <- ill.data.cutp
colnames(ill.data.cutp.tab) <- c("Original Education Categories", "Mid-Cutpoints")

```


Table \ref{ordmiss-ill-res} illustrates this procedure with results from running `polr` on rudimentary survey data, with column "Thresholds" showing the estimated cutoff thresholds between the education categories. Table \ref{ordmiss-ill-mid} in turn shows the estimated mid-cutpoints for each of the education categories. The mid-cutpoint values for the categories in Table \ref{ordmiss-ill-mid} fall between the adjacent values in Table \ref{ordmiss-ill-res}, i.e. the mid-cutpoint of `r ill.data.cutp$midc[ill.data.cutp$origed == "Some High School"]` for "Some High School" lies between the respective thresholds of `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"]` and `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"]`. To estimate the beginning cutpoint for the first category ("Less Than High School"), we halve the difference between the first and second threshold and subtract this value from the first threshold: `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"]` $-$ (`r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"]` $-$ `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric`) / 2 =  `r ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric - ((ill.data.int.df$Values[ill.data.int.df$Intercepts == "Some High School|High School Graduate"] %>% as.numeric - ill.data.int.df$Values[ill.data.int.df$Intercepts == "Less Than High School|Some High School"] %>% as.numeric) / 2) %>% round(digits = 3)`. The same process is applied to estimate the ending cutpoint for the last category ("Master's Degree"). The mid-cutpoint values are then scaled and used for the calculation of the affinity scores.


```{r Illustrative Data Table 1, include=FALSE}

stargazer(ill.data.int.df.tab, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Illustrative Data `polr` Results",
          label = "ordmiss-ill-res")
```


\begin{table}[!htbp] \centering 
  \caption{Illustrative Data `polr` Results} 
  \label{ordmiss-ill-res} 
\begin{tabular}{r@{}lr@{}l} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{2}{c}{Thresholds} & \multicolumn{2}{c}{Coefficients} \\ 
\hline \\[-1.8ex] 
Less Than High School $\mid$ & \,Some High School & 2.&418 \\ 
Some High School $\mid$ & \,High School Graduate & 3.&495 \\ 
High School Graduate $\mid$ & \,Some College & 4.&214 \\ 
Some College $\mid$ & \,Bachelor's Degree & 5.&727 \\ 
Bachelor's Degree $\mid$ & \,Master's Degree & 7.&412 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 


```{r Illustrative Data Table 2, results='asis', echo=FALSE}

stargazer(ill.data.cutp.tab, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Illustrative Data Value Replacements",
          label = "ordmiss-ill-mid")

```




Figure \ref{hd.ord-func} shows `hd.ord` with all its arguments. As before, I will use `hd.ord` with its default settings. Since `hd.ord` is an adaptation of `hot.deck`, the two functions are identical except for the `ord` argument, which allows users to specify the ordinal variables for `polr` treatment.

\vspace{0.5cm}

\begin{figure}[!htbp]
  \centering
  \includegraphics{figures/hd.ord.png}
  \caption{The \texttt{hd.ord} Function}
  \label{hd.ord-func}
\end{figure}

\vspace{-0.5cm}




<!--
`data`
A data frame or matrix with missing values to be imputed using multiple hot deck imputation.
`ord`
An ordinal variable to be treated with polr()
`m`	
Number of imputed data sets required.
`method`	
Method used to draw donors based on affinity either best.cell (the default) or p.draw for probabilistic draw
`cutoff`	
A numerical scalar such that any variable with fewer than cutoff unique non-missing values will be considered discrete and necessarily imputed with hot deck imputation.
`sdCutoff`	
Number of standard deviations between observations such that observations fewer than sdCutoff standard deviations away from each other are considered sufficiently close to be a match, otherwise they are considered too far away to be a match.
`optimizeSD`	
Logical indicating whether the sdCutoff parameter should be optimized such that the smallest possible value is chosen that produces no thin cells from which to draw donors. Thin cells are those where the number of donors is less than m.
`optimStep`	
The size of the steps in the optimization if optimizeSD is TRUE.
`optimStop`	
The value at which optimization should stop if it has not already found a value that produces no thin cells. If this value is reached and thin cells still exist, a warning will be returned, though the routine will continue using optimStop as sdCutoff.
`weightedAffinity`	
Logical indicating whether a correlation-weighted affinity score should be used.
`impContinuous`
Character string indicating how continuous missing data should be imputed. Valid options are HD (the default) in which case hot-deck imputation will be used, or mice in which case multiple imputation by chained equations will be used.
`IDvars`
A character vector of variable names not to be used in the imputation, but to be included in the final imputed data sets.
-->






## Data {#ordmiss-data}

To test the performance of imputation methods, we need to work with complete data, as only complete data allow us to obtain the true values needed as a benchmark for comparison. I choose two different sets of survey data from the 2016 ANES and the 2016 CCES. Data for all selected variables in both data sets is complete. In order to test the accuracy of several imputation methods, I delete data from these complete data sets with the `ampute` function from the `mice` package [@buuren_2020_package]. `ampute` allows the removal of data MCAR, MAR, and MNAR. Particularly the availability of the latter offers unique opportunities, as it can be a difficult feat to establish whether real-life missing data is MNAR. Data that are artificially created to be MNAR, however, circumvent this problem and allow us to test the accuracy of imputation methods for this type of missing data as well. `ampute` has been shown to accurately remove data MCAR, MAR, and MNAR [@schouten_2018_generating].

Each data set is imputed with four different `R` functions: `hd.ord`, `hot.deck`, `amelia`, and `mice`. I also apply list-wise deletion with `na.omit`. As outlined in section \ref{ordmiss-theory-multimpute}, all functions are used with their default settings but with the number of imputations set to the percentage of missingness.

I test each function for accuracy and speed for binary, ordinal, and interval variables in both data sets. Each data set contains two ordinal (`Education`, `Interest`), two interval (`Age`, `Income`) and numerous binary variables. In order to enable factually accurate comparison and unless explicitly specified otherwise, each data set contains 1,000 observations and six levels of the ordinal variable `Education`. 1,000 observations represent a common size for survey and survey experiment data, and the `polr` analysis from section \ref{ordblock-data} estimates five or six levels to best represent `Education` in a US context. Each data set was imputed 1,000 times with each of the four imputation methods. With the exception of Figure \ref{accuracy50} and Table \ref{run.cces.perc}, 20 percent missing data were randomly amputed in each iteration for each data set.^[The decision to reduce the number of education categories in the ANES data is made out of necessity. It is not feasible to use all ANES observations, as repeated multiple imputation is computationally intensive for a sample of this size. The reduction to 1,000 observations in turn makes it impossible to use all original categories, since the insertion of missing data would consistently lead to dropped categories, which in turn would render a comparison of imputation runs pointless. See appendix section \ref{app-ordmiss-allObs} for imputations for all ANES and CCES observations (as much as computationally possible).] 

Following @collins_2001_comparison and @honaker_2010_what, as many relevant observed variables as possible were used to impute each of the data sets. For the ANES data, up to 14 predictor variables were used: `Independent`, `Moderate`, `Black`, `Hispanic`, `Asian`, `Employed`, `Student`, `Religious`, `InternetHome`, `OwnHome`, `Rally` (have you attended a political rally), `Donate` (have you donated to a political candidate), `Married`, and `Separated`. For the CCES data, up to 17 predictor variables were used: `Republican`, `Moderate`, `Liberal`, `Black`, `Hispanic`, `Asian`, `Employed`, `Unemployed`, `Student`, `Gay`, `Bisexual`, `StudLoans` (do you have student loans), `InternetHome`, `NotReligious`, `RentHome`, `Separated`, and `Single`. Highly collinear variables were excluded with a cutoff of 0.6.
<!-- For the Framing data, up to 13 predictor variables were used: `Ind`, `Conservative`, `Liberal`, `Black`, `Hisp`, `White`, `Asian`, `Unempl`, `Ret` (Retired), `Stud`, `Official` (have you written to a political official), `Media` (how much do you follow public affairs in the media), and `Participation` (how many political activities have you participated in).  -->

The variable mean serves as the baseline of comparison for the performance of each imputation method. Since each data set is complete, we know the true variable mean of all variables. The closer a method comes to the true mean, the better its performance. I impute both data sets for data MAR and MNAR for five amputed variables: `Democrat` (binary), `Male` (binary), `Interest` (ordinal, scaled from 1 to 4), `Income` (interval), and `Age` (interval). Imputation is not necessary for data MCAR because simple deletion leads to unbiased and therefore valid results. I subsequently increase the number of ordinal variables to be treated by `polr` in `hd.ord` by including `Interest`. Imputations are again conducted MAR and MNAR, this time for four amputed variables. The amputed variables are the same as before but omitting `Interest`. Next, I test the effect of increasing the amount of missing data to 50 percent. Finally, I compare the imputation runtimes for each method.








## Results {#ordmiss-results}

<!--
I didn't find any scenario where hd.ord actually did significantly and consistently better than amelia or mice. It comes close on occasion for some binary variables and MNAR, but not often and consistently enough. I told Jeff this, and he asked me to write the chapter along my findings -- that the assumption behind polr didn't pan out, at least not for missing data. Everything below is built around that argument.

If it should come up, maybe because a committee member is unhappy, who knows, these are half-hearted ideas I currently have that I might possibly still investigate if I had to:
-- Something with correlation. When I made the coding error that resulted in sampling from 85 observations, amelia was awful and hd.ord much better. Jeff also uses very high correlation in his hot.deck paper (I believe it was 0.8). Jeff also used data on modernization theory for 135 countries between 1950 and 1990, though. That data is very different from surveys and doesn't suffer from any external validity problems. I'm not sure this can be applied to surveys. I frame my stuff around surveys, and what good are artificial data with super high correlations that don't occur in the 'wild' in actual surveys?
-- My second idea is related to the first, I suspect: If hd.ord was close or good in my analyses so far, it was for framing. Why? I would guess that it has to do with correlations and external validity. My sample is markedly different from ANES or CCES when you look at the true variable means. The correlations are probably also very different. That would mean that hd.ord works well for non-externally valid samples -- and what good would that be, really?
-->



<!-- ---- MAIN TEXT ---- -->

<!-- MAR  -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- 12 variables: Dem, Male, Interest, Inc, Age, Black, Empl + 5 selected -->

<!-- MNAR -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- 12 variables: Dem, Male, Interest, Inc, Age, Black, Empl + 5 selected -->

<!-- INCREASED ORDINAL VARIABLES (DVs: educ, interest): -->
<!-- MAR -->
<!-- 4 variables: Dem, Male, Inc, Age -->
<!-- 11 variables: Dem, Male, Inc, Age, Black, Empl + 5 selected -->

<!-- MNAR -->
<!-- 4 variables: Dem, Male, Inc, Age -->
<!-- 11 variables: Dem, Male, Inc, Age, Black, Empl + 5 selected -->

<!-- INCREASED MISSINGNESS (MAR, 20, 50 percent): -->
<!-- Framing, 1000 iterations, 5 variables: Dem, Male, Interest, Inc, Age -->

<!-- SPEED: -->
<!-- MAR 5 variables, MAR 12 variables -->
<!-- MAR 20, 50 percent CCES -->



<!-- ---- APPENDIX ---- -->

<!-- ALL OBSERVATIONS ( section \ref{app-ordmiss-allObs}): -->
<!-- ANES all obs. 1000 iterations, CCES all obs. 10 iterations (maxed out RAM) -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR (Table \ref{mar.5var.all}) and MNAR (\ref{mnar.5var.all}) -->


<!-- INCREASED MISSINGNESS ( section \ref{app-ordmiss-increaseNA}): -->
<!-- CCES 10,000 iterations -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->
<!-- 20, 50 percent -->


<!-- SPEED ( section \ref{app-ordmiss-speed}): -->
<!-- CCES 10,000 iterations -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->
<!-- 20, 50 percent -->

<!-- ANES all obs. 1000 iterations, CCES all obs. 10 iterations (maxed out RAM) -->
<!-- 5 variables: Dem, Male, Interest, Inc, Age -->
<!-- MAR -->


<!--
Problem with CCES 1,000 n 10,000 its MAR 5 Var 20, 50 percent (which I had planned for the appendix):
The 20 percent worked fine on CO, but 50 percent failed because RAM was maxed out

Problem with CCES 5 Var all obs. (MAR + MNAR) (which I had planned for the appendix):
No more than 10 iterations are possible before RAM is maxed out, on Jeff and CO
-->




### MAR {#ordmiss-results-mar}

```{r MAR 5 Variables, include=FALSE}

mar.5var.anes <- read.csv("data/anes/mar/results/anes.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% addPlus
mar.5var.cces <- read.csv("data/cces/mar/results/cces.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% addPlus
# mar.5var.frame <- read.csv("data/framing/mar/results/framing.mar.results.5var.1000n.1000it.20perc.csv")  %>% .[,-1] %>% addPlus

mar.5var.anes[1:6, 2] <- mar.5var.cces[1:6, 2] <- rep("Democrat", 6)
mar.5var.anes[19:24, 2] <- mar.5var.cces[19:24, 2] <- rep("Income", 6)

mar.5var.anes$diff[mar.5var.anes$method == "true"] <- mar.5var.anes$value[mar.5var.anes$method == "true"]
mar.5var.cces$diff[mar.5var.cces$method == "true"] <- mar.5var.cces$value[mar.5var.cces$method == "true"]
# mar.5var.frame$diff[mar.5var.frame$method == "true"] <- mar.5var.frame$value[mar.5var.frame$method == "true"]

levels(mar.5var.anes$method) <- levels(mar.5var.cces$method) <- levs
# levels(mar.5var.frame$method) <- levs
  
mar.5var <- cbind(mar.5var.anes[, c(1,2,4)], mar.5var.cces[,4])
# mar.5var <- cbind(mar.5var.anes[, c(1,2,4)], mar.5var.cces[,4], mar.5var.frame[,4])
colnames(mar.5var) <- col.names

# to make the in-text citations shorter
mar.5.anes <- mar.5var$ANES
mar.5.cces <- mar.5var$CCES
# mar.5.frame <- mar.5var$Framing
mar.5.meth <- mar.5var$Method
mar.5.var <- mar.5var$Variable

tab.mar.5var <- stargazer(mar.5var, 
                          summary = FALSE,
                          align = TRUE,
                          header = FALSE,
                          rownames = FALSE,
                          digits = 4,
                          title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, MAR, 5 Variables with NA",
                          label = "mar.5var")

dt <- gsub("\\multicolumn{1}{c}{", "", tab.mar.5var, fixed = TRUE)
cat(dt)



```

<!--
MAR 5 Var
Explain vars: Dem and Male binary, Interest ordinal (scale 1-4), Inc and Age interval
	Binary
		hd.ord on par or worse than hot.deck for all three ds for both vars
		mice best overall across all ds and vars
		mice and amelia very close (.0001 difference)
		More difference for ANES (.0003 mice vs .0011 hd.ord Dem, .0001 mice vs. .0013 hd.ord Male) and Male CCES (.0001 amelia vs. .0014 hd.ord)
		Fewer difference for Dem CCES (.0001 amelia vs. .0004 hd.ord)
		hot.deck performs on par with mice and amelia for framing for both vars
		hd.ord further off for framing as well (.0008 vs. .0000 mice Dem; .0005 vs. .0000 amelia Male)
	Ordinal
		hd.ord worst for all three ds, with considerable distance to hot.deck (.0191 vs. .0130 ANES; .0196 vs. .0125 CCES; .0248 vs. .0213 framing)
		mice and amelia by far best across all ds
		Much larger performance difference between methods for ordinal than for binary: mice is not more than .0003 (Dem ANES) away from true value for all ds. hd.ord's max difference is .0248 (framing)
		Equal performance of mice and amelia, with the edge to mice because of CCES (.0000 vs. 0.0003)
	interval
		hd.ord worst for all three ds for both vars
		Again considerable distance to hot.deck
		mice best for Inc
		amelia best for Age
		Even larger performance difference between methods: mice is no more than .0014 (framing) away from true value across ds for Inc. hd.ord's max difference is .1278 (ANES)
		Same for amelia: amelia's max difference is .0039 (framing). hd.ord's is .4597 (ANES)
-->

This section shows the imputation results for the MAR missing data mechanism. MAR amputation was achieved by setting the `mech` argument in the `ampute` function to `MAR`. Table \ref{mar.5var} shows the results of imputing both data sets MAR for five amputed variables. For the two binary variables, `Democrat` and `Male`, `hd.ord` performs on par or worse than `hot.deck` for both data sets, while `mice` and `amelia` perform best. `hd.ord` is relatively close for CCES `Democrat` (`r mar.5.cces[mar.5.meth == "amelia" & mar.5.var == "Democrat"]` `amelia` vs. `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Democrat"]` `hd.ord`) but further away for ANES ( `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Democrat"]` `mice` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Democrat"]` `hd.ord` `Democrat`; `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Male"]` `mice` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` `hd.ord` `Male`) and CCES `Male` (`r mar.5.cces[mar.5.meth == "amelia" & mar.5.var == "Male"]` `amelia` vs. `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` `hd.ord`).



 \begin{table}[!htbp] \centering   
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, MAR, 5 Variables with NA}   
 \label{mar.5var}  
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l}  
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex]  
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex]  
 true & Democrat & 0.&3420 & 0.&3770 \\
 hd.ord & Democrat & --0.&0010 & +0.&0000 \\ 
 hot.deck & Democrat & --0.&0011 & --0.&0004 \\ 
 amelia & Democrat & +0.&0004 & +0.&0001 \\
 mice & Democrat & +0.&0003 & +0.&0002 \\
 na.omit & Democrat & --0.&0290 & --0.&0229 \\
 true & Male & 0.&4890 & 0.&4830 \\
 hd.ord & Male & --0.&0013 & --0.&0011 \\
 hot.deck & Male & --0.&0013 & --0.&0014 \\
 amelia & Male & +0.&0002 & --0.&0001 \\
 mice & Male & +0.&0001 & --0.&0001 \\
 na.omit & Male & --0.&0392 & --0.&0414 \\
 true & Interest & 2.&9340 & 3.&3290 \\
 hd.ord & Interest & --0.&0130 & --0.&0125 \\
 hot.deck & Interest & --0.&0191 & --0.&0196 \\ 
 amelia & Interest & +0.&0003 & +0.&0003 \\ 
 mice & Interest & +0.&0003 & +0.&0000 \\
 na.omit & Interest & --0.&0705 & --0.&0724 \\
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&1068 & --0.&0259 \\
 hot.deck & Income & --0.&1278 & --0.&0407 \\
 amelia & Income & +0.&0008 & --0.&0004 \\
 mice & Income & +0.&0003 & --0.&0002 \\
 na.omit & Income & --0.&5631 & --0.&2468 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&3888 & --0.&2616 \\
 hot.deck & Age & --0.&4597 & --0.&3895 \\
 amelia & Age & +0.&0007 & --0.&0033 \\
 mice & Age & +0.&0017 & --0.&0073 \\
 na.omit & Age & --1.&1875 & --1.&2361 \\
 \hline \\[-1.8ex]  
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 


For the ordinal variable, `Interest`, `hd.ord` performs worst for both data sets, with considerable distance to `hot.deck` (`r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` vs. `r mar.5.anes[mar.5.meth == "hot.deck" & mar.5.var == "Interest"]` ANES; `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` vs. `r mar.5.cces[mar.5.meth == "hot.deck" & mar.5.var == "Interest"]` CCES). `mice` and `amelia` perform by far best across both data sets. The performance differences between the methods are far larger for the ordinal than for the binary variables: `mice` is not more than `r mar.5.anes[mar.5.meth == "mice" & mar.5.var == "Interest"]` (ANES) away from the true value across both data sets, while the maximum difference for `hd.ord` amounts to `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Interest"]` (ANES).

For the interval variables, `Income` and `Age`, `hd.ord` also performs worst for both data sets. The distance to `hot.deck` is once more considerable. `mice` performs best for `Income` and `amelia` shows the best results for `Age`. The performance differences between the methods are even larger here: For `Income`, `mice` is not more than `r mar.5.cces[mar.5.meth == "mice" & mar.5.var == "Income"]` (CCES) away from the true value across both data sets, but the maximum difference for `hd.ord` is `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Income"]` (ANES). Similarly, `amelia`'s largest deviation from the true value for `Age` is `r mar.5.cces[mar.5.meth == "amelia" & mar.5.var == "Age"]` (CCES) as opposed to `hd.ord`'s `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Age"]` (ANES).^[For a repeat of this MAR analysis for 12 amputed variables, see appendix section \ref{app-ordmiss-12var}. The results do not change substantively.]








### MNAR {#ordmiss-results-mnar}

```{r MNAR 5 Variables, include=FALSE}

mnar.5var.anes <- read.csv("data/anes/mnar/results/anes.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mnar.5var.cces <- read.csv("data/cces/mnar/results/cces.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mnar.5var.frame <- read.csv("data/framing/mnar/results/framing.mnar.results.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mnar.5var.anes[1:6, 2] <- mnar.5var.cces[1:6, 2] <- rep("Democrat", 6)
mnar.5var.anes[19:24, 2] <- mnar.5var.cces[19:24, 2] <- rep("Income", 6)

mnar.5var.anes$diff[mnar.5var.anes$method == "true"] <- mnar.5var.anes$value[mnar.5var.anes$method == "true"]
mnar.5var.cces$diff[mnar.5var.cces$method == "true"] <- mnar.5var.cces$value[mnar.5var.cces$method == "true"]
# mnar.5var.frame$diff[mnar.5var.frame$method == "true"] <- mnar.5var.frame$value[mnar.5var.frame$method == "true"]

levels(mnar.5var.anes$method) <- levels(mnar.5var.cces$method) <- levs
# levels(mnar.5var.frame$method) <- levs

mnar.5var <- cbind(mnar.5var.anes[,c(1,2,4)], mnar.5var.cces[,4])
# mnar.5var <- cbind(mnar.5var.anes[,c(1,2,4)], mnar.5var.cces[,4], mnar.5var.frame[,4])
colnames(mnar.5var) <- col.names

# to make the in-text citations shorter
mnar.5.anes <- mnar.5var$ANES
mnar.5.cces <- mnar.5var$CCES
# mnar.5.frame <- mnar.5var$Framing
mnar.5.meth <- mnar.5var$Method
mnar.5.var <- mnar.5var$Variable

tab.mnar.5var <- stargazer(mnar.5var,
                           summary = FALSE,
                           align = TRUE,
                           header = FALSE,
                           rownames = FALSE,
                           digits = 4,
                           title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, MNAR, 5 Variables with NA",
                           label = "mnar.5var")

et <- gsub("\\multicolumn{1}{c}{", "", tab.mnar.5var, fixed = TRUE)
cat(et)


```

<!--
MNAR 5 VAR
Vars the same as for MAR 5 Var
	Binary
		Overall difference to true value much higher for all methods for all ds for all vars. Around .0100 for Dem ANES and CCES, around .004 for Dem framing. Around .0125 for Male across all ds. For comparison: Around .0005 for Dem and Male for all ds for MAR 5 Var.
		hd.ord is closer to amelia and mice than for MAR, sometimes more (.0133 hd.ord vs. .0132 amelia Male ANES), sometimes less (.0120 hd.ord vs. .0099 mice Dem ANES). hd.ord actually best of all methods for framing for both vars, but overall amelia and mice perform better
		Interesting: na.omit as good as amelia and mice for Male framing and rather close for the other vars and ds too
	Ordinal
		Consistent picture of overall much higher difference to true value. In terms of method performance, same picture as for MAR: hd.ord worst across all ds. 
		amelia and mice best by far, virtually identical, though with much higher differences than for MAR
		Interesting: na.omit close to hd.ord for framing. 
	interval
		Much higher differences to true value, as consistent for all MNAR results. Same picture in terms of method performance, similar to Ordinal
		Interesting: na.omit better than hd.ord for Age ANES. na.omit also better than hd.ord and hot.deck for Age framing
-->

This section shows the imputation results for the MNAR missing data mechanism. MNAR amputation was achieved by setting the `mech` argument in the `ampute` function to `MNAR`. All MAR and MNAR analyses are otherwise identical. Table \ref{mnar.5var} shows the results of imputing both data sets MNAR for five amputed variables. It is immediately noticeable that the differences between the methods' imputation results and the true values are much higher for all methods for all variables for both data sets. The results for `Democrat` and `Male`, for instance, hover around 0.0100 and 0.0125 for both data sets. In the corresponding MAR analysis, however, the results for `Democrat` and `Male` showed around 0.0005. 



 \begin{table}[!htbp] \centering 
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, MNAR, 5 Variables with NA}  
 \label{mnar.5var} 
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex]  
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex]  
 true & Democrat & 0.&3420 & 0.&3770 \\
 hd.ord & Democrat & --0.&0114 & --0.&0099 \\ 
 hot.deck & Democrat & --0.&0120 & --0.&0105 \\
 amelia & Democrat & --0.&0106 & --0.&0102 \\
 mice & Democrat & --0.&0099 & --0.&0101 \\ 
 na.omit & Democrat & --0.&0176 & --0.&0140 \\
 true & Male & 0.&4890 & 0.&4830 \\ 
 hd.ord & Male & --0.&0136 & --0.&0116 \\ 
 hot.deck & Male & --0.&0133 & --0.&0124 \\
 amelia & Male & --0.&0132 & --0.&0121 \\
 mice & Male & --0.&0132 & --0.&0120 \\
 na.omit & Male & --0.&0214 & --0.&0219 \\
 true & Interest & 2.&9340 & 3.&3290 \\ 
 hd.ord & Interest & --0.&0288 & --0.&0246 \\ 
 hot.deck & Interest & --0.&0335 & --0.&0296 \\
 amelia & Interest & --0.&0167 & --0.&0146 \\
 mice & Interest & --0.&0167 & --0.&0146 \\
 na.omit & Interest & --0.&0379 & --0.&0372 \\
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&2299 & --0.&0928 \\ 
 hot.deck & Income & --0.&2554 & --0.&1038 \\
 amelia & Income & --0.&1225 & --0.&0578 \\
 mice & Income & --0.&1229 & --0.&0566 \\
 na.omit & Income & --0.&2770 & --0.&1334 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&6319 & --0.&4596 \\ 
 hot.deck & Age & --0.&7415 & --0.&5929 \\ 
 amelia & Age & --0.&2450 & --0.&2266 \\
 mice & Age & --0.&2369 & --0.&2160 \\ 
 na.omit & Age & --0.&6427 & --0.&6392 \\
 \hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table}


For the binary variables, `hd.ord` performs more closely on par with `amelia` and `mice` than in the corresponding MAR analysis above, sometimes more (`r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]` `hd.ord` vs. `r mnar.5.anes[mnar.5.meth == "amelia" & mnar.5.var == "Male"]` `amelia` ANES `Male`) and sometimes less so (`r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Democrat"]` `hd.ord` vs. `r mnar.5.anes[mnar.5.meth == "mice" & mnar.5.var == "Democrat"]` `mice` ANES `Democrat`). The results for the ordinal variables confirm those of the MAR analysis: `hd.ord` represents the worst method across both data sets. `amelia` and `mice` show by far the best results and are virtually identical with each other, though the differences to the true values are much higher than in the MAR analysis, as is the case for the entire MNAR analysis. The results for the interval variables paint the same picture as the ordinal ones. `hd.ord` shows the worst performance. Note that `na.omit` performs equally well as `hd.ord` for ANES `Age`.^[For a repeat of this MNAR analysis for 12 amputed variables, see appendix section \ref{app-ordmiss-12var}. The results do not change substantively.]




### Increased Number of Ordinal Variables {#ordmiss-results-increaseOrd}

```{r MULT MAR 4 Variables, include=FALSE}

mult.mar.4var.anes <- read.csv("data/anes/mar/results/anes.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mult.mar.4var.cces <- read.csv("data/cces/mar/results/cces.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mult.mar.4var.frame <- read.csv("data/framing/mar/results/framing.mar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mult.mar.4var.anes[1:6, 2] <- mult.mar.4var.cces[1:6, 2] <- rep("Democrat", 6)
mult.mar.4var.anes[13:18, 2] <- mult.mar.4var.anes[13:18, 2] <- rep("Income", 6)

mult.mar.4var.anes$diff[mult.mar.4var.anes$method == "true"] <- mult.mar.4var.anes$value[mult.mar.4var.anes$method == "true"]
mult.mar.4var.cces$diff[mult.mar.4var.cces$method == "true"] <- mult.mar.4var.cces$value[mult.mar.4var.cces$method == "true"]
# mult.mar.4var.frame$diff[mult.mar.4var.frame$method == "true"] <- mult.mar.4var.frame$value[mult.mar.4var.frame$method == "true"]

levels(mult.mar.4var.anes$method) <- levels(mult.mar.4var.cces$method) <- levs
# levels(mult.mar.4var.frame$method) <- levs

mult.mar.4var <- cbind(mult.mar.4var.anes[, c(1,2,4)], mult.mar.4var.cces[,4])
# mult.mar.4var <- cbind(mult.mar.4var.anes[, c(1,2,4)], mult.mar.4var.cces[,4], mult.mar.4var.frame[,4])
colnames(mult.mar.4var) <- col.names

# to make the in-text citations shorter
mult.mar.4.anes <- mult.mar.4var$ANES
mult.mar.4.cces <- mult.mar.4var$CCES
# mult.mar.4.frame <- mult.mar.4var$Framing
mult.mar.4.meth <- mult.mar.4var$Method
mult.mar.4.var <- mult.mar.4var$Variable

tab.mult.mar.4var <- stargazer(mult.mar.4var, 
                               summary = FALSE,
                               align = TRUE,
                               header = FALSE,
                               rownames = FALSE,
                               digits = 4,
                               title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MAR, 4 Variables with NA",
                               label = "mult.mar.4var")

ft <- gsub("\\multicolumn{1}{c}{", "", tab.mult.mar.4var, fixed = TRUE)
cat(ft)


```

<!--
Increased Ordinal
Treats Education and Interest with polr
Also split into MAR/MNAR and 4/11 Variables
MAR 4 Var
Vars the same as for MAR and MNAR 5 Var, just without Interest (so no ordinal vars here)
	Binary
		hd.ord worst for Dem across all ds. hd.ord better than hot.deck for Male across all ds.
		amelia and mice best, virtually identical, often zero
		Results for hd.ord get slightly worse when compared to MAR 5 Var:
		MAR 5 Var hd.ord: Dem .0011, .0004, .0008. Male .0013, .0014, .0005
		MAR 4 Var hd.ord: Dem .0018, .0005, .0012. Male .0015, .0018, .0011		
	interval
		hd.ord worst for all ds for all vars
		amelia and mice far best. mice does better for Inc and Age CCES, amelia does better for Inc and Age framing. Both equally good for Inc and Age ANES
		Consistent with the binary changes from one to two ordinal variables, the results for hd.ord get slightly worse when compared to MAR 5 Var:
		MAR 5 Var hd.ord: Inc .1278, .0407, .0192. Age .4597, .3895, .3923
		MAR 4 Var hd.ord: Inc .1523, .0516, .0225. Age .5431, .4664, .4583
-->

This section shows the imputation results for an increased number of ordinal variables to be treated by `polr` in `hd.ord`. Specifically, I add `Interest` to the `ord` argument in `hd.ord`. The intuition behind this is a strengthening of the underlying latent continuous variable assumption. The results of the previous analyses do not show superior performance by `hd.ord`, but perhaps this might be due to a lack of 'influence' so far. Perhaps one ordinal variable treated with `polr` is not enough to manifest itself in improved results. By including another ordinal variable in the treatment, this 'influence' is strengthened and the `polr` assumption is put to another test. As in sections \ref{ordmiss-results-mar} and \ref{ordmiss-results-mnar}, imputations are conducted MAR and MNAR. Because `Interest` 'moves' to the `polr` treatment, the number of imputed variables is reduced to four and 11, respectively, to ensure accurate comparison. This means the amputed variables do not include an ordinal variable any more. The remaining variables are the same as before.

Table \ref{mult.mar.4var} shows the results of imputing both data sets with two `polr`-treated variables MAR for four amputed variables. `hd.ord` displays the worst results for `Democrat` across both data sets and beats only `hot.deck` for `Male`. `amelia` and `mice` perform best and show virtually identically results that often match the true variable means. A comparison with the MAR analysis of five imputed variables reveals that `hd.ord` consistently performs slightly worse here: `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Democrat"]` and `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Democrat"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Democrat"]` and `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Democrat"]` for `Democrat`; `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Male"]` and `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Male"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Male"]`and `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Male"]` for `Male`.

 \begin{table}[!htbp] \centering  
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MAR, 4 Variables with NA} 
 \label{mult.mar.4var} 
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex]
 true & Democrat & 0.&3420 & 0.&3770 \\
 hd.ord & Democrat & --0.&0008 & +0.&0002 \\
 hot.deck & Democrat & --0.&0018 & --0.&0005 \\
 amelia & Democrat & +0.&0002 & +0.&0001 \\
 mice & Democrat & +0.&0001 & +0.&0002 \\
 ha.omit & Democrat & --0.&0333 & --0.&0294 \\
 true & Male & 0.&4890 & 0.&4830 \\
 hd.ord & Male & --0.&0022 & --0.&0019 \\
 hot.deck & Male & --0.&0015 & --0.&0018 \\
 amelia & Male & +0.&0001 & +0.&0000 \\
 mice & Male & +0.&0000 & +0.&0000 \\ 
 ha.omit & Male & --0.&0396 & --0.&0407 \\
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&0830 & --0.&0246 \\
 hot.deck & Income & --0.&1523 & --0.&0516 \\
 amelia & Income & +0.&0010 & --0.&0006 \\
 mice & Income & --0.&0008 & +0.&0002 \\
 ha.omit & Income & --0.&5771 & --0.&2564 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&2889 & --0.&2350 \\
 hot.deck & Age & --0.&5431 & --0.&4664 \\
 amelia & Age & +0.&0018 & +0.&0085 \\
 mice & Age & +0.&0024 & --0.&0002 \\ 
 ha.omit & Age & --1.&1521 & --1.&1228 \\
 \hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 
 
 

`hd.ord` also performs worst for both data sets across both interval variables. `amelia` and `mice` again perform best. `amelia` and `mice` perform equally well for ANES, while `mice` does better for CCES. As for the binary variables, the results for `hd.ord` consistently get slightly worse in the switch from one to two ordinal variables in `polr`-treatment: `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Income"]` and `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Income"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Income"]` and `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Income"]` for `Income`; `r mult.mar.4.anes[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Age"]` and `r mult.mar.4.cces[mult.mar.4.meth == "hd.ord" & mult.mar.4.var == "Age"]` vs. `r mar.5.anes[mar.5.meth == "hd.ord" & mar.5.var == "Age"]` and `r mar.5.cces[mar.5.meth == "hd.ord" & mar.5.var == "Age"]` for `Age`.^[For a repeat of this MAR analysis for 11 amputed variables and two ordinal variables, see appendix section \ref{app-ordmiss-mult-11var}. The results do not change substantively.]






```{r MULT MNAR 4 Variables, include=FALSE}

mult.mnar.4var.anes <- read.csv("data/anes/mnar/results/anes.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mult.mnar.4var.cces <- read.csv("data/cces/mnar/results/cces.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mult.mnar.4var.frame <- read.csv("data/framing/mnar/results/framing.mnar.mult.results.4var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mult.mnar.4var.anes[1:6, 2] <- mult.mnar.4var.cces[1:6, 2] <- rep("Democrat", 6)
mult.mnar.4var.anes[13:18, 2] <- mult.mnar.4var.anes[13:18, 2] <- rep("Income", 6)

mult.mnar.4var.anes$diff[mult.mnar.4var.anes$method == "true"] <- mult.mnar.4var.anes$value[mult.mnar.4var.anes$method == "true"]
mult.mnar.4var.cces$diff[mult.mnar.4var.cces$method == "true"] <- mult.mnar.4var.cces$value[mult.mnar.4var.cces$method == "true"]
# mult.mnar.4var.frame$diff[mult.mnar.4var.frame$method == "true"] <- mult.mnar.4var.frame$value[mult.mnar.4var.frame$method == "true"]

levels(mult.mnar.4var.anes$method) <- levels(mult.mnar.4var.cces$method) <- levs
# levels(mult.mnar.4var.frame$method) <- levs

mult.mnar.4var <- cbind(mult.mnar.4var.anes[, c(1,2,4)], mult.mnar.4var.cces[,4])
# mult.mnar.4var <- cbind(mult.mnar.4var.anes[, c(1,2,4)], mult.mnar.4var.cces[,4], mult.mnar.4var.frame[,4])
colnames(mult.mnar.4var) <- col.names

# to make the in-text citations shorter
mult.mnar.4.anes <- mult.mnar.4var$ANES
mult.mnar.4.cces <- mult.mnar.4var$CCES
# mult.mnar.4.frame <- mult.mnar.4var$Framing
mult.mnar.4.meth <- mult.mnar.4var$Method
mult.mnar.4.var <- mult.mnar.4var$Variable

tab.mult.mnar.4var <- stargazer(mult.mnar.4var, 
                                summary = FALSE,
                                align = TRUE,
                                header = FALSE,
                                rownames = FALSE,
                                digits = 4,
                                title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MNAR, 4 Variables with NA",
                                label = "mult.mnar.4var")

gt <- gsub("\\multicolumn{1}{c}{", "", tab.mult.mnar.4var, fixed = TRUE)
cat(gt)


```

<!--
MNAR 4 Var
Vars the same as for MAR 4 Var
	Binary
		hd.ord equally close to amelia and mice as for MNAR 5 Var, sometimes more (.0131 hd.ord vs. .0130 mice Dem CCES), sometimes less (.0155 hd.ord vs. .0127 mice Dem ANES). hd.ord actually best of all methods for framing for both vars, but overall amelia and mice perform better
		na.omit not as close as for MNAR 5 Var
		hd.ord consistently gets slightly worse when compared to MNAR 5 Var:		
		MNAR 5 Var hd.ord: Dem .0120, .0105, .0033. Male .0133, .0124, .0125
		MNAR 4 Var hd.ord: Dem .0155, .0131, .0042. Male .0172, .0160, .0157
	interval
		Consistent with previous analyses. 
		na.omit better than hd.ord for Age for all three ds and for Inc ANES
		hd.ord consistently gets slightly worse when compared to MNAR 5 Var:		
		MNAR 5 Var hd.ord: Inc .2554, .1038, .0648. Age .7415, .5929, .7477
		MNAR 4 Var hd.ord: Inc .3174, .1303, .0812. Age .8997, .7259, .9349
-->

Table \ref{mult.mnar.4var} shows the results of imputing both data sets with two `polr`-treated variables MNAR for four amputed variables. For the binary variables, `hd.ord` performs on the same level as `amelia` and `mice` when compared to the MNAR analysis of five imputed variables with only `Education` treated by `polr`; sometimes more (`r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Democrat"]` `hd.ord` vs. `r mult.mnar.4.cces[mult.mnar.4.meth == "mice" & mult.mnar.4.var == "Democrat"]` `mice` CCES `Democrat`), sometimes less so (`r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Democrat"]` `hd.ord` vs. `r mult.mnar.4.anes[mult.mnar.4.meth == "mice" & mult.mnar.4.var == "Democrat"]` `mice` ANES `Democrat`). `na.omit` does not perform as well as it does in Table \ref{mnar.5var}. `hd.ord` again consistently performs slightly worse with the two-ordinal-variable-`polr`-treatment: `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Democrat"]` and `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Democrat"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Democrat"]` and `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Democrat"]` for `Democrat`; `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Male"]` and `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Male"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]` and `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Male"]` for `Male`.


 \begin{table}[!htbp] \centering
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MNAR, 4 Variables with NA} 
 \label{mult.mnar.4var} 
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline  
 \hline \\[-1.8ex]
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3420 & 0.&3770 \\ 
 hd.ord & Democrat & --0.&0142 & --0.&0133 \\ 
 hot.deck & Democrat & --0.&0155 & --0.&0131 \\
 amelia & Democrat & --0.&0136 & --0.&0131 \\
 mice & Democrat & --0.&0127 & --0.&0130 \\
 na.omit & Democrat & --0.&0211 & --0.&0185 \\
 true & Male & 0.&4890 & 0.&4830 \\
 hd.ord & Male & --0.&0180 & --0.&0162 \\
 hot.deck & Male & --0.&0172 & --0.&0160 \\
 amelia & Male & --0.&0170 & --0.&0154 \\
 mice & Male & --0.&0170 & --0.&0153 \\ 
 na.omit & Male & --0.&0233 & --0.&0241 \\
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&2481 & --0.&1034 \\ 
 hot.deck & Income & --0.&3174 & --0.&1303 \\
 amelia & Income & --0.&1555 & --0.&0741 \\
 mice & Income & --0.&1568 & --0.&0730 \\
 na.omit & Income & --0.&3114 & --0.&1513 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&6020 & --0.&4844 \\
 hot.deck & Age & --0.&8997 & --0.&7259 \\
 amelia & Age & --0.&3103 & --0.&2831 \\
 mice & Age & --0.&2994 & --0.&2702 \\
 na.omit & Age & --0.&6726 & --0.&6482 \\
 \hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 
 
 


The results for the interval variables are consistent with the previous analyses. In addition, note that `na.omit` performs better than `hd.ord` for `Age` in both data sets and for ANES `Income`. Once more, `hd.ord` consistently performs slightly worse with more than two ordinal variables: `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Income"]` and `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Income"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Income"]` and `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Income"]` for `Income`; `r mult.mnar.4.anes[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Age"]` and `r mult.mnar.4.cces[mult.mnar.4.meth == "hd.ord" & mult.mnar.4.var == "Age"]` vs. `r mnar.5.anes[mnar.5.meth == "hd.ord" & mnar.5.var == "Age"]` and `r mnar.5.cces[mnar.5.meth == "hd.ord" & mnar.5.var == "Age"]` for `Age`.^[For a repeat of this MNAR analysis for 11 amputed variables and two ordinal variables, see appendix section \ref{app-ordmiss-mult-11var}. The results do not change substantively.]



\clearpage

### Increased Percentage of Missingness {#ordmiss-results-increaseNA}


```{r Increasing Missingness Percentage Table, include=FALSE}

# There is currently no CCES 80 percent .csv file because polr is acting up. In case the committee wants that, I can address it then

mar.5var.cces.20 <- mar.5var.cces
mar.5var.cces.50 <- read.csv("data/cces/mar/results/cces.mar.results.5var.1000n.1000it.50perc.csv") %>% .[,-1] %>% addPlus
# mar.5var.cces.80 <- read.csv("data/cces/mar/results/cces.mar.results.5var.1000n.1000it.80perc.csv") %>% .[,-1] %>% addPlus

mar.5var.cces.50[1:6, 2] <- rep("Democrat", 6)
mar.5var.cces.50[19:24, 2] <- rep("Income", 6)

mar.5var.cces.20$diff[mar.5var.cces.20$method == "true"] <-
  mar.5var.cces.50$diff[mar.5var.cces.50$method == "true"] <-
  # mar.5var.cces.80$diff[mar.5var.cces.80$method == "true"] <-
  mar.5var.cces.20$value[mar.5var.cces.20$method == "true"]

levels(mar.5var.cces.20$method) <-
  levels(mar.5var.cces.50$method) <-
  # levels(mar.5var.cces.80$method) <-
  levs

mar.5var.cces.perc <- cbind(mar.5var.cces.20[,c(1,2,4)], mar.5var.cces.50[,4])
# mar.5var.cces.perc <- cbind(mar.5var.cces.20[,c(1,2,4)], mar.5var.cces.50[,4], mar.5var.cces.80[,4])
colnames(mar.5var.cces.perc) <- c("Method", "Variable", "20% NA", "50% NA")
# colnames(mar.5var.cces.perc) <- c("Method", "Variable", "20% NA", "50% NA", "80% NA")

# to make the in-text citations shorter
cces.NA20 <- mar.5var.cces.perc$`20% NA`
cces.NA50 <- mar.5var.cces.perc$`50% NA`
# cces.NA80 <- mar.5var.cces.perc$`80% NA`
cces.perc.meth <- mar.5var.cces.perc$Method
cces.perc.var <- mar.5var.cces.perc$Variable

tab.mar.5var.cces.perc <- stargazer(mar.5var.cces.perc,
                                    summary = FALSE,
                                    align = TRUE,
                                    header = FALSE,
                                    rownames = FALSE,
                                    digits = 4,
                                    title = "Accuracy of Multiple Imputation Methods for Increasing Percentages of Missingness. CCES Data, MAR, Five Variables with NA",
                                    label = "mar.5var.cces.perc")

ht <- gsub("\\multicolumn{1}{c}{", "", tab.mar.5var.cces.perc, fixed = TRUE)
cat(ht)


```


<!--
Increased Missingness
Framing MAR 5 Var
	Binary
		hd.ord worst for all percentages for all vars
		hot.deck outperforms amelia and mice for Dem and Male CCES
		amelia and mice virtually identical
	Ordinal
		hd.ord worst for all percentages for all vars
		amelia and mice far best and virtually identical
	interval
		hd.ord worst for all percentages for all vars
		amelia and mice far best
		amelia outperforms mice for all percentages for all vars	
-->

This brief section shows the imputation results when the percentage of missingness is increased. I conduct this for data MAR for five variables with the CCES data. Figures \ref{accuracy20} and \ref{accuracy50} show the results for 20 and 50 percent missingness, respectively.


```{r Increasing Missingness Percentage Plots, include=FALSE}

# 20 percent missingness
mar.5var.cces.20.plot <- mar.5var.cces.20[,1:3]
mar.5var.cces.20.plot$value <- as.numeric(mar.5var.cces.20.plot$value)
mar.5var.cces.20.plot.dem <- filter(mar.5var.cces.20.plot, variable == "Democrat")

yint <- mar.5var.cces.20.plot.dem$value[mar.5var.cces.20.plot.dem$variable == "Democrat" & mar.5var.cces.20.plot.dem$method == "true"]
var.filter <- filter(mar.5var.cces.20.plot.dem, variable == "Democrat", method != "true")
var.filter$method <- factor(var.filter$method, levels = c("hd.ord", "hot.deck", "amelia", "mice", "na.omit"))
plot.dem.20 <- ggplot(var.filter, aes(x=method, value)) + 
    geom_point(size = 3) + 
    geom_hline(yintercept= yint, linetype = "dashed", color = "red", size = 1) + 
    theme(axis.title = element_blank()) + 
    annotate(geom="text", x=4.4, y=yint+0.0025, label="True benchmark",
             color="red") + 
  ggtitle("Democrat") +
  theme(plot.title = element_text(hjust = 0.5))

mar.5var.cces.20.plot.other <- filter(mar.5var.cces.20.plot, variable != "Democrat")
vars.unique <- mar.5var.cces.20.plot.other$variable %>% unique
plot.list.20 <- list()
plot.list.20[[1]] <- plot.dem.20

for(i in 1:length(vars.unique)){
  yint <- mar.5var.cces.20.plot.other$value[mar.5var.cces.20.plot.other$variable == vars.unique[i] & mar.5var.cces.20.plot.other$method == "true"]
  var.filter <- filter(mar.5var.cces.20.plot.other, variable == vars.unique[i], method != "true")
  var.filter$method <- factor(var.filter$method, levels = c("hd.ord", "hot.deck", "amelia", "mice", "na.omit"))
  plot.list.20[[i+1]] <- ggplot(var.filter, aes(x=method, value)) + 
    geom_point(size = 3) + 
    geom_hline(yintercept= yint, linetype = "dashed", color = "red", size = 1) + 
    theme(axis.title = element_blank()) +  
    ggtitle(vars.unique[i]) +
    theme(plot.title = element_text(hjust = 0.5))
}


# 50 percent missingness
mar.5var.cces.50.plot <- mar.5var.cces.50[,1:3]
mar.5var.cces.50.plot$value <- as.numeric(mar.5var.cces.50.plot$value)
mar.5var.cces.50.plot.dem <- filter(mar.5var.cces.50.plot, variable == "Democrat")

yint <- mar.5var.cces.50.plot.dem$value[mar.5var.cces.50.plot.dem$variable == "Democrat" & mar.5var.cces.50.plot.dem$method == "true"]
var.filter <- filter(mar.5var.cces.50.plot.dem, variable == "Democrat", method != "true")
var.filter$method <- factor(var.filter$method, levels = c("hd.ord", "hot.deck", "amelia", "mice", "na.omit"))
plot.dem.50 <- ggplot(var.filter, aes(x=method, value)) + 
    geom_point(size = 3) + 
    geom_hline(yintercept= yint, linetype = "dashed", color = "red", size = 1) + 
    theme(axis.title = element_blank()) + 
    annotate(geom="text", x=4.4, y=yint+0.006, label="True benchmark",
             color="red") + 
  ggtitle("Democrat") +
  theme(plot.title = element_text(hjust = 0.5))

mar.5var.cces.50.plot.other <- filter(mar.5var.cces.50.plot, variable != "Democrat")
vars.unique <- mar.5var.cces.50.plot.other$variable %>% unique
plot.list.50 <- list()
plot.list.50[[1]] <- plot.dem.50

for(i in 1:length(vars.unique)){
  yint <- mar.5var.cces.50.plot.other$value[mar.5var.cces.50.plot.other$variable == vars.unique[i] & mar.5var.cces.50.plot.other$method == "true"]
  var.filter <- filter(mar.5var.cces.50.plot.other, variable == vars.unique[i], method != "true")
  var.filter$method <- factor(var.filter$method, levels = c("hd.ord", "hot.deck", "amelia", "mice", "na.omit"))
  plot.list.50[[i+1]] <- ggplot(var.filter, aes(x=method, value)) + 
    geom_point(size = 3) + 
    geom_hline(yintercept= yint, linetype = "dashed", color = "red", size = 1) + 
    theme(axis.title = element_blank()) +  
    ggtitle(vars.unique[i]) +
    theme(plot.title = element_text(hjust = 0.5))
}

```

```{r Increased-Missingness-20-Percent, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Accuracy of Multiple Imputation Methods for 20 Percent Missingness. CCES Data, MAR, Five Variables with NA. Y-Axis Shows Percentages/Means.\\label{accuracy20}"}

grid.arrange(grobs = plot.list.20, ncol = 3, nrow = 2)

```

```{r Increased-Missingness-50-Percent, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Accuracy of Multiple Imputation Methods for 50 Percent Missingness. CCES Data, MAR, Five Variables with NA. Y-Axis Shows Percentages/Means.\\label{accuracy50}"}

grid.arrange(grobs = plot.list.50, ncol = 3, nrow = 2)

```

`hd.ord` performs comparatively well for 50 percent missing data `Democrat` (`r cces.NA50[cces.perc.meth == "hd.ord" & cces.perc.var == "Democrat"]` vs. `r cces.NA50[cces.perc.meth == "amelia" & cces.perc.var == "Democrat"]` `amelia`) but falls short for `Male` (`r cces.NA50[cces.perc.meth == "hd.ord" & cces.perc.var == "Male"]` vs. `r cces.NA50[cces.perc.meth == "mice" & cces.perc.var == "Male"]` `mice`). `hd.ord` also represents the second-worst imputation method for both percentages for the ordinal and interval variables. `amelia` and `mice` show virtually identical results, are hardly affected by the increase in missingness, and far outperform the other methods.


<!--
Table \ref{mar.5var.cces.perc}: `hd.ord` performs comparatively well for 50 percent missing data `Democrat` (`r cces.NA50[cces.perc.meth == "hd.ord" & cces.perc.var == "Democrat"]` vs. `r cces.NA50[cces.perc.meth == "amelia" & cces.perc.var == "Democrat"]` `amelia`) but falls short for `Male` (`r cces.NA50[cces.perc.meth == "hd.ord" & cces.perc.var == "Male"]` vs. `r cces.NA50[cces.perc.meth == "mice" & cces.perc.var == "Male"]` `mice`).


 \begin{table}[!htbp] \centering 
 \caption{Accuracy of Multiple Imputation Methods for Increasing Percentages of Missingness. CCES Data, MAR, Five Variables with NA} 
 \label{mar.5var.cces.perc}
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{20\% NA} & \multicolumn{2}{c}{50\% NA} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3770 & 0.&3770 \\
 hd.ord & Democrat & +0.&0000 & --0.&0006 \\
 hot.deck & Democrat & --0.&0004 & --0.&0012 \\
 amelia & Democrat & +0.&0001 & +0.&0000 \\
 mice & Democrat & +0.&0002 & +0.&0002 \\ 
 na.omit & Democrat & --0.&0229 & --0.&0516 \\
 true & Male & 0.&4830 & 0.&4830 \\
 hd.ord & Male & --0.&0011 & --0.&0020 \\
 hot.deck & Male & --0.&0014 & --0.&0036 \\
 amelia & Male & --0.&0001 & --0.&0001 \\
 mice & Male & --0.&0001 & +0.&0000 \\
 na.omit & Male & --0.&0414 & --0.&1032 \\ 
 true & Interest & 3.&3290 & 3.&3290 \\
 hd.ord & Interest & --0.&0125 & --0.&0336 \\
 hot.deck & Interest & --0.&0196 & --0.&0538 \\
 amelia & Interest & +0.&0003 & +0.&0001 \\
 mice & Interest & +0.&0000 & --0.&0003 \\
 na.omit & Interest & --0.&0724 & --0.&2014 \\ 
 true & Income & 6.&4810 & 6.&4810 \\
 hd.ord & Income & --0.&0259 & --0.&0809 \\
 hot.deck & Income & --0.&0407 & --0.&1240 \\
 amelia & Income & --0.&0004 & +0.&0010 \\
 mice & Income & --0.&0002 & +0.&0019 \\
 na.omit & Income & --0.&2468 & --0.&5958 \\
 true & Age & 52.&8230 & 52.&8230 \\ 
 hd.ord & Age & --0.&2616 & --0.&7685 \\
 hot.deck & Age & --0.&3895 & --1.&1573 \\ 
 amelia & Age & --0.&0033 & --0.&0075 \\
 mice & Age & --0.&0073 & --0.&0137 \\
 na.omit & Age & --1.&2361 & --3.&1442 \\
 \hline \\[-1.8ex]  
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 
-->








### Speed {#ordmiss-results-speed}

```{r Runtimes 5 Variables MAR, include=FALSE}

run.5var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.5var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.5var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.5var <- cbind(run.5var.anes, run.5var.cces[,2])
colnames(run.5var) <- c("", "ANES", "CCES")

## Older table for 5var and 12var together
# run.5var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.5var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# # run.5var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.5var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.12var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.12var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# # run.12var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.5var.12var <- data.frame(matrix(NA, 4, 5))
# # run.5var.12var <- data.frame(matrix(NA, 4, 7))
# colnames(run.5var.12var) <- c("Method", "ANES5", "CCES5", "ANES12", "CCES12")
# # colnames(run.5var.12var) <- c("Method", "ANES5", "CCES5", "Framing5", "ANES12", "CCES12", "Framing12")
# run.5var.12var$Method <- run.5var.anes$V2
# 
# runs <- list(run.5var.anes, run.5var.cces,
#              run.12var.anes, run.12var.cces)
# # runs <- list(run.5var.anes, run.5var.cces, run.5var.frame,
# #              run.12var.anes, run.12var.cces, run.12var.frame)
# 
# for(x in 1:length(runs)){
#   run.5var.12var[1:4, x+1] <- runs[[x]][,2]
# }
# 
# run.meth <- run.5var.12var$Method
# 
# # so I can use how much slower amelia is than hd.ord
# var.hd.am.div <- sapply(2:5, 
#                     function(x)
#                       run.5var.12var[run.meth == "amelia", x] / 
#                       run.5var.12var[run.meth == "hd.ord", x]) 
# 
# # so I can use how much slower mice is than hd.ord
# var.hd.mi.div <- sapply(2:5, 
#                     function(x)
#                       run.5var.12var[run.meth == "mice", x] / 
#                       run.5var.12var[run.meth == "hd.ord", x]) 
# 
# stargazer(run.5var.12var, 
#           summary = FALSE,
#           align = TRUE,
#           header = FALSE,
#           rownames = FALSE,
#           title = "Runtimes of Multiple Imputation Methods (in Minutes) by Number of Imputed Variables. ANES and CCES Data",
#           label = "runtimes5var12var")
# 
# 
# var.hd.am.div %>% min %>% round(., digits = 1)
# var.hd.am.div 
# var.hd.mi.div %>% max %>% round(., digits = 1)
# var.hd.mi.div
# var.hd.mi.div %>% min %>% round(., digits = 1)
```

<!--Speed
Analyzed by number of imputed variables for all ds (MAR)
	hd.ord and hot.deck much faster and virtually identical
	amelia consistently several times slower for all ds for both numbers of imputed vars
	mice much, much worse than all other methods across the board
Analyzed by percentage of missingness for framing data (MAR 5 Var)
	amelia and mice get closer as the percentage of missingess increases-->

This section shows the running times for all methods. I outline the speed differences for both data sets (Table \ref{runtimes5var}) and by the percentage of missingness for the CCES data (Table \ref{run.cces.perc}). Both analyses are conducted MAR for five imputed variables. All running times are given in minutes, apply to all 1,000 imputation iterations combined, and were achieved on a Code Ocean AWS EC2 instance with 16 cores and 120 GB of memory.

Table \ref{runtimes5var} shows `hd.ord` and `hot.deck` with virtually identical running times for both data sets. This is to be expected as both methods are very similar in terms of their code build-up. More importantly, however, we observe that both methods are much faster than `amelia` and `mice`: `amelia` is `r (run.5var[3, "ANES"] / run.5var[1, "ANES"]) %>% round(., digits = 1)` times slower than `hd.ord` for the ANES data and `r (run.5var[3, "CCES"] / run.5var[1, "CCES"]) %>% round(., digits = 1)` times slower for the CCES data. `mice`, however, is by far the slowest method and takes `r (run.5var[4, "CCES"] / run.5var[1, "CCES"]) %>% round(., digits = 1)` (CCES) and `r (run.5var[4, "ANES"] / run.5var[1, "ANES"]) %>% round(., digits = 1)` (ANES) times as long as `hd.ord`.^[For the runtimes for 12 imputed variables, see appendix section \ref{app-ordmiss-speed-12var}. The results do not change substantively.]


```{r Runtimes Increased Missingness MAR 5 Variables Table, results='asis', echo=FALSE}

stargazer(run.5var,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes). ANES and CCES Data, MAR, 5 Variables with NA",
          label = "runtimes5var")

```

<!--
\begin{table}[!htbp] \centering 
  \caption{Runtimes of Multiple Imputation Methods (in Minutes) by Number of Imputed Variables. ANES and CCES Data} 
  \label{runtimes5var12var} 
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{5 Variables with NA} & \multicolumn{2}{c}{12 Variables with NA}\\ 
\cline{2-5}  \\[-1.8ex]
 & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES}\\ 
\cline{2-3} 
\cline{4-5}  \\[-1.8ex]
\hline \\[-1.8ex] 
\multicolumn{1}{c}{hd.ord} & 2.632 & 2.778 & 2.614 & 2.679 \\ 
\multicolumn{1}{c}{hot.deck} & 2.628 & 2.786 & 2.640 & 2.690 \\ 
\multicolumn{1}{c}{amelia} & 9.052 & 10.611 & 9.038 & 10.345 \\ 
\multicolumn{1}{c}{mice} & 50.445 & 57.205 & 104.143 & 113.390 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}
-->


```{r Runtimes Increased Missingness MAR Code, include=FALSE}

# There is currently no CCES 80 percent .csv file because polr is acting up. In case the committee wants that, I can address it then

run.cces.20 <- run.5var.cces
run.cces.50 <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.50perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.cces.80 <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.5var.1000n.1000it.80perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.cces.perc <- cbind(run.cces.20$V1, run.cces.50$V1) %>% as.data.frame %>% round(., digits = 2)
# run.cces.perc <- cbind(run.cces.20$V1, run.cces.50$V1, run.cces.80$V1) %>% as.data.frame %>% round(., digits = 2)
run.cces.perc$V3 <- run.cces.20$V2 %>% as.character
# run.cces.perc$V4 <- run.cces.20$V2 %>% as.character
run.cces.perc <- run.cces.perc[,c(3, 1:2)]
# run.cces.perc <- run.cces.perc[,c(4, 1:3)]
colnames(run.cces.perc) <- c("Method", "20% NA", "50% NA")
# colnames(run.cces.perc) <- c("Method", "20% NA", "50% NA", "80% NA")

perc.hd.am.div <- run.cces.perc[3,2:3] %>% as.numeric / run.cces.perc[1,2:3] %>% as.numeric
perc.hd.mi.div <- run.cces.perc[4,2:3] %>% as.numeric / run.cces.perc[1,2:3] %>% as.numeric


```


Table \ref{run.cces.perc} shows that `hd.ord` and `hot.deck` remain the fastest methods across both percentages of missingness, but the gap to `amelia` and `mice` narrows as the missingness increases. `amelia` improves from `r perc.hd.am.div %>% max %>% round(., digits = 1)` times slower than `hd.ord` for 20 percent missing data to `r perc.hd.am.div %>% min %>% round(., digits = 1)` times slower for 50 percent missing data. Similarly, `mice` speeds up from `r perc.hd.mi.div %>% max %>% round(., digits = 1)` to `r perc.hd.mi.div %>% min %>% round(., digits = 1)` times slower.



```{r Runtimes Increased Missingness MAR Table, results='asis', echo=FALSE}

stargazer(run.cces.perc,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes) by Percentage of Missingness. CCES Data",
          label = "run.cces.perc")

```


## Brief Evaluation of Shortcomings {#ordmiss-eval}

Given `hd.ord`'s inferior performance, we need to speculate about possible reasons. Perhaps the importance of the uneven distances between ordinal variable categories is over-emphasized in the literature, with the distances potentially being not as uneven as previously thought. The fact that `hd.ord` consistently performs worse when a second ordinal variable is added to the `polr` treatment seems to point in this direction. To investigate this possibility, I test the influence of `polr` on regression coefficient transformations with data from previous publications.

<!-- 
Background for this bit -- Jeff suggested this as part of his feedback:
	\begin{coi}
		\item Take research (Dataverse) from prominent research people: Michigan folks, Larry Bartels
		\item Take a glm regression (logit, probit, poisson model) with a RHS variable that is ordinal, perhaps scaled 1-7
		\item Throw away the outcome variable, put the ordinal variable as the DV
		\item Run polr and run linear regression on that
		\item What I'm looking for: How different are the regression coefficients (the RHS) between the two models? This difference shows how important the re-estimating of the ordinal spaces is
		\item If the results show no differences, this makes my speculation at the end, that the uneven spaces might not be that important after all, much stronger $\rightarrow$ and if the results show that there are actually big differences, this then excludes the this-isn't-that-important-after-all explanation as a possible reason why my analysis wasn't working $\rightarrow$ so it's a win-win situation no matter what the results show
	\end{coi}
-->

The first column of Table \ref{ordmiss-bartels-92} shows a replication of a linear model estimated by @bartels_1999_panel on the 1992 ANES data. Bartels regresses several explanatory variables on `Campaign Interest` (this estimation corresponds to the column "Panel" in Table 2 of the original publication). As we can see, the ordinal variable `Education` is part of the explanatory variables in the model. To test the transformations implemented by an ordered probit model for ordinal variables, I remove the current outcome variable (`Campaign Interest`) and replace it with `Education`. The resulting model is then estimated as a linear regression (column two) and an ordered probit regression (column three).


```{r lm and polr Differences on External Data, include=FALSE}

bart.92 <- read_sav("data/bartels/panel92p.sav")
# bart.96 <- read_sav("data/bartels/panel96p.sav")
# bart.96$V3b <- bart.96$V3^2

# all variables taken from data/bartels/var_descriptions.txt
bart.cols <- list(
  c("V4", "V3", "V5", "V6", "V7", "V13", "V16", "V17")
  # , c("V4", "V10", "V9", "V3", "V3b", "V8", "V6", "V12", "V18")
  )

bart.colnames <- list(
  c("Education", "Age", "Income", "Black", "Female", "Camp_int", "Part_strength", "Days_bef_elec")
  # , c("Education", "Rep_part", "Cons_ideol", "Age", "Age_squared", "Married", "Black", "Foll_pub_aff", "Clinton_morality")
  )

bart.dv.orig <- c("Camp_int"#, "Clinton_morality"
                  )
bart.dv.lm <- "Education"
dv.polr <- paste0(bart.dv.lm, ".fac")

bart.dfs <- list(bart.92#, bart.96
                 )
bart.dfs.sub <- list()
bart.evs.orig <- list()
bart.evs.new <- list()
bart.reg.res <- rep(list(list()), length(bart.dfs))

models <- c("orig", "lm", "polr")
years <- c(92#, 96
           )
mod.temp <- rep(list(list()), length(models))
names(mod.temp) <- models
bart.reg.res <- rep(list(mod.temp), length(bart.dfs))
names(bart.reg.res) <- years

for (n in 1:length(bart.dfs)){
  bart.dfs.sub <- subset(bart.dfs[[n]], subset = c(V1 == 1)) %>% .[bart.cols[[n]]]
  colnames(bart.dfs.sub) <- bart.colnames[[n]]
  bart.evs.orig <- colnames(bart.dfs.sub)[!colnames(bart.dfs.sub) %in% bart.dv.orig[n]]
  bart.dfs.sub[[dv.polr]] <- bart.dfs.sub[[bart.dv.lm]] %>% as.factor
  bart.evs.new <- bart.evs.orig[!bart.evs.orig %in% c(bart.dv.lm, dv.polr)]
  bart.reg.res[[n]][["orig"]] <- lm(paste(bart.dv.orig[[n]], paste(bart.evs.orig, collapse = " + "), sep = " ~ "),
                               data = bart.dfs.sub)
  bart.reg.res[[n]][["lm"]] <- lm(paste(bart.dv.lm, paste(bart.evs.new, collapse = " + "), sep = " ~ "),
                               data = bart.dfs.sub)
  bart.reg.res[[n]][["polr"]] <- polr(paste(dv.polr, paste(bart.evs.new, collapse = " + "), sep = " ~ "),
                                 data = bart.dfs.sub, Hess=TRUE)
  }

bart.92.lm <- bart.reg.res[["92"]][["lm"]] %>% 
  summary %>% 
  .$coefficients %>% 
  data.frame %>% 
  .[-1, 1:2]
bart.92.polr <- bart.reg.res[["92"]][["polr"]] %>% 
  summary %>% 
  .$coefficients %>%
  data.frame %>%
  .[1:nrow(bart.92.lm), 1:2]

xlab <- c("Age", "Income", "Black", "Female", "Partisan strength", "Days before election")

plots.92 <- list()
ov.perc <- c()
reps <- 100000

for (i in 1:nrow(bart.92.lm)){
  set.seed(126)
  lm.norm.92 <- rnorm(reps, mean = bart.92.lm[i,1], 
                      sd = bart.92.lm[i,2])
  polr.norm.92 <- rnorm(reps, mean = bart.92.polr[i,1], 
                        sd = bart.92.polr[i,2])
  
  # # to possibly add to plots
  # t.res <- t.test(lm.norm.92, polr.norm.92)
  # t.res %>% .$statistic
  # t.res %>% .$conf.int
  lm.92 <- cbind(lm.norm.92, rep("lm", reps)) %>% data.frame
  polr.92 <- cbind(polr.norm.92, rep("polr", reps)) %>% data.frame
  colnames(lm.92) <- colnames(polr.92) <- c("Coefficient", "Model")
  over.lap  <- list(lm.92$Coefficient %>% as.numeric, 
                    polr.92$Coefficient %>% as.numeric) %>% 
    overlap(.) %>%
    .$OV * 100
  ov.perc[i] <- round(over.lap, digits = 2)
  df.92 <- rbind(lm.92, polr.92)
  df.92$Coefficient <- df.92$Coefficient %>% as.numeric
  grob <- grobTree(textGrob(paste("Overlap:",  paste0(ov.perc[i], " %"), sep="\n"),
                            x=0.75,  y=0.75, hjust=0,
                            gp=gpar(col="red", fontsize=10)))
  if(i == 1){
    plots.92[[i]] <- ggplot(df.92, aes(x=Coefficient, fill=Model)) + 
      geom_density(alpha=0.2, aes(y=..density..), position="identity") + 
      xlab(xlab[i]) + 
      theme(axis.title.y=element_blank()) + 
      theme(legend.title=element_blank()) + 
      theme(legend.position = c(0.15, 0.75)) + 
      theme(plot.title = element_text(hjust = 0.5)) +
      annotation_custom(grob)
  } else{
    plots.92[[i]] <- ggplot(df.92, aes(x=Coefficient, fill=Model)) + 
      geom_density(alpha=0.2, aes(y=..density..), position="identity") + 
      xlab(xlab[i]) + 
      theme(axis.title.y=element_blank()) + 
      theme(legend.title=element_blank()) + 
      theme(legend.position = "none") + 
      theme(plot.title = element_text(hjust = 0.5)) +
      annotation_custom(grob)
  }
}

stargazer(bart.reg.res[["92"]][["orig"]],
          bart.reg.res[["92"]][["lm"]],
          bart.reg.res[["92"]][["polr"]],
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          dep.var.labels = c("Campaign Interest", "Education", "Education"),
          covariate.labels = c("Education", "Age", "Income", "Black", "Female", "Partisan strength", "Days before election"),
          title = "`lm` and `polr` Differences in 1992 ANES Data as Used by Bartels (1999)",
          no.space = TRUE,
          model.numbers = FALSE,
          star.char = c("", "", ""),
          label = "ordmiss-bartels-92",
          omit.table.layout = "n")

```


\begin{table}[!htbp] \centering 
  \caption{`lm` and `polr` Differences in 1992 ANES Data as Used by Bartels (1999)} 
  \label{ordmiss-bartels-92} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{1}{c}{Campaign Interest} & \multicolumn{1}{c}{Education} & \multicolumn{1}{c}{Education} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{\textit{OLS}} & \multicolumn{1}{c}{\textit{OLS}} & \multicolumn{1}{c}{\textit{ordered}} \\ 
 & \multicolumn{1}{c}{\textit{}} & \multicolumn{1}{c}{\textit{}} & \multicolumn{1}{c}{\textit{logistic}} \\ 
\hline \\[-1.8ex] 
 Education & 0.023^{} &  &  \\ 
  & (0.004) &  &  \\ 
  Age & 0.002^{} & -0.032^{} & -0.021^{} \\ 
  & (0.001) & (0.004) & (0.003) \\ 
  Income & 0.071^{} & 4.092^{} & 3.133^{} \\ 
  & (0.037) & (0.245) & (0.204) \\ 
  Black & -0.028 & -0.865^{} & -0.673^{} \\ 
  & (0.028) & (0.198) & (0.150) \\ 
  Female & -0.055^{} & -0.058 & -0.054 \\ 
  & (0.018) & (0.133) & (0.102) \\ 
  Partisan strength & 0.214^{} & 0.290 & 0.196 \\ 
  & (0.027) & (0.198) & (0.152) \\ 
  Days before election & -0.001^{} & 0.014^{} & 0.012^{} \\ 
  & (0.0005) & (0.004) & (0.003) \\ 
  Constant & 0.391^{} & -0.388 &  \\ 
  & (0.038) & (0.273) &  \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{1,359} & \multicolumn{1}{c}{1,359} & \multicolumn{1}{c}{1,359} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.114} & \multicolumn{1}{c}{0.248} &  \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.109} & \multicolumn{1}{c}{0.245} &  \\ 
Residual Std. Error & \multicolumn{1}{c}{0.331 (df = 1351)} & \multicolumn{1}{c}{2.401 (df = 1352)} &  \\ 
F Statistic & \multicolumn{1}{c}{24.750$^{}$ (df = 7; 1351)} & \multicolumn{1}{c}{74.507$^{}$ (df = 6; 1352)} &  \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}


To determine how statistically distinct the variables on the right-hand side in these models are (`Age`, `Income`, `Black`, `Female`, `Partisan strength`, `Days before election`), I simulate the posterior distribution of each $\bm{\beta}$ coefficient from both regressions in columns two and three as a normal distribution with mean $\bm{\hat{\beta}}$, standard error $SE(\bm{\hat{\beta}})$, and $n = 100,000$. I then plot the overlapping distributions of each linear regression coefficient with the corresponding ordered probit regression coefficient to assess the posterior percentage of overlay. The results are shown in Figure \ref{DensBart92}. 


```{r Density-Plots-Bartels-1992, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Distributions of `lm` and `polr` Coefficients in 1992 ANES Data as Used by Bartels (1999)\\label{DensBart92}"}

grid.arrange(grobs = plots.92, ncol = 2, left = "Density")

```

```{r Bartels 1996 data, results='asis', echo=FALSE}

# stargazer(bart.reg.res[["96"]][["orig"]],
#           bart.reg.res[["96"]][["lm"]],
#           bart.reg.res[["96"]][["polr"]],
#           align = TRUE,
#           header = FALSE,
#           rownames = FALSE,
#           title = "lm and polr Differences in 1996 ANES Data as used by Bartels (1999)",
#           dep.var.labels = c("Clinton Morality", "Education", "Education"),
#           covariate.labels = c("Education", "Republican partisanship", "Conservative ideology", "Age", "Age squared", "Married", "Black", "Follow public affairs"),
#           no.space = TRUE,
#           model.numbers = FALSE,
#           label = "ordmiss-bartels-96")

```


We observe a small posterior percentage of overlay for the distributions of `Age` and `Income` (< 10 percent) and a large posterior percentage of overlay for the distributions of `Black`, `Female`, `Partisan strength`, and `Days before election` (> 40 percent). Since a large percentage indicates little difference in significance between the two sets of coefficients, these results appear provide evidence against the importance of re-estimating ordinal variable categories with an ordered probit model. It seems that uneven distances between ordinal variable categories might not actually be of crucial importance when it comes to missing data imputation.



## Conclusion {#ordmiss-conclusion}

I set out to improve multiple imputation results with ordinal variables by accounting for the unevenly spaced ordering contained in ordinal variables. I did so by adapting the multiple hot deck imputation function `hot.deck` to treat ordinal variables with an ordered probit model in order to estimate numerical thresholds from an assumed underlying latent continuous variable. The results clearly show diverging outcomes from the different algorithms, with each algorithm behaving differently under changing circumstances. 

`hd.ord` performs on par with some binary variables but overall worse than `amelia` and `mice` for data MAR with 5 variables with missing values. The results for the MNAR analyses paint a more mixed but overall unchanged picture. `hd.ord` performs somewhat better for binary variables but remains the least accurate method for interval and ordinal variables. Increasing the number of ordinal variables included in the `polr` treatment does not have a positive effect on the performance of `hd.ord`. In fact, `hd.ord` consistently performs slightly worse for both data sets for all mechanisms of missingness. Unlike `amelia` and `mice`, `hd.ord`'s performance also drastically worsens when the percentage of missingness in the data is increased to 50 percent.

On the positive side, `hd.ord` performs multiple imputation much more quickly: `amelia` and `mice` are at least `r (run.5var[3, "CCES"] / run.5var[1, "CCES"]) %>% round(., digits = 1)` and `r (run.5var[4, "CCES"] / run.5var[1, "CCES"]) %>% round(., digits = 1)` times slower than `hd.ord` for 20 percent of missing data, respectively. This speed gain is of dubious value, however. While it is necessary to iterate multiple imputation runs many times over for simulation purposes like this one, users likely will not do so, which greatly diminishes the computing time saved. Even if users do opt to run multiple imputation for 1,000 times, the differences in terms of absolute time for `amelia` are not so great as to be impractical. `amelia` took at most a little under `r run.5var[3, "CCES"] %>% round(., digits = 0)` minutes to compute 1,000 iterations of multiple imputation for 20 percent missing data, regardless of the data set in question. In absolute terms, this is not a lot of time for the general user to invest in data preparation. With a minimum of just over `r run.5var[4, "ANES"] %>% round(., digits = 0)` minutes, the same cannot be said for `mice`. Nonetheless, `hd.ord`'s speed gain over `amelia` cannot be considered enough reason to choose `hd.ord` over `amelia`, unless the data consists of exclusively binary variables where the differences between `hd.ord` and `amelia` are small.

Given all the above, the result of this quality comparison of major missing data solutions is a clear endorsement of `amelia`. It performs well for all types of variables in all stages of missingness and does so in a reasonably short amount of time. The combination of EM with bootstrapping clearly represents a great improvement in terms of speed over IP used in `mice`. While it offers a wealth of sophisticated options for specialized users, `amelia`'s default out-of-the-box settings are simple and intuitive for general users. On top of that, it is notable that `amelia` produces better results than `na.omit` when data is MNAR, i.e. it performs well in a setting it was not designed for. `amelia` thus represents the best `R` solution to problems of missing data for general users.

While `hd.ord` did not yield the desired improvements, my analysis provides in-depth insights into and corroborates the robustness of multiple imputation implementations like `amelia` and `mice` for a variety of variable types in survey settings.


<!--

What doesn't work

-- Increasing percentage of NAs == worse `hd.ord` performance
-- Ordinal and interval variables == worse `hd.ord` performance
-- High number of observations == reduced number of iterations (crashes and/or RAM maxing out)
-- High number of observations == makes `amelia` faster relative to `hd.ord`
-- 17 ANES education levels == increases needed number of iterations
-- 17 ANES education levels == causes `amelia` to stop on CO and Jeff
-- 10,000 iterations == results with 1,000 iterations are just as good
-- `ampute` with `bycases=FALSE` and `cont=FALSE` == worse `hd.ord` performance, good `na.omit` performance
-- `own.NA` == `na.omit` performs best (currently in appendix)
-- `own.NA.rows`== pretty much everything is zero, incl. `na.omit`; `mice` is awful (currently in appendix)
-- Running `hd.ord` with `method = p.draw` == worse `hd.ord` performance
-- Running `hd.ord` with `method = p.draw` == only works with only binary vars in the data
-- Increasing `sdCutoff` == only does something with only binary vars in the data
-- Only binary vars in data == no gain in `hd.ord` performance
-- amelia is very finicky when it comes to collinearity



What works

-- Results for binary variables == equal performance of `hd.ord`, `mice`, `amelia`
-- Increasing number of variables with NAs (all, not just binary) == better `hd.ord` performance
-- 1000 observations in data sets == increases `amelia` running time
-- MNAR == MAR in terms of `hd.ord` performance
-- Multiple ordinal variables == same performance as with one ordinal variable

?? mice.pdf: 6.2. Sensitivity analysis under MNAR

-->

<!--chapter:end:03-ordinal-missing.Rmd-->

# MORALITY, SELF-INTEREST, AND FRAME STRENGTH {#framing}

## Introduction {#framing-intro}

<!--
Barack Obama presented the first outline of the Affordable Care Act in the summer of 2009. The content of the reform was put online for everyone to see, but since the administration was still working on details, it refrained from actively communicating it. Published press releases simply stated that the ACA would expand coverage and lower healthcare costs for everyone. This hesitancy turned out to be a big mistake. At the end of July, support for the ACA hovered around 43 percent. Then Sarah Palin, John McCain's choice for running mate in 2008, posted the following statement on Facebook on August 7\textsuperscript{th}: "The America I know and love is not one in which my parents or my baby with Down Syndrome will have to stand in front of Obama's 'death panel' so his bureaucrats can decide, based on a subjective judgment of their 'level of productivity in society'" [@palin_statement_2009]. Palin implied that federal government workers would be able to refuse treatment to any patients and thus 'decide their fate'. Over the next two weeks, support for the ACA dropped to 35 percent while opposition rose to 52 percent. Republican lawmakers jumped at the opportunity and repeated the claim of 'death panels' whenever possible. The reform never recovered from this drop. In December 2009, four months after the statement, support and opposition were virtually identical to August. While the public was still uncertain about the exact contents of the law, Palin had asserted that it would include a Big Brother type panel that decided whether people would live or die. This drowned out any efforts by the Obama administration to show the law as a cost-reducing reform. Palin's frame of the ACA, in other words, drastically influenced public opinion of the reform.
-->

In today's world, we rarely consume information directly, for instance by attending a demonstration or listening to a talk. Instead, most of the information we consume is mediated through television, radio, blogs, social media, messenger apps, and so forth. Mediated information by definition only represents a selective fragment of reality. The messenger -- news anchors, pundits, influencers, politicians etc. -- makes conscious and unconscious choices what information to distribute and how to present this information to us. The mediated message that eventually reaches us has been manipulated, often not intentionally. Numerous strategies to carry out such manipulations exist. One of these strategies is framing. 

Framing is the practice of presenting an issue to affect the way people see it [@chong_framing_2007]. It reorganizes existing information already present in people's minds and attempts to direct people's attention towards particular considerations [@druckman_2003_framing]. Numerous experiments have shown that frames can have substantial influence on people's opinions [@chong_dynamic_2010;@sniderman_structure_2004]. Work remains, however, to uncover which types of frames have more influence than others. I provide an avenue of clarification by testing the influence of morality and self-interest in political framing.

For some time, scholars have postulated the 'death' of self-interest to explain issue positioning: People eschew their self-interest to defend and vote according to their morals instead, as they consider them more important [@frank_whats_2004; @haidt_2008_morality]. Others, however, cast doubt on these claims: Morality and cultural values do not outweigh self-interest concerns, which in turn represent a significant factor in determining issue positioning [@bartels_2005_homer; @bartels_2006_whats]. Which is it? Have moral concerns displaced self-interest? Does self-interest matter more than postulated by many? I intend to shed some light on these questions with an online framing survey experiment that directly juxtaposes morality and self-interest. Morality is assessed on the basis of Moral Foundations Theory [@haidt_2012_righteous]. Self-interest is defined to include any goals related to personal autonomy, health/safety, wealth, and status [@gintis_2017_individuality; @weeden_2017_self-interest]. Demographic information and conviction measurements are used to analyze the influence of each concept. To my knowledge, this setup has not been studied before. In addition to the substantive analysis, both methods developed in chapters \ref{ordblock} and \ref{ordmiss} are applied and their performance is analyzed.




## Theory {#framing-theory}

### Framing {#framing-theory-framing}

People usually do not have stable, consistent, and informed opinions [@converse_nature_1964; @zaller_nature_1992]. It is possible to influence people's opinion through communication. One of the ways to do so is framing, which is the practice of presenting an issue to affect the way people see it [@aaroe_investigating_2011;@druckman_evaluating_2001;@gross_framing_2008]. We learn about issues such as healthcare reform through articles, reports, speeches, commercials and social media. This mediated communication possesses tremendous potential influence on our perception of political issues [@iyengar_framing_1996;@kam_risk_2010;@tversky_framing_1981]. Framing research has established that a variety of frames substantively influence how people view and think about issues [@price_switching_1997;@andsager_how_2000;@callaghan_introduction_2005;@entman_framing_1993;@entman_projections_2004;@gamson_media_1989;@lahav_ideological_2012;@pan_framing_1993;@slothuus_political_2010;@sniderman_structure_2004;@vreese_effects_2004]. When frames influence people's opinion about an issue, we speak of a framing effect. Two different types of framing effects have evolved in the literature: equivalency and emphasis framing effects.

#### Equivalency Framing {#framing-theory-framing-equiv}

Equivalency framing involves phrasing the same logical content in different ways, i.e. "casting the same information in either a positive or negative light" [@druckman_2004_political, p. 671]. One example is @tversky_framing_1981's death and survival experiment (see page \pageref{death}): Presented with a hypothetical disease outbreak in the US that is expected to kill 600 people, respondents are asked to choose between alternating programs. The first group chooses between programs A and B. In program A, "200 people will be saved". In program B, "there is a 1/3 probability that 600 people will be saved and a 2/3 probability that no people will be saved". The second group chooses between programs C and D. In program C, "400 people will die". In program D, "there is a 1/3 probability that nobody will die, and a 2/3 probability that 600 people will die". The factual content given to both groups is of identical: In both groups, the first program results in the death of 400 people and the second program has a 66 percent chance of killing all 600 people. The difference is the light cast on this information. The frames given to group 1 focus on lives saved, whereas the frames presented to group 2 center around lives lost. Rationally, both are the same. However, this difference in equivalency framing matters: When the experiment was run, the majority in group 1 chose program A. The majority in group 2 chose program D. Framing thus greatly shifted support for the programs.

Other scholarly framing work focuses on policy evaluations, rather than risk preferences. For example, respondents are found to rate a proposed economic program more favorably when it is framed as resulting in 95 percent employment, instead of its logical equivalent of 5 percent unemployment [@quattrone_1988_contrasting]. Equivalency framing effects have been likened by some to survey question wording effects [@zaller_nature_1992]. Examples here are the alternate use of "not allow" and "forbid" in reference to Communist speech in the 1970s, which can arguably be described as substantively and logically equivalent [@bartels_2003_democracy, p. 61].

#### Emphasis Framing {#framing-theory-framing-emph}

Emphasis framing on the other hand stresses a particular aspect, viewpoint, or consideration about an issue in the attempt to get people to look at the issue in the proposed way. Describing the invasion of Iraq as freeing the population and gifting them democracy, as done by the Bush administration, for instance, emphasizes a very different viewpoint than a possible alternative narrative that focuses on the economic benefits of gaining access to oil fields and trade opportunities. Unlike equivalency frames, emphasis frames are not logically identical ways of phrasing the same issue. Instead, they highlight qualitatively different considerations of that issue [@druckman_implications_2001; @druckman_limits_2001; @kinder_1996_divided; @levin_1998_frames; @nelson_1996_issue; @sniderman_structure_2004], forcing people to think about the importance of the considerations suggested by the frame [@nelson_media_1997].

<!--
More recently, a third (sub-)strain in framing literature has emerged that aims at the separation of framing and persuasion in experimental framing research. It rejects 'classic' definitions of emphasis frames as "reducing a usually complex issue to one or two central aspects" [@nelson_media_1997, p. 568] or identifying "the essence of the issue" [@gamson_1987_changing, p. 143] in favor of "a message that provides an interpretation of an issue or policy by emphasizing which aspect of the issue is relevant for evaluating it, without the frame itself providing any new substantive information about the issue" [@leeper_2020_news, p.4; also see @price_switching_1997]. Whereas persuasion entails the provision of new information to change people's opinions, framing reorganizes existing information already present in people's minds and attempts to direct people's attention towards these particular considerations [@druckman_2003_framing]. Emphasis framing should thus be seen as emphasis of existing information only, in contrast to persuasion that aspires to change people's opinions by "the supply of arguments and evidence" [@kinder_20003_communication, p. 367]. Opinion change in framing thus occurs when people put more or less consideration on previously known aspects about an issue, whereas opinion change in persuasion results from the processing of new information.

We know that emphasis frames work because they move certain information to the top of people's minds [@zaller_nature_1992].
-->

My focus lies on emphasis frames. An abundance of experiments have shown that emphasis frames elicit significant changes in issue positioning [@chong_counterframing_2013;@chong_dynamic_2010;@druckman_2003_framing;@druckman_how_2013;@druckman_limits_2001;@druckman_source_2012;@nelson_media_1997;@slothuus_more_2008]. @brewer_values_2005, for instance, find significant effects for the frames "School vouchers create an unfair advantage" and "School vouchers provide help for those who need it". @druckman_source_2012 provide similar evidence for "The Affordable Care Act gives more people equal access to health insurance" and "The ACA increases government costs", while @druckman_how_2013 do so for "Oil drilling provides economic benefits" and "Oil drilling endangers marine life". Despite the mass of experimental framing research, however, we still have little insight into what makes an emphasis frame strong. We don't know why some emphasis frames elicit effects and others don't, i.e. we don't know why some frames are strong and other frames are weak. @chong_framing_2007 attempt a differentiation into weak and strong frames. They ask respondents in a pre-test what arguments the respondents consider strong or weak for a variety of chosen issues. Frames containing strong arguments are then deemed strong frames, while frames containing weak arguments are considered weak frames. These weak and strong frames are then used in the subsequent framing experiment. While it is no doubt laudable to explore which arguments in selected political issues are deemed more persuasive than others, this setup does not provide insights as to what actually makes a frame strong or weak. We simply know which arguments (to then be embedded in frames) are considered to be strong and weak, but not why. A major challenge for framing research thus still "concerns the identification of factors that make a frame strong" [@chong_framing_2007, p. 116]. 

In the attempt to address this challenge, scholars have increasingly adopted theory-driven approaches that deduce likely strong frames and subsequently empirically evaluate their theoretical expectations. @arceneaux_cognitive_2012 for instance theorizes that "individuals are more likely to be persuaded by political arguments that evoke cognitive biases" (p. 280) and asserts that messages which highlight out-group threats resonate with respondents to a greater extent. @druckman_framing_2011 report that adding factual information to messages about carbon nanotubes does nothing to enhance their strength and actually makes the message weaker. @feinberg_2013_moral assess whether frames are stronger when they cohere with an individual's personal value system. They frame environmental issues as a matter of moral 'purity', a theme that supposedly correlates with conservative ideology, and find this approach leads to increased conservative support of environmental policies. I attempt to provide a further avenue of clarification by investigating how morality and self-interest contribute to persuasive strength in emphasis framing.




### Morality {#framing-theory-morality}

<!--
What is morality? How are morals/values related to the concept of self? How are they related to political issues? What are the main morals in human interaction?

Conservatives want strong government regulation in personal behaviors and lifestyles (abortion, gay marriage), but autonomy in economics and public goods
Liberals want strong government regulation in economics and public goods (welfare, affirmative action), but autonomy in personal behaviors and lifestyles [@janoff-bulman_2013_surveying]

Political conservatism is based in avoidance motivation
Political liberalism is based in approach motivation

The idea of deciding up-front whether an issue is moral, i.e. examining how people respond in situations that meet pre-set definitions of morality, follows the tradition of moral judgment. In the realm of moral judgment, researchers decide, objectively along a set of criteria, whether an issue is moral. This stands in contrast to moral conviction. In moral conviction theory, morality lies in the eye of the beholder, i.e. people decide whether they consider an issue to be moral. Moral conviction "classifies stimuli as moral or non-moral as a function of individuals' perceptions of situations rather than according to characteristics of the situation" [@bauman_2009_mind, p. 342]. Rather than presupposing the morality of an issue externally, it is argued that people recognize a moral sentiment when they experience one, as it touches their perceived connection of what is right and wrong [@bandura_1986_social; @cervone_2004_architecture; @goodwin_2008_psychology; @skitka_2008_morality]. This recognition and perception, however, varies considerably depending on the person experiencing the potentially moral situation. Defining an issue or situation as moral then becomes a subjective endeavor for each individual person and their psychological reality [@haidt_2001_emotional; @haidt_2003_emotional; @morgan_2010_moral; @skitka_2008_moral]. What I consider to be moral might not be moral to you, and vice versa. The literature shows that moral conviction represents an important force that guides citizen behavior and development of public opinion [@converse_nature_1964;@skitka_moral_2005;@skitka_moral_2011;@smith_typologies_2002;@tatalovich_moral_2011;@zaller_nature_1992]. This has consequences for survey design. Rather than deciding pre-survey which issues are moral (moral judgment), researchers "directly assess the extent that a given issue engages moral conviction and then use those responses to classify whether situations are or are not moral for a specific individual" [@bauman_2009_mind, p. 343]. 

[@bandura_1986_social; ; @goodwin_2008_psychology]
[@morgan_2010_moral; @skitka_2008_moral]

Morality in the Western world focuses mostly on the ethic of autonomy (i.e. harming, cheating, or oppressing other people) [@haidt_2012_righteous].

Conservatives resist change, emphasize social order, see liberty as the right to be left alone (e.g. with regards to the environment or workers' rights), and define fairness in terms of proportionality (i.e. those who work hardest should be paid the most). 

Liberals embrace change, emphasize social justice, see liberty as the right of vulnerable populations (minorities, children, animals) to be defended by the government, and define fairness as equal provision for all. 

The strength of attitudes meaningfully differs when they are held with moral conviction [@baron_protected_1997;@bennis_costs_2010;@ditto_motivated_2009;@tetlock_correspondence_2003]. 

-->


Two of the main ingredients of public opinion are commitment to moral principles and interests that citizens see at stake [@kinder_1998_opinion]. Morality plays a role in establishing one's self-concept and identity. Moral arguments are ubiquitous in political issues because they are essential to how people perceive and make sense of the world around them [@cervone_2004_architecture;@frank_whats_2004;@mooney_public_2001;@skitka_2008_morality;@tatalovich_moral_1994]. They present themselves in the form of 'oughts' and 'shoulds' [@rokeach_1968_beliefs]. People feel that moral arguments (1) represent near-universal standards of truth, (2) are almost objective facts about the world, and (3) exist independent of institutional authority [@skitka_psychology_2010]. It is widely argued that people rely to a disproportionate extent on moral arguments to form their opinions since moral arguments achieve a high emotional connection due to invoked values and feelings [@bauman_2009_mind;@haidt_moral_2003;@ryan_no_2014;@ryan_reconsidering_2014;@skitka_moral_2005;@skitka_moral_2011;@smith_typologies_2002;@tatalovich_moral_2011]. Emotions play an important role in how people conceptualize moral stimuli since people feel emotionally committed to their values [@feinberg_2013_moral; @ryan_reconsidering_2014; @suhay_2008_group; @tetlock_1996_revising].
 
 
#### Moral vs. Non-Moral Issues {#framing-theory-morality-issues}

A lot of scholarly work tries to establish a distinction between moral and non-moral issues, yet agreement on the definition of a 'moral issue' remains elusive. To some, an issue is moral when at least one side sees the issue as threatening a core value/principle and/or uses moral arguments to support their position [@mooney_public_2001; @haider-markel_1996_politics]. To others, moral issues are based on values rooted deeply within people's belief systems [@biggers_2011_when; @glick_2001_physician-assisted]. Yet another strain sees morality as intrinsic to some issues, i.e. non-technical issues that are easy to understand or issues that concern fundamental aspects of life and death [@studlar_2001_what; @tavits_2007_principle]. Yet others again assert that economic issues are not to be considered moral [@abramowitz_1995_abortion; @engeli_2012_theoretical; @mooney_1995_legislative; @tatalovich_moral_2011] despite evidence to the contrary: @ryan_reconsidering_2014 finds that some people perceive distinctly economic issues such as labor relations or social security reform in moral ways. In addition, studies on abortion show that, despite widespread media depiction to the contrary, not everyone conceives of it as a moral issue [@mullen_2006_exploring; @skitka_2002_means; @skitka_moral_2005]. Assertions of moral and non-moral issues are misleading and unnecessary. Instead, we should concern ourselves with the moral and non-moral content of each respective issue. I thus eschew moral and non-moral issue definitions and focus on moral and self-interest frames within issues, rather than the moral nature of the issues themselves.

#### Moral Foundations Theory {#framing-theory-morality-mft}

In normative terms, we differentiate between prescriptive and proscriptive morality, which are based on approach-avoidance differences in self-regulation [@carnes_2012_harm]. Prescriptive morality is sensitive to positive outcomes and focused on what we should do. Proscriptive morality is sensitive to negative outcomes and focused on what we should not do [@janoff-bulman_2009_proscriptive]. Based on this categorization, Haidt and his colleagues developed Moral Foundations Theory (MFT), which encompasses cognitive foundations of moral matrices and has become widely adopted in moralization literature [@graham_2009_liberals; @haidt_2001_emotional; @haidt_2003_emotional; @haidt_2012_righteous; @haidt_moral_2003; @haidt_2004_intuitive; @haidt_2007_moral; @haidt_2007_synthesis; @haidt_2007_when; @haidt_2008_morality; @haidt_2010_morality; @hofmann_2014_morality; @schein_2017_theory; @clifford_2015_moral; @koleva_2012_tracing; @feinberg_2013_moral; @clifford_2013_words]. These foundations are shown in Table \ref{framing-foundations}.

\begin{table}[!htbp]
\caption{Foundations of Moral Intuitions}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{>{\itshape}l ll >{\itshape}l l}
\bottomrule 
\midrule
\multicolumn{2}{c}{\textbf{Positive}} & & \multicolumn{2}{c}{\textbf{Negative}}\\
\cmidrule{1-2}
\cmidrule{4-5}
Care & Cherishing, protecting others & & Harm & Hurting others\\
Fairness & Rendering justice by shared rules & & Cheating & Flouting justice/shared rules\\
Loyalty & Standing with your group & & Betrayal & Opposing your group\\
Authority & Submitting to tradition/authority & & Subversion & Resisting tradition/authority\\
Sanctity & Repulsion at disgust & & Degradation & Enjoyment of disgust \\
Liberty & Acting without constraint & & Oppression & Dominate/Constrain others\\
\bottomrule
\multicolumn{5}{l}{\footnotesize{Positive and negative foundations are conceptual opposites.}} \\
\end{tabular}}
\label{framing-foundations}
\end{table} 


Moral intuitions are a powerful psychological mechanism that underlies issue positions [@koleva_2012_tracing]. Haidt and his colleagues argue that they intuitions are rooted in evolved mechanisms. They propose that Care/Harm developed as the response to the adaptive challenge of protecting and caring for children. It makes us sensitive to signs of suffering and activates us to care for those who are being harmed. Fairness/Cheating developed as the response to acts of cooperation or selfishness that people show towards us. It makes us trust fair actors and punish cheaters. Loyalty/Betrayal developed to meet the adaptive challenge of forming and maintaining cohesive coalitions. It makes us reward loyalty and ostracize those who betray us. Authority/Subversion developed as the response to forging beneficial relationships within social hierarchies. It makes us sensitive to signs that people do not behave in accordance with their social position. Sanctity/Degradation developed as a response to the challenge of living in a world full of pathogens and parasites. It binds us into moral communities by shared notions of sacredness. Liberty/Oppression developed as a response to the challenge of living in small groups whose members can dominate and constrain us. It makes us value freedom and reject repression.

Because of the deep connection between morality and emotions, people have a tendency to internalize and defend their values. This "makes it very difficult for people to consider the possibility that there might really be more than one form of moral truth, or more than one valid framework for judging people or running a society" [@haidt_2012_righteous, p. 111]. Haidt distinguishes the use of these foundations by ideology. According to him, liberals tend to utilize a three-foundation morality, whereas conservatives allegedly lean towards using all six foundations. He also estimated the means of each moral foundation on a 6-point Likert scale ("Not at all relevant" to "Extremely relevant"). Liberals score 3.62 in Harm and 3.74 in Fairness but only 2.07 in Loyalty, 2.06 in Authority, and 1.27 in Sanctity. Conservatives score 2.98 in Harm, 3.02 in Fairness, 3.08 in Loyalty, 3.28 in Authority, and 2.89 in Sanctity [@graham_2011_mapping]. Additionally, both sides tend to focus on different aspects in the three foundations they share (Care/Harm, Fairness/Cheating, Liberty/Oppression): Conservatives are more likely to emphasize society's protection and security and aim to prevent losses and generally negative outcomes, while liberals tend to emphasize the provision of welfare for others and aim to advance gains and generally positive outcomes. Both are said to seek normatively 'good' outcomes for society, but with different orientations [@janoff-bulman_2009_provide; @janoff-bulman_2009_proscriptive; @janoff-bulman_2013_surveying].

While the liberal/conservative divide of course plays a role in opinion formation, it is not central to moral analyses. Liberals and conservatives are said to display differing tendencies towards each individual foundation, but the majority of people value all foundations. The foundations themselves, not party ID, are what matters most. I thus use the foundations as the building block of moral arguments in my juxtaposition of morality and self-interest.




### Self-Interest {#framing-theory-self_interest}

<!--
[@kahneman_fairness_1986; @weatherford_economic_1983]

Unenlightened self-interest [@bartels_2005_homer]: 
A majority of Americans supported the 2001 and 2003 Bush administration tax cuts, even though they were designed to disproportionately benefit the rich. Economic self-interest can't explain this, since the majority of Americans suffered economically from the cuts. As Bartels shows, neither can people's attitudes towards the tax burden born by the rich. 

It follows the idea that the individual and society both benefit most when each individual pursues his or her own interest. This assumption has been highly influential in many theories of human behavior, has pervaded models of neoclassical economics, political science, sociology, as well as philosophy, and is not restricted to particular issues. It presents self-interest as the potential cardinal human motive: "I assume that most people most of the time act in their own self-interest" [@fiorina_congress:_1977, p. 38].

@sears_role_1991 indeed suggest objective (amount of income, state of personal finances) and subjective (perception of the amount of taxes paid) parameters as the most suitable statistical measurements to accurately capture any evidence for self-interest-motivated human behavior.

On the other hand, others find that the unemployed actually do not favor policies to increase employment more than those with jobs [@schlozman_1979_injury], people with health concerns do not show stronger support for government health insurance than others [@sears_self-interest_1980], working women do not differ from homemakers in their attitudes towards benefits for women in the workplace [@sears_1990_origins], and pocketbook voting does not determine congressional votes [@kiewet_macroeconomics_1983].

The unemployed line up behind policies that help the unemployed, uninsured Americans favor a government role in health insurance, and African-Americans are the biggest supporters of race-based affirmative action [@weeden_2014_hidden]. 

"Thus, we think that its generally appropriate in studying self-interested political views to piece together a more objective view of respondents personal interests (through demographic and related measures) rather than asking them to directly characterize their motives or their views on who benefits from particular policies" [@weeden_2017_self-interest, p. 87].
-->


Self-interest refers to actions or behaviors that elicit personal benefit. When acting in self-interest, individuals look out for themselves and act on the grounds of personal gains [@arrow_1967_values; @downs_economic_1957; @ferejohn_paradox_1974; @fiorina_congress:_1977; @olson_logic_1965; @coleman_1986_individual; @riker_theory_1968; @sears_role_1991]. In neoclassical economics, this claim is extended to the overall postulation of human beings as \textit{homines economici} [@coleman_1986_individual]; creatures who act strictly rationally with the intent to maximize utility at all times. In political psychology, scholars instead focus on the importance of self-interest as a motivation for opinions on political issues.

In the 1960s, self-interest was considered a major determinant of opinions on political issues. @campbell_american_1960 for instance report that people with less income and education are more likely to support a strong government role in the provision of social welfare, stating that "people presented with certain policy alternatives can do a reasonable job of selecting responses that appear to further their self-interest" (p. 208). Empirical research from the 1970s and 1980s appears to confirm this [@batson_altruism_1991; @etzioni_moral_1988; @feldman_economic_1982; @feldman_economic_1984; @friedland_beyond_1989; @hirschman_against_1985; @kohn_brighter_1990; @lau_cognitive_1981; @lerner_belief_1980; @mansbridge_preface_1990; @miller_power_1996; @schwartz_battle_1986; @sears_tax_1985; @weatherford_economic_1983]. @hawthorne_individual_1987 find that income level and tax bracket significantly predict support for the tax cuts in the 1978 federal Tax Revenue Act. @lau_proposition_1983 identify a similar relationship for high taxpayers in their Massachusetts income tax study. @coughlin_economic_1990 discover that higher income citizens oppose redistribution slightly more than those with lower income. @sears_self-interest_1980 and @sears_inducing_1983 find evidence for pocketbook voting in their studies on government-guaranteed full employment. @courant_why_1980 discover modest but consistent effects for the perceived property tax burden in Michigan. @lau_fact_1990 find a significant relationship between perceived financial situation and anti-incumbent voting. The analysis by @sears_inducing_1983 reveals that perceived personal federal tax burden predicts support for the tax cuts specified in the 1979 Kemp-Roth bill. @bartels_2005_homer identifies a high subjective tax burden as the source of support for the 2001 and 2003 Bush tax cuts. @sears_tax_1985 similarly find that citizens with a high subjective tax burden acted out of self-interest in the so-called California Tax Revolt at the end of the 1970s.

Despite this evidence, an abundance of criticism has claimed the inadequacy of self-interest as a major predictor of human behavior. @miller_power_1996 call "homo economicus (...) a social construction, not a biological entity" (p. 45). Human behavior is argued to be too complex and much more than self-interest is claimed to come into play when people think about what they want in life [@batson_altruism_1991; @friedland_beyond_1989; @kohn_brighter_1990; @lerner_belief_1980; @mansbridge_preface_1990; @mansbridge_1990_relation; @schwartz_battle_1986; @sen_rational_1977; @tyler_1990_people]. The study of social movements, for instance, asserts clear evidence that motivations other than self-interest also majorly account for human behavior [@mansbridge_rise_1990]. The overall most important reasons for humans' choices are said to be affective and normative [@etzioni_moral_1988] and researchers are urged to shift their focus away from the obsession with self-interest and towards moral values [@hirschman_against_1985]. The dominant theories of motivation are said to be fundamentally based on personal gains and miss the influence that values and ideologies have on people's attitudes towards policies [@sears_self-interest_1990]. People are asserted as more interested in the fairness of procedures and policies than the self-beneficial outcomes of these processes [@brockner_2002_making; @hollander-blumoff_2008_procedural; @tyler_justice_1990]. When procedural fairness is high or perceived as being high, it is argued that outcome favorability is likely to decline as participants consider themselves heard and listened to [@gibson_2002_truth; @tyler_1981_influence; @vidmar_1990_origins].

Others critique that it is possible and plausible for self-interest to play a role in opinion formation, but even then it does not play the only role [@chong_when_2001; @citrin_1990_self-interest; @miller_norm_1999; @sears_self-interest_1990]. It is argued that self-interest is limited in explanatory scope [@chong_when_2001; @huddy_2013_from; @lewis-beck_2008_american; @kinder_1998_opinion; @lau_2030_self-interest; @sears_self-interest_1990] and only applies to short-term material gains [@sears_self-interest_1980]. As a result, self-interest is seen as negligible: "Many political scientists used to assume that people vote selfishly, choosing the candidate or policy that will benefit them the most. But decades of research on public opinion have led to the conclusion that self-interest is a weak predictor of policy preferences" [@haidt_2012_righteous, p. 85]. Furthermore, standard demographic measures such as education, income, race, and gender are not deemed adequate indicators of self-interest, thus discounting findings from the 1960s [@kinder_1998_opinion; @sears_self-interest_1990]. 

Countering such criticism, a different group of scholars assert that self-interest does indeed form a major predictor of human behavior. They argue that the self-interest criticism defines away any chance of self-interest being a major determinant of political opinion by restricting it to financial benefits and declaring ordinary demographic effects uninterpretable: Instead of a restricted categorization as purely economic gains, self-interest can be defined as "advancing any of a range of people's typical goals, whether directly involving material gain or not, whether involving immediate gain or something more subtle that advances someone's progress over the longer term" [@weeden_2014_hidden, p. 38]. Following evolutionary psychology, self-interest thus applies not only to economic aspects and material gains but also to various social and cultural domains [@buss_2015_handbook; @kurzban_2010_sex; @nteta_2013_united; @petersen_2016_evolutionary]. This includes emotions, which are evoked whenever a person's self-interest goals are advanced or threatened [@lazarus_1994_emotion]. Self-serving preferences range from having more money to gaining prestige, having sex, or wishing success for one's children, among many others [@becker_1996_accounting; @owens_2014_material; @weeden_2015_losing; @weeden_2016_people]. Self-interest is here defined as related to personal autonomy, health/safety, wealth, and status [@gintis_2017_individuality]. 

<!--
Demographics are argued to provide clues to particular outcomes that help or harm people's self-interests, which enables us to predict which opinion they have. By linking demographic measures with answers from the General Social Survey, @weeden_2014_hidden find strong links between self-interest and meritocracy, discrimination, sexual lifestyles, religiosity, abortion, and marijuana legalization. Crucially, not all demographics provide clues about all kinds of issues. Religion can provide insights into people's views on abortion, but is unlikely to do so for immigration. Household income can predict views on taxes but probably not on legalizing marijuana [@weeden_2017_self-interest]. From this perspective, self-interest arguably matters most for issues that greatly affect people's everyday interests [@descioli_2020_selfish; @kurzban_2010_everyone; @sznycer_2017_support]. Rather than focusing on material gains -- such as people's perceived tax burden [@bobo_whites_1983] -- @weeden_2014_hidden identify issues related to sex/reproduction (pre-marital sex, pornography, abortion, birth control access) and government spending on healthcare as prime candidates for a major influence of self-interest. Sexually conservative, married people with high regular church attendance can be predicted to be pro-life and against pre-marital sex because it aligns with their chosen lifestyle. Wealthy, white, heterosexual, Christian men can be predicted to oppose government redistribution of wealth as such a policy would endanger their own position. In both cases, it is in these individuals' self-interest to take these issue positionings because these issues directly relate to their lives.
-->

Defining self-interest in exclusively material terms seems needlessly restrictive, as is concluding that self-interest is not a valid predictor of human behavior. For one, empirical evidence indicates that self-interest at least plays some role for economic aspects. For another, arguing that self-interest does not matter because of its exclusive focus on material gains after having defined self-interest in precisely this way has the feel of a snake biting its own tail. Self-interest is unlikely to matter for many policy preferences when it is restricted to purely economic matters, but such a narrow definition appears unwarranted. Material gains are not all that humans desire. It is much more plausible that self-interest covers a range of people's self-advancing goals instead. As a result, I follow the definition of self-interest as one of personal autonomy, health/safety, wealth, and status.





## Design {#framing-design}

<!--
Framing on morals/values so far:
@slothuus_political_2010: how framing political messages in terms of values consistent with partisan motivations shapes acceptance of these messages
@wolsko_2016_white: conservatives who receive a message framed on the moral foundations that their in-group strongly endorses (loyalty, authority, sanctity) are likely to believe more in climate change 
@feinberg_2013_moral: reframing environmental messages in line with partisan moral values decreases polarization gap between liberals and conservatives

Applying Moral Foundation Theory, both frames in @brewer_values_2005 could be categorized as containing moral arguments, even though the authors do not explicitly do so. 'School vouchers create an unfair advantage' can be argued to contain the negative moral foundation of \textit{Cheating}, while 'School vouchers provide help for those who need it' contains the positive moral foundations of \textit{Care} and \textit{Fairness}. 'The ACA gives more people equal access to health insurance' [@druckman_source_2012] also could be said to contain the positive moral foundations of \textit{Care} and \textit{Fairness}, while 'Oil drilling endangers marine life' [@druckman_how_2013] contains the negative moral foundation of \textit{Harm}. 
In the case of @druckman_source_2012, 'The Affordable Care Act gives more people equal access to health insurance' contains a positive, i.e. support-inducing, moral argument, while 'The ACA increases government costs' contains a negative, i.e. opposition-inducing, non-moral argument. They act in opposite directions. A comparison of these frames alone does not yield sufficient results as we would not be able to identify the exact cause of the framing effect. Is 'The Affordable Care Act gives more people equal access to health insurance' strong because it supports the issue or because it contains a moral argument? Similarly, is 'The ACA increases government costs' strong because it opposes the issue or because it contains a non-moral argument? This set-up cannot answer these questions. 

\noindent "Self-interest is commonly defined as a short to medium-term impact on the material well-being of an individual's own personal life (or that of his or her immediate family)" [based on @sears_role_1991, p. 16].

\noindent "Morals concern what people should or should not do" -- 'Moral' description from Ryan or Skitka?. An action of yours is moral if it treats others the way you would like others to treat you (Dal Bo and Dal Bo 2014)

Following moral conviction theory, each respective respondent decides for themselves whether they consider each issue moral by answering moral conviction measures such as "To what extent is your opinion on [issue] a reflection of your core moral beliefs and convictions?" [based on @ryan_no_2014; @skitka_moral_2005; @skitka_psychology_2010]. Note that respondents 'rate' each issue in terms of morality but without actually stating their position on the issue. This is to avoid anchoring.

I design frames to correspond with the partisan messages identified by Haidt and assign them according to respondents' self-identification on party ID. This means a self-identified Democrat receives a fairness/cheating frame built on aspects of fairness/cheating that most apply to liberals. Conversely, a self-identified Republican receives a fairness/cheating frame designed to reflect fairness/cheating aspects most dear to conservatives. Independent respondents randomly receive the liberal or the conservative version.

Taxes, abortion, climate change, school choice, takings, same-sex marriage, collective bargaining rights, capital punishment, school vouchers, Black Lives Matter, social welfare/redistribution, gun control, animal testing, healthcare, urban growth/conservation, federal assistance to the poor, immigration, Patriot Act, gambling, Corona, social spending/reducing the deficit, minimum wage

Everyone pays taxes and can state how they feel about their tax burden. Respondents' self-identified income indicates which tax bracket they fall into and gives insights into their self-interest when presented with a fictitious tax policy (see the frames below). We can also assess perceived self-interest by asking "Do you feel you are asked to pay more than you should in federal income taxes, about the right amount, or less than you should?" (question wording from the ANES).

Like corporations and administrations, people need to maintain positive public images that are typically most effective when stories are spun to disguise these principal objectives [@weeden_2014_hidden]. People often take credit for having brought about good outcomes, and blame others, chance, fate, or the gods for bad outcomes [@kurzban_2010_everyone]. The result, when it comes to political opinions, is a messy combination of preferences for policies that advance people's own inclusive interests coupled with a stubborn set of talking points used to insist that, in fact, these policy preferences really just come from a well-reasoned concern for the welfare of everyone. I have the positions I have because I am a smart person and my favored policies are smart policies. I have the positions I have because I am a good person and my favored policies are good for society.
-->

Each of the previous areas has been studied, but they have not been applied together. An abundance of framing research exists and we know that emphasis frames can be successful in directing people's attention towards particular considerations of an issue, but we still don't know why some emphasis frames succeed while others fail. We know that morality plays a large role in people's issue positioning and that self-interest accounts for some human behavior, but we don't know how much self-interest and morality matter in direct comparison when people choose issue positions. I conduct a framing experiment that explores what we don't know by investigating how morality and self-interest contribute to persuasive strength in emphasis framing.

Juxtaposing morality and self-interest in frames follows the recognition that people don't just want to satisfy their interests, but neither do they completely set aside their self-regarding preferences in favor of altruistic motives [@stoker_1992_interest]. Both concepts are in constant movement, depending on the situation. People deal with the 'central problem of ethics': "how the lives, interests, and welfare of others make claims on us, and how these claims, of various forms, are to be reconciled with the aim of living our own lives" [@nagel_1970_possibility, p. 142]. Support for issues, for instance the welfare state, is based on "generalized and reciprocal self-interest" [@baldwin_1990_politics, p. 299], i.e. it depends on both value-based attitudes and immediate self-interest [for macro factors influencing welfare support, such as a country's social structure and major institutions, see @brooks_2007_welfare]. 

Opinions can be influenced by self-interest or alterations in empathy towards others [@hacker_insecure_2013; @rehm_2012_insecure]. It is often difficult to tell which is which: @frank_whats_2004 claims Republicans have duped rural and working-class Americans into voting against their self-interest by supporting Republican tax cuts that negatively affect them financially. @bartels_2005_homer however argues that they instead vote in unenlightened self-interest because they perceive their personal tax burden to be too high and (falsely) believe tax cuts can lower them. @haidt_2012_righteous, finally, states that they vote for their moral interest, as they don't want the country to devote itself primarily to the pursuit of social justice. It is difficult to assess which is correct.

Previous studies and experiments on moral framing that utilize MFT focus on partisan division [@clifford_2013_words; @feinberg_2013_moral; @koleva_2012_tracing; @slothuus_political_2010; @wolsko_2016_white]. Party ID is not central to analyses of moral concerns, though it undoubtedly matters in political analysis. I thus do not focus on party ID but instead utilize MFT to focus on the morality vs. self-interest juxtaposition within framing whilst also accounting for the influence of party ID as a possible moderator. I combine emphasis framing with aspects of MFT and the literature on self-interest.


### Issues and Frames {#framing-design-issues-frames}

Issues are selected from previous research and based on suitability to assess self-interest and morality. In order to assess self-interest, all issues need to apply directly to respondents' daily lives [@descioli_2020_selfish; @kurzban_2010_everyone; @sznycer_2017_support]. I design frames that propose new policies in the areas of healthcare and the environment. Everyone is affected by healthcare in their lives, so everyone has a vested self-interest in this issue. At the same time, how much the government should be involved in the provision of healthcare coverage is the subject of great debate. Similarly, everyone is affected by the environment, resulting in viable cause for self-interest, while the topic is also highly controversial in moral terms.

Each respondent is presented with one of five randomly assigned frames for each issue: an issue-supporting moral frame, an issue-opposing moral frame, an issue-supporting self-interest frame, an issue-opposing self-interest frame, or the control frame. The design consists of issue-supporting and issue-opposing frames in order to control for both signs in determining frame strength. The self-interest frames aim to activate each respondent's self-interest as identified by @weeden_2014_hidden. The moral frames are designed on the basis of MFT. For simplicity and to avoid cluttering, I only use the foundation that most applies to each issue and to the majority of people. For both issues, that is Care/Harm. Support/Opposition for/to the frame's content is assessed on a 5-point Likert scale (Q27 (healthcare) and Q30 (environment) in the questionnaire).^[The complete questionnaire in chronological order with all frames can be found in appendix section \ref{framing-questionnaire}.] The self-interest frames are based on prior research [@gerbasi_2013_self].

Each issue frame consists of an identical paragraph of information and a paragraph containing the morality/self-interest content. The frames for healthcare introduce a potential new healthcare plan that covers everyone in the US, is paid for with a mix of fees paid by individuals and employers as well as tax dollars, and provides free healthcare for all services and drugs to those over 65 and those with low income. The frames for the environment introduce potential new environmental regulations which restrict the use of toxic pesticides that might contaminate crops, soil, and ground water and that are particularly harmful to the elderly and those with various medical conditions. The environmental information paragraph also states that farmers receive state subsidies to cover parts of their increased costs as a result of these restrictions. The control frames contain just the introductory paragraph of information without any added content.

Prior to frame treatment, respondents answer a series of questions to measure their morality and self-interest levels. To measure morality, respondents are asked to identify the personal relevance of six questions on emotional suffering, caring for the vulnerable, cruelty, compassion, hurting animals, and killing human beings (Q4-Q9). These questions are taken from the Moral Foundations Questionnaire [@haidt_2012_righteous]. To measure self-interest, respondents similarly state the personal relevance of six statements about telling white lies, caring for friends and family over others, giving your own children advantages over others, choosing to kill over being killed, lying for your own good, and helping those who help you (Q10-Q15). These questions are taken from @raine_2019_selfishness. The questionnaire also collects standard demographic information (Q16-Q18, Q20-Q26) and gives respondents the chance to enter comments (Q33). The question inquiring after respondents' education (Q26) is split into two sets (ANES, ordered probit) of education categories to allow for a comparison of results. The survey starts with questions about respondents' online work as a means to ease entry into the survey (Q1-Q3). 


### Attention and Manipulation Checks {#framing-design-checks}

The questionnaire contains an attention check (Q19) which is used to filter out inattentive respondents as they answer the survey. The check asks respondents to identify the correct statement ("The letter B comes before the letter K in the alphabet") amongst a list of three false statements. If respondents fail to give the correct response, the website terminates the survey and their data is not processed. Respondents are not allowed to continue the survey and drop out of the sample. This happens after the morality and self-interest questions as part of the demographics and is treatment-independent. Respondents who fail the check drop out of the survey before they are assigned to treatment groups. Attention checks, or instructional manipulation checks, are commonly used to avoid respondent satisficing, i.e. selecting responses without paying attention to the instructions [@anduiza_2016_answering; @clifford_2014_there; @curran_2016_methods; @krosnick_1996_satisficing; @oppenheimer_2009_instructional]. The idea is to filter out inattentive respondents who provide low-quality data. 

The questionnaire also contains factual manipulation checks after each frame (Q29 and Q32). These checks inquire after the nature of the political topic just discussed. Respondents are asked to identify the correct issue (healthcare, environment) out of a list of seven political topics. Unlike the attention check, the factual manipulation checks do not determine whether respondents can continue the survey. All respondents who are presented with treatment frames remain part of the sample, regardless of their answers to the factual manipulation checks. The checks are not used to filter out respondents. Doing so could introduce post-treatment bias of causal effects [@montgomery_1962_conditioning]. In any randomized experiment, the ability to establish causality rests on the comparison of similar pre-treatment groups that act as each other's counterfactuals [@berinsky_2014_separating; @horiuchi_2007_designing]. Conditioning on a post-treatment manipulation check invalidates this comparison because we are then comparing two dissimilar groups. Conditioning on these variables can lead to sample imbalance regarding observed and unobserved confounders. Respondents who fail the check in one treatment group might not look like respondents who fail the check in a different group [@acharya_2016_explaining; @aronow_2019_note; @healy_2014_substituting]. Instead, these checks are solely used in post-experiment analysis to estimate any potential differences between attentive and less attentive respondents in terms of issue support [@kane_2019_harm]. All question wordings are based on the ANES, CCES, and GSS surveys.^[In line with modern social science transparency [@ioannidis_2005_most; @miguel_2014_promoting], the experimental design, the hypothesis, and the planned regression models were pre-registered with the Open Science Foundation prior to data analysis: [https://osf.io/tvpwq/](https://osf.io/tvpwq/)]






### Expectations {#framing-design-expectations}

<!--
-- We can test which issues are considered more moral than others

Persuasion literature asserts that people with strongly held attitudes are not easily persuaded with arguments to the contrary. This indicates that it is difficult to move the attitudes of respondents who show high scores on moralization or self-interest, no matter what frame they are presented with. This in turn suggests that we can only move the attitudes of people who don't hold their attitudes strongly, i.e. those who score in the lower range on moralization and self-interest. For the respondents with strongly held attitudes on the issue, their support/opposition for/to the issue should be statistically the same across all frames, since their attitudes are set and can't easily be moved. For the respondents without strongly held attitudes on the issue, the frames should move responses towards support or opposition, depending on the direction of the frame. For these people, we can explore whether the moral or the self-interest frames cause bigger shifts. 
-->

Moral arguments are said to relate more strongly than non-moral ones since they are deeply connected to emotions. Non-moral arguments, on the other hand, are said to be more loosely connected to emotions and thus deemed less powerful [@haugtvedt_1994_message; @johnson_1989_effects]. This would suggest that moral frames exert a stronger influence on respondents' issue positioning than self-interest frames. The effect of different frames should also be differently felt conditional on respondents' own moral and self-interest priorities. A moral frame should affect respondents' opinion more if they score highly on the moral measures as this type of frame is likely to connect more with their values. Likewise, a self-interest frame should register more strongly with respondents if they score highly on the self-interest measures. I thus post the following hypotheses:

\vspace{0.3cm}
\begin{adjustwidth*}{+1cm}{+1cm}
\noindent \textbf{H1.} Moral frames move people more than self-interest frames.\\
\noindent \textbf{H2.} Moral frames move people with higher morality scores more than people with lower morality scores.\\
\noindent \textbf{H3.} Self-interest frames move people with higher self-interest scores more than people with lower self-interest scores.\\
\end{adjustwidth*}




## Data {#framing-data}

### MTurk {#framing-data-mturk}

The data consist of a randomized survey experiment and an accompanying pre-test, both fielded online. Pre-testing frames with participants who are not part of the main survey experiment is crucial in framing analysis to test the mechanisms of the designed questionnaire [@carpini_1993_measuring;@stanley_2016_using;@conover_1991_nature]. The participants are exposed to the designed frames to test the core ideas behind morality and self-interest. This pre-test structure builds on work by @slothuus_political_2010, @chong_framing_2007 and the mass communication and persuasion literature [@okeefe_2002_persuasion]. The pre-test is carried out on Amazon's online platform MTurk. 

MTurk is a service where researchers can host tasks to be completed by anonymous participants. Participants receive financial compensation for their work and Amazon collects a commission. The use of MTurk in political science experiments has increased dramatically over the past decade due to its availability and cost efficiency [@hauser_attentive_2016]. The median hourly wage for an MTurk worker is \$1.38 [@paolacci_2010_running]. A short survey of up to 5 minutes commonly offers compensation around \$0.25 [@berinsky_evaluating_2012]. MTurk with its standing pool of participants is thus highly useful particularly to early career scholars who lack the funds for a traditional phone or in-person survey. 

While MTurk samples have been shown to be internally valid in survey experiments [@berinsky_evaluating_2012], caution is advised when it comes to external validity. MTurk samples have repeatedly been shown to be disproportionately male, atheist, white, liberal, and young [@clifford_2015_samples; @huff_who_2015]. While these differences usually do not amount to more than single-digit percentages, MTurk samples thus nonetheless suffer from bias. This limits their explanatory power in terms of national representation. Scholars widely agree, however, that MTurk provides a great source for pre-testing survey items and testing new ideas or concepts where external validity considerations do not come into play [@burnham_2018_mturk; @stritch_2017_opportunities; @coppock_2019_validating]. My pre-test collects a sample of 240 participants at a compensation of \$0.50 each. At a length of 15 minutes, it thus provides somewhat above average compensation.

### Lucid {#framing-data-lucid}

The experiment itself is carried out on Lucid. Lucid is an online marketplace aggregator of survey respondents from a variety of providers. Respondents receive compensation from the providers rather than from Lucid directly. Lucid's marketplace collects basic demographic information and matches US Census margins. Like MTurk, it is possible to access specific subsections of the population (only people of Hispanic origin, only bilingual speakers etc.) as well as the US population overall [@flores_2018_bilinguals]. Costs for researchers are slightly higher than on MTurk. A 5-minute survey that matches census demographics is charged at \$1 per completed response [@graham_2020_self-awareness].

Lucid outperforms MTurk in terms of external validity. Lucid respondents are more similar to US census benchmarks regarding demographic, political, and psychological attributes. They are also closer to ANES means on gender, education, age, income, and race than MTurk. While MTurk respondents skew towards being more liberal, Lucid respondents exhibit the same party ID averages as the ANES [@coppock_2019_validating]. Lucid has been shown to be reliable and performs well on a national scale in survey experiments [@coppock_2019_validating]. As any non-probability platform, however, the use of Lucid in social science research nonetheless has limitations. Studies estimating sample average treatment effects (SATEs) for instance replicate well on Lucid. The SATE represents the mean difference in the counterfactual outcomes for respondents. Research into other forms of estimates is limited, though, and it remains to be seen whether more complex analysis designs lead to reliable results when used with Lucid respondents. Since my experiment estimates SATEs, however, fielding it with Lucid for a random sample of US adults appears valid.

### Methods Application {#framing-data-methods}

Both methods from the previous chapters are applied in the fielding and analysis of the experiment. Respondents are randomly presented with either the ANES education categories set or the ordered probit (OP) education categories set (see Q26) and subsequently blocked into four treatment groups and one control group with the method from chapter \ref{ordblock}. This results in a set of results based on ANES blocking and a second set of results based on OP blocking. Blocking is necessary here in order to ensure balance on the potential outcomes. In small samples such as this, blocking is better suited to achieving balance than randomization, as it accounts for estimation errors in diverging levels of covariates in the treatment groups. We block on education as education is arguably the most important predictor in political science and thus crucial for the estimation of results. Both sets of blocked results are evaluated substantively to reveal the effect of morality and self-interest on respondents' issue positions. To assess framing effects, each treatment group in each set is contrasted with the frameless control group serving as the baseline. I estimate both Average Treatment Effects (ATEs) and conditional ATEs. The latter subsets the data into respondents with low/high morality/self-interest scores. 

In addition to the ordinal variable blocking method from chapter \ref{ordblock}, the ordinal variable imputation method from chapter \ref{ordmiss} is applied to artificially insert missing data. This allows us to assess the performance of the newly created imputation method with original data.

Popular survey design providers such as Qualtrics currently do not offer the functionality to include `R` code as the basis for randomization. In order to conduct this experiment based on my blocking method, I set up an online survey environment based on `R shiny` that is fed into Lucid's marketplace.^[Because of the use of this 'external' platform, Lucid's charge per completed response was \$2.] The environment uses Dropbox to store responses and blocking information. The process is explained in detail in section \ref{app-ordblock-env} of the appendix.




## Results {#framing-results}

### Survey Experiment {#framing-results-experiment}

```{r Framing-Experiment-ANES, include=FALSE}

cols.fac <- c("hc.likert", "ev.likert")
df.an <- read.csv("data/framing/experiment/an.clean.csv", stringsAsFactors = TRUE, check.names = FALSE)

for (i in 1:length(cols.fac)){
    df.an[,cols.fac[i]] <- factor(df.an[,cols.fac[i]])
}

# to extract the var names from the .csv question files
varNames <- function(df, varName){
  filter(df, id == varName) %>% 
    dplyr::select(., choiceNames) %>% 
    .[,1] %>% 
    strsplit(., ",") %>%
    .[[1]]
} 
questions.path <- "data/framing/questions"
questions.csv <- list.files(path = questions.path, pattern="*.csv")
names(questions.csv) <- questions.csv

q1.an <- read.csv(paste0(questions.path, "/", questions.csv["education.an.csv"])) # education.an for educ
var.educ.an <- varNames(q1.an, "educ")
df.an$educ <- factor(df.an$educ, levels = var.educ.an)

df.an$emply <- relevel(df.an$emply, "Unemployed") # to make Unemployed the base category

colnames(df.an)
vars.an <- c("hc.likert", "ev.likert", "hc.group", "ev.group",
             "emply", "Income", "Democrat", "Male",
             "educ")
# vars.an <- c("hc.likert", "ev.likert", "hc.group", "ev.group", 
#           "Employed" , "Retired" , "Student" , "Homemaker" , 
#           "Income" , "Democrat" , "Male" , "Moral conviction" , "Self-interest",
#           "1st-4th grade", "5th-6th grade", "7th-8th grade", "9th grade", 
#           "10th grade", "11th grade", "12th grade", "High school graduate", "Some college", 
#           "Associate degree", "Bachelor", "Master", "Professional degree", "Doctor")
names(vars.an) <- vars.an

create.formula.plr.out <- function(df, variables, treat){
  treat.dv <- paste0(treat, ".likert")
  if(treat == "hc"){
    other.dvs <- c("ev.likert", "ev.group")
  }else{
    other.dvs <- c("hc.likert", "hc.group")
  }
  aa <- df %>% subset(.,  select = variables) # select the vars
  ee <- c() # empty vector
  for(i in 1:ncol(aa)){ # identify columns with only one value (deficient)
    if(aa[, i] %>% unique %>% length == 1){
      ee[i] <- paste0(colnames(aa)[i])
      }
    }
  if(!is.null(ee)){ # if ee is not null, i.e. if there are deficient cols, remove NAs from vector, remove ee cols
    ee <- na.omit(ee)
    variables <- variables[!names(variables) %in% ee]
  }
  # exclude <- c(treat.dv, other.dvs)
  # df.tmp <- dplyr::select(df.an, -one_of(exclude))
  # vars.temp <- vars[!names(vars) %in% exclude]
  # df.num <- dplyr::select_if(tmp.df[, vars.temp], is.numeric) 
  # cor.vars <- cor(df.num) %>% abs() %>% findCorrelation(., cutoff = .5) %>% sort()
  # if(!is.null(cor.vars)){ # if ee is not null, i.e. if there are deficient cols, remove NAs from vector, remove ee cols
  #   vars <- vars[!names(vars) %in% names(df.num)[cor.vars]]
  # }
  variables <- variables[!names(variables) %in% other.dvs]
  form <- as.formula(df[,variables]) # save regression formula
  plr.out <- polr(form, data = df, Hess = TRUE)
  return(plr.out)
}

plr.out.an.hc <- create.formula.plr.out(df.an, vars.an, "hc")
plr.out.an.ev <- create.formula.plr.out(df.an, vars.an, "ev")

# tidy version with confidence intervals for plots
plot.an.hc <- tidy(plr.out.an.hc, exponentiate = TRUE, conf.int = TRUE) %>%
  .[1:(nrow(.)-4),] # results in NAs for conf ints and huge estimates for educ categories
plot.an.ev <- tidy(plr.out.an.ev, exponentiate = TRUE, conf.int = TRUE) %>%
  .[1:(nrow(.)-4),] # results in massive high conf ints for educ categories that completely ruin the scale

# adjust names (has to be done after tidy() because tidy() throws up an error otherwise)
vars.repl <- c("emply", "educ")
treat.names <- c("Moral opposing", "Moral supporting", "Self-interest opposing", "Self-interest supporting")

repl.names <- function(vec, vars.repl){ # remove the patterns in vars.repl and replace treat names
  new.names <- c(treat.names, # combine treatment names with other replaced names
                 str_remove_all(vec, paste(vars.repl, collapse = "|")) %>% 
                   .[5:length(.)]
                 )
  return(new.names)
}

tidy.names <- function(df, vec, vars.repl, plr.names){ 
  df[, vec] <- repl.names(df[[vec]], vars.repl) %>%
    factor(., levels = plr.names) # factorize replaced names for tidy plots
  df <- plyr::rename(df, replace = c("estimate" = "Exponentiated Coefficient")) # rename column for plot
  return(df)
}

names(plr.out.an.hc$coefficients) <- repl.names(names(plr.out.an.hc$coefficients), vars.repl)
names(plr.out.an.ev$coefficients) <- repl.names(names(plr.out.an.ev$coefficients), vars.repl)

# plot.an.hc <- tidy.names(plot.an.hc, "term", vars.repl) # no use since the code returns NAs
plot.an.ev <- tidy.names(plot.an.ev[1:12,], "term", vars.repl, plr.names = names(plr.out.an.ev$coefficients)) # included but only the non-education categories (otherwise the scale is useless)

```

```{r Framing-Experiment-OP, include=FALSE}

df.op <- read.csv("data/framing/experiment/op.clean.csv", stringsAsFactors = TRUE, check.names = FALSE)

for (i in 1:length(cols.fac)){
    df.op[,cols.fac[i]] <- factor(df.op[,cols.fac[i]])
}

q1.op <- read.csv(paste0(questions.path, "/", questions.csv["education.op.csv"])) # education.an for educ
var.educ.op <- varNames(q1.op, "educ")
df.op$educ <- factor(df.op$educ, levels = var.educ.op)

df.op$emply <- relevel(df.op$emply, "Unemployed") # to make Unemployed the base category

colnames(df.op)
vars.op <- c("hc.likert", "ev.likert", "hc.group", "ev.group",
             "emply", "Income", "Democrat", "Male",
             "educ")
# vars.op <- c("hc.likert", "ev.likert", "hc.group", "ev.group", 
#           "Employed" , "Retired" , "Student" , "Homemaker" , 
#           "Income" , "Democrat" , "Male" , "Moral conviction" , "Self-interest",
#           "High school or lower", "Some college", 
#           "Associate degree", "Bachelor", "Master or higher")
names(vars.op) <- vars.op

# logistic regressions
plr.out.op.hc <- create.formula.plr.out(df.op, vars.op, "hc")
plr.out.op.ev <- create.formula.plr.out(df.op, vars.op, "ev")

# tidy version with confidence intervals for plots
plot.op.hc <- tidy(plr.out.op.hc, exponentiate = TRUE, conf.int = TRUE) %>%
  .[1:(nrow(.)-4),]
plot.op.ev <- tidy(plr.out.op.ev, exponentiate = TRUE, conf.int = TRUE) %>%
  .[1:(nrow(.)-4),]

# adjust names (has to be done after tidy() because tidy() throws up an error otherwise)
names(plr.out.op.hc$coefficients) <- repl.names(names(plr.out.op.hc$coefficients), vars.repl)
names(plr.out.op.ev$coefficients) <- repl.names(names(plr.out.op.ev$coefficients), vars.repl)

plot.op.hc <- tidy.names(plot.op.hc, "term", vars.repl, names(plr.out.op.hc$coefficients))
plot.op.ev <- tidy.names(plot.op.ev, "term", vars.repl, names(plr.out.op.ev$coefficients))

```

```{r ANES-OP-Table, include=FALSE}

stargazer(plr.out.an.hc, plr.out.op.hc, plr.out.an.ev, plr.out.op.ev,
          header= FALSE,
          align = TRUE,
          single.row = TRUE,
          title = "Ordinal Logistic Regression Results",
          label = "exp-anes-op",
          star.char = c("", "", ""),
          omit.table.layout = "n",
          ci = TRUE,
          dep.var.labels = c("Healthcare", "Environment"),
          column.labels = c("ANES", "OP", "ANES", "OP"),
          column.sep.width = "-60pt",
          model.numbers = FALSE)

```


Table \ref{exp-anes-op} shows the results of ordinal logistic regressions for both issues, separated into the ANES and the OP sets. The regressions include control variables (e.g. `Democrat`) to improve precision. While random assignment alone is sufficient for large-sample experiments, this is not the case for a small $n$, which in turn makes it necessary to include controls. For ANES healthcare, the confidence intervals for most variables include the null. The exceptions are `Moral Supporting`, `Income`, and `Democrat`. Identifying as a Democrat leads to an increase in support for the healthcare policy (`r plr.out.an.hc$coefficients["Democrat"] %>% round(., digits = 3)`), as does receiving the `Moral Supporting` frame (`r plr.out.an.hc$coefficients["Moral supporting"] %>% round(., digits = 3)`). An increase in income leads to a decrease in support (`r plr.out.an.hc$coefficients["Income"] %>% round(., digits = 3)`). It is notable that the volume of education categories leads to very high education coefficients, which is likely due to a small number of observations in one of the categories. In the ANES environment regression, two variables exclude the null: Identifying as a Democrat increases support for the environmental policy (`r plr.out.an.ev$coefficients["Democrat"] %>% round(., digits = 3)`). Receiving the `Moral opposing` frame increases opposition (`r plr.out.an.ev$coefficients["Moral opposing"] %>% round(., digits = 3)`). The education coefficients are much smaller here, suggesting a better spread of observations across categories. All education confidence intervals nonetheless include the null.

For OP healthcare, the confidence intervals for the coefficients of `Moral Opposing` (`r plr.out.op.hc$coefficients["Moral opposing"] %>% round(., digits = 3)`), `Moral Supporting` (`r plr.out.op.hc$coefficients["Moral supporting"] %>% round(., digits = 3)`), `Self-interest opposing` (`r plr.out.op.hc$coefficients["Self-interest opposing"] %>% round(., digits = 3)`), `Democrat` (`r plr.out.op.hc$coefficients["Democrat"] %>% round(., digits = 3)`), and `Master or higher` (`r plr.out.op.hc$coefficients["Master or higher"] %>% round(., digits = 3)`) exclude the null. `Moral Supporting` thus shows a negative effect of support, i.e. an increase in opposition to the proposed healthcare policy. This goes against expectations, as the supporting frame was designed to increase support. In the OP environment regression, `Moral Opposing` (`r plr.out.op.ev$coefficients["Moral opposing"] %>% round(., digits = 3)`), `Student` (`r plr.out.op.ev$coefficients["Student"] %>% round(., digits = 3)`), `Democrat` (`r plr.out.op.ev$coefficients["Democrat"] %>% round(., digits = 3)`), and `Male` (`r plr.out.op.ev$coefficients["Male"] %>% round(., digits = 3)`) exclude the null.


\ssp

\footnotesize

\blandscape
\centering
\captionof{table}{Ordinal Logistic Regression Results\label{exp-anes-op}}
\begin{tabular}{@{\extracolsep{-60pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{2}{c}{Healthcare} & \multicolumn{2}{c}{Environment} \\ 
 & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{OP} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{OP} \\ 
\hline \\[-1.8ex] 
 Moral opposing &0.001$ $(-0.373$, $0.376) &-0.773^{}$ $(-1.121$, $-0.425) &-0.662^{}$ $(-1.035$, $-0.288) &-0.390^{}$ $(-0.730$, $-0.051) \\ 
  Moral supporting &0.481^{}$ $(0.124$, $0.838) &-0.352^{}$ $(-0.696$, $-0.007) &-0.128$ $(-0.503$, $0.247) &-0.222$ $(-0.571$, $0.127) \\ 
  Self-interest opposing &-0.053$ $(-0.385$, $0.279) &-0.578^{}$ $(-0.928$, $-0.228) &-0.207$ $(-0.575$, $0.161) &-0.270$ $(-0.614$, $0.073) \\ 
  Self-interest supporting &0.132$ $(-0.227$, $0.490) &-0.212$ $(-0.548$, $0.124) &0.292^{}$ $(-0.056$, $0.640) &-0.054$ $(-0.397$, $0.289) \\ 
  Employed full time &-0.022$ $(-0.416$, $0.371) &-0.126$ $(-0.482$, $0.230) &-0.122$ $(-0.515$, $0.272) & -0.068$ $(-0.425$, $0.289) \\ 
  Employed part time &0.072$ $(-0.363$, $0.508) & -0.223$ $(-0.625$, $0.180) & -0.165$ $(-0.600$, $0.271) & -0.030$ $(-0.434$, $0.375) \\ 
  Homemaker & -0.038$ $(-0.567$, $0.492) & -0.143$ $(-0.651$, $0.365) &0.275$ $(-0.271$, $0.820) & -0.151$ $(-0.658$, $0.356) \\ 
  Retired & -0.239$ $(-0.654$, $0.177) & -0.268$ $(-0.663$, $0.126) & -0.010$ $(-0.422$, $0.403) &0.301$ $(-0.096$, $0.699) \\ 
  Student & -0.184$ $(-0.828$, $0.459) & -0.411$ $(-0.968$, $0.145) & -0.500$ $(-1.158$, $0.158) & -0.732^{}$ $(-1.307$, $-0.158) \\ 
  Income & -0.083^{}$ $(-0.154$, $-0.012) & -0.062^{}$ $(-0.129$, $0.005) & -0.018$ $(-0.089$, $0.054) &0.032$ $(-0.035$, $0.099) \\ 
  Democrat & 1.135^{}$ $(0.896$, $1.374) & 1.124^{}$ $(0.893$, $1.355) &0.727^{}$ $(0.492$, $0.961) &0.794^{}$ $(0.568$, $1.020) \\ 
  Male & -0.075$ $(-0.305$, $0.155) & -0.181$ $(-0.404$, $0.042) & -0.105$ $(-0.334$, $0.124) & -0.381^{}$ $(-0.606$, $-0.156) \\ 
  1st-4th grade & 14.492^{}$ $(12.189$, $16.795) &  &0.332$ $(-4.322$, $4.987) &  \\ 
  5th-6th grade & 16.039^{}$ $(13.666$, $18.411) &  &0.487$ $(-4.099$, $5.074) &  \\ 
  7th-8th grade & 16.656^{}$ $(15.203$, $18.108) &  & 1.426$ $(-3.010$, $5.861) &  \\ 
  9th grade & 17.894^{}$ $(16.588$, $19.200) &  &0.217$ $(-4.110$, $4.544) &  \\ 
  10th grade & 17.849^{}$ $(16.649$, $19.050) &  &0.781$ $(-3.487$, $5.049) &  \\ 
  11th grade & 17.962^{}$ $(17.030$, $18.894) &  &0.660$ $(-3.531$, $4.850) &  \\ 
  12th grade & 17.221^{}$ $(16.375$, $18.067) &  &0.721$ $(-3.466$, $4.908) &  \\ 
  High school graduate & 17.777^{}$ $(17.393$, $18.162) &  & 1.157$ $(-2.934$, $5.248) &  \\ 
  Some college & 17.814^{}$ $(17.437$, $18.190) &0.214$ $(-0.109$, $0.536) & 1.397$ $(-2.692$, $5.485) &0.227$ $(-0.098$, $0.552) \\ 
  Associate degree & 17.888^{}$ $(17.453$, $18.322) &0.083$ $(-0.273$, $0.438) & 1.472$ $(-2.622$, $5.566) &0.136$ $(-0.222$, $0.493) \\ 
  Bachelor & 17.844^{}$ $(17.450$, $18.239) &0.064$ $(-0.264$, $0.392) & 1.639$ $(-2.445$, $5.724) &0.115$ $(-0.218$, $0.449) \\ 
  Master & 17.906^{}$ $(17.453$, $18.360) &  & 1.546$ $(-2.543$, $5.635) &  \\ 
  Professional degree & 17.560^{}$ $(16.828$, $18.292) &  & 1.306$ $(-2.830$, $5.442) &  \\ 
  Doctorate & 18.117^{}$ $(17.406$, $18.828) &  & 1.122$ $(-3.013$, $5.257) &  \\ 
  Master or higher &  &0.437^{}$ $(0.048$, $0.825) &  &0.139$ $(-0.245$, $0.523) \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{1,062} & \multicolumn{1}{c}{1,103} & \multicolumn{1}{c}{1,062} & \multicolumn{1}{c}{1,103} \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\fillandplacepagenumber
\elandscape

\dsp

\normalsize


Figures \ref{ScatExpOpHc}, \ref{ScatExpOpEv}, and \ref{ScatExpAnEv} display the same results from a different perspective. They show the exponentiated regression coefficients with their confidence intervals for the ANES environment, OP healthcare, and OP environment regressions. Exponentiated coefficients ease interpretations as they represent proportional odds ratios, which we can interpret like odds ratios from a binary logistic regression.

```{r Scatterplot-Exponentiated-OP-HC, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Exponentiated Coefficients with Confidence Intervals. OP, Healthcare\\label{ScatExpOpHc}"}

ggplot(plot.op.hc, aes(x=term, y=`Exponentiated Coefficient`)) + 
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), color = "blue") + 
  theme(axis.text.x = element_text(angle = 90), 
        axis.title.x=element_blank(),
        axis.ticks.x=element_blank()) +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r Scatterplot-Exponentiated-OP-EV, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Exponentiated Coefficients with Confidence Intervals. OP, Environment\\label{ScatExpOpEv}"}

ggplot(plot.op.ev, aes(x=term, y=`Exponentiated Coefficient`)) + 
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), color = "blue") + 
  theme(axis.text.x = element_text(angle = 90), 
        axis.title.x=element_blank(),
        axis.ticks.x=element_blank()) +
  theme(plot.title = element_text(hjust = 0.5))

```


In the OP healthcare plot, for respondents who received the healthcare `Moral Opposing` frame, the odds of supporting the policy are between `r ((1-plot.op.hc$conf.high[1])*100) %>% round` and `r ((1-plot.op.hc$conf.low[1])*100) %>% round` percent lower than for respondents in other treatment groups. Similarly, the odds of supporting the OP environment policy for Democrats are between `r plot.op.ev$conf.low[11] %>% round(., digits = 1)` and `r plot.op.ev$conf.high[11] %>% round(., digits = 1)` times those of Republicans and Independents. The exponentiated estimates for the education categories in ANES environment are so large that they are excluded from Figure \ref{ScatExpAnEv} to make the other variables visible. There is no figure for ANES healthcare because the very high education coefficients did not allow exponentiated estimations.

<!--
ANES hc: Moral supporting, Income (minus),  Democrat (plus)
ANES ev: Democrat (plus), Moral opposing
OP hc: Moral opposing, Moral supporting (wrong sign), Self-interest opposing, Democrat (plus), Master or higher (plus)
Op ev: Moral opposing, Student (minus), Democrat (plus), Male (minus)
-->

The results between the two sets of education categories and the two issues differ markedly. There is a positive effect for Democrats across all regressions, but this is hardly surprising since the proposed policies in both issues lean towards liberalism. `Moral Opposing` also shows consistent results across ANES environment, OP healthcare, and OP environment. There is little consistency, however, regarding the other variables. `Income` only shows significant results for ANES healthcare, where wealthier respondents are more likely to oppose the policy. `Moral Supporting` shows significant results for ANES healthcare and OP healthcare, but the sign in the latter is negative. `Self-Interest Opposing` and `Master or higher` are only significant for OP healthcare, while `Male` only significantly decreases support for the OP environmental policy.

Overall, the `Moral Supporting` frames do not move people more towards supporting the issue policies than the self-interest frames. The ANES healthcare `Moral Supporting` frame does elicit a stronger positive effect than the corresponding `Self-Interest Supporting` frame, but the `Moral Supporting` frames for ANES environment and OP environment include the null. `Moral Supporting` for OP healthcare even shows statistically significant negative effects. There is some evidence, however, that `Moral Opposing` frames move people more towards opposing the issue policies than the `Self-Interest Opposing` frames. The `Moral Opposing` frames in ANES environment, OP environment, and OP healthcare all show statistically significant negative effects that are larger than their self-interest counterparts. One could thus argue that the results partially confirm \textbf{H1} for opposing frames. These results do not change when we account for the influence of party ID as a possible moderator (see appendix section \ref{app-framing-pid} for details).


```{r Scatterplot-Exponentiated-AN-EV, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Exponentiated Coefficients with Confidence Intervals. ANES, Environment\\label{ScatExpAnEv}"}

ggplot(plot.an.ev, aes(x=term, y=`Exponentiated Coefficient`)) + 
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), color = "blue") + 
  theme(axis.text.x = element_text(angle = 90), 
        axis.title.x=element_blank(),
        axis.ticks.x=element_blank()) +
  theme(plot.title = element_text(hjust = 0.5))

```





#### High Morality {#framing-results-experiment-mor-high}

```{r Framing-Experiment-ANES-Moral-High-Low, include=FALSE}

mor.si <- c("Moral conviction", "Self-interest") # take out these vars here
vars.excl.mor.si.an <- vars.an[!names(vars.an) %in% mor.si]
df.an <- df.an %>% dplyr::mutate(mor.tercile = ntile(`Moral conviction`, 3)) # create column that orders moral conviction distribution into terciles

df.an %>% filter(., mor.tercile == 3) %>% nrow # 354 people
df.an.high.mor <- df.an %>% filter(., mor.tercile == 3) %>% dplyr::select(., -one_of(mor.si))
plr.out.an.high.mor.hc <- create.formula.plr.out(df.an.high.mor, vars.excl.mor.si.an, "hc")
plr.out.an.high.mor.ev <- create.formula.plr.out(df.an.high.mor, vars.excl.mor.si.an, "ev")
names(plr.out.an.high.mor.hc$coefficients) <- repl.names(names(plr.out.an.high.mor.hc$coefficients), vars.repl)
names(plr.out.an.high.mor.ev$coefficients) <- repl.names(names(plr.out.an.high.mor.ev$coefficients), vars.repl)

df.an %>% filter(., mor.tercile == 1) %>% nrow # 354 people
df.an.low.mor <- df.an %>% filter(., mor.tercile == 1) %>% dplyr::select(., -one_of(mor.si))
plr.out.an.low.mor.hc <- create.formula.plr.out(df.an.low.mor, vars.excl.mor.si.an, "hc")
plr.out.an.low.mor.ev <- create.formula.plr.out(df.an.low.mor, vars.excl.mor.si.an, "ev")
names(plr.out.an.low.mor.hc$coefficients) <- repl.names(names(plr.out.an.low.mor.hc$coefficients), vars.repl)
names(plr.out.an.low.mor.ev$coefficients) <- repl.names(names(plr.out.an.low.mor.ev$coefficients), vars.repl)

```

```{r Framing-Experiment-OP-Moral-High-Low, include=FALSE}

vars.excl.mor.si.op <- vars.op[!names(vars.op) %in% mor.si]
df.op <- df.op %>% dplyr::mutate(mor.tercile = ntile(`Moral conviction`, 3)) # create column that orders moral conviction distribution into terciles

df.op %>% filter(., mor.tercile == 3) %>% nrow # 367 people
df.op.high.mor <- df.op %>% filter(., mor.tercile == 3) %>% dplyr::select(., -one_of(mor.si))
plr.out.op.high.mor.hc <- create.formula.plr.out(df.op.high.mor, vars.excl.mor.si.op, "hc")
plr.out.op.high.mor.ev <- create.formula.plr.out(df.op.high.mor, vars.excl.mor.si.op, "ev")
names(plr.out.op.high.mor.hc$coefficients) <- repl.names(names(plr.out.op.high.mor.hc$coefficients), vars.repl)
names(plr.out.op.high.mor.ev$coefficients) <- repl.names(names(plr.out.op.high.mor.ev$coefficients), vars.repl)

df.op %>% filter(., mor.tercile == 1) %>% nrow # 368 people
df.op.low.mor <- df.op %>% filter(., mor.tercile == 1) %>% dplyr::select(., -one_of(mor.si))
plr.out.op.low.mor.hc <- create.formula.plr.out(df.op.low.mor, vars.excl.mor.si.op, "hc")
plr.out.op.low.mor.ev <- create.formula.plr.out(df.op.low.mor, vars.excl.mor.si.op, "ev")
names(plr.out.op.low.mor.hc$coefficients) <- repl.names(names(plr.out.op.low.mor.hc$coefficients), vars.repl)
names(plr.out.op.low.mor.ev$coefficients) <- repl.names(names(plr.out.op.low.mor.ev$coefficients), vars.repl)

```


In order to create a collective measure of morality, I take the average of all morality question responses. This results in one overall morality score for each respondent. Respondents with high morality scores are defined as respondents with an average morality score in the top-third of the morality score distribution. Respondents with low morality scores are defined as respondents with an average morality score in the bottom-third of the morality score distribution. This results in groups of comparable sizes.

<!--
HC
ANES
Moral opposing: High is null, Low is negative -> Fail to reject null hypothesis
Moral supporting: High is positive, Low is null -> Reject nh

OP
Moral opposing: High is negative, Low is null -> Reject nh
Moral supporting: Both are null -> Fail to reject nh

EV
ANES
Moral opposing: High is negative, Low is null -> Reject nh
Moral supporting: Both are null -> Fail to reject nh

OP
Moral opposing: High is negative, Low is null -> Reject nh 
Moral supporting: Both are null -> Fail to reject nh
-->

I run the same regression models as before after subsetting both data sets. Tables \ref{exp-hc-mor-high-low} and \ref{exp-ev-mor-high-low} show the results separated by issue. For healthcare, the confidence intervals for ANES high `Moral Opposing` and ANES low `Moral Opposing` include the null. ANES high `Moral Supporting` on the other hand is positive, while ANES low `Moral Supporting` again includes the null. \textbf{H2} claims that moral frames move people with higher morality scores more than people with lower morality scores. Based on the first finding, we fail to reject the null hypothesis for \textbf{H2}. Based on the second finding, however, we would reject it. This pattern continues for all estimates. For healthcare, OP high `Moral Opposing` is negative but OP low `Moral Opposing` includes the null, which means we would reject the null hypothesis. Both confidence intervals for OP high `Moral Supporting` and OP low `Moral Supporting` include zero, though, which means we would fail to reject the null hypothesis here. The same picture shows for environment: ANES high `Moral Opposing` is negative and ANES low `Moral Opposing` includes zero (i.e. reject the NH), but both CIs for `Moral Supporting` include the null (i.e. fail to reject the NH). Likewise, OP high `Moral Opposing` is negative and OP low `Moral Opposing` includes the null (i.e. reject), while both CIs for `Moral Supporting` again include the null (i.e. fail to reject). Results are thus mixed. The `Moral Opposing` frames appear to resonate more strongly with respondents with high morality scores, which would support \textbf{H2}, but the `Moral Supporting` frames mostly show null results, which would lead us to reject \textbf{H2}.


```{r Healthcare-Moral-High-Low-Table, include=FALSE}

stargazer(plr.out.an.high.mor.hc, plr.out.an.low.mor.hc, plr.out.op.high.mor.hc, plr.out.op.low.mor.hc,
          header= FALSE,
          align = TRUE,
          single.row = TRUE,
          title = "Ordinal Logistic Regression Results. High and Low Morality, Healthcare",
          label = "exp-hc-mor-high-low",
          star.char = c("", "", ""),
          omit.table.layout = "n",
          ci = TRUE,
          column.labels = c("ANES High", "ANES Low", "OP High", "OP Low"),
          column.sep.width = "-60pt",
          dep.var.labels = c("Healthcare"),
          model.numbers = FALSE)

```


\ssp

\footnotesize

\blandscape
\centering
\captionof{table}{Ordinal Logistic Regression Results. High and Low Morality, Healthcare\label{exp-hc-mor-high-low}}
\begin{tabular}{@{\extracolsep{-60pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{Healthcare} \\ 
 & \multicolumn{1}{c}{ANES High} & \multicolumn{1}{c}{ANES Low} & \multicolumn{1}{c}{OP High} & \multicolumn{1}{c}{OP Low} \\ 
\hline \\[-1.8ex] 
 Moral opposing & -0.508$ $(-1.167$, $0.151) &0.205$ $(-0.513$, $0.923) & -1.132^{}$ $(-1.747$, $-0.517) & -0.467$ $(-1.084$, $0.150) \\ 
  Moral supporting &0.760^{}$ $(0.114$, $1.406) &0.510^{}$ $(-0.083$, $1.103) & -0.452$ $(-1.069$, $0.165) & -0.236$ $(-0.869$, $0.397) \\ 
  Self-interest opposing & -0.00002$ $(-0.600$, $0.600) &0.013$ $(-0.552$, $0.578) & -0.917^{}$ $(-1.553$, $-0.282) & -0.463$ $(-1.102$, $0.177) \\ 
  Self-interest supporting &0.023$ $(-0.620$, $0.667) &0.044$ $(-0.577$, $0.665) & -0.223$ $(-0.801$, $0.355) & -0.265$ $(-0.877$, $0.348) \\ 
  Employed full time &0.245$ $(-0.444$, $0.934) & -0.078$ $(-0.819$, $0.664) &0.182$ $(-0.469$, $0.834) & -0.317$ $(-0.964$, $0.330) \\ 
  Employed part time &0.352$ $(-0.450$, $1.153) & -0.036$ $(-0.832$, $0.759) &0.152$ $(-0.609$, $0.913) & -0.402$ $(-1.115$, $0.311) \\ 
  Homemaker &0.324$ $(-0.608$, $1.255) & -0.164$ $(-1.116$, $0.788) &0.523$ $(-0.326$, $1.373) & -0.815^{}$ $(-1.719$, $0.090) \\ 
  Retired & -0.487$ $(-1.177$, $0.204) & -0.304$ $(-1.164$, $0.556) & -0.257$ $(-0.910$, $0.396) & -0.321$ $(-1.094$, $0.451) \\ 
  Student &0.191$ $(-1.281$, $1.664) &0.291$ $(-0.756$, $1.338) & -0.967$ $(-2.299$, $0.365) & -0.411$ $(-1.330$, $0.508) \\ 
  Income & -0.107$ $(-0.237$, $0.023) & -0.104$ $(-0.234$, $0.026) & -0.146^{}$ $(-0.264$, $-0.028) &0.054$ $(-0.074$, $0.182) \\ 
  Democrat & 1.131^{}$ $(0.712$, $1.550) &0.695^{}$ $(0.264$, $1.127) & 1.490^{}$ $(1.072$, $1.907) &0.773^{}$ $(0.365$, $1.180) \\ 
  Male &0.324$ $(-0.101$, $0.748) &0.021$ $(-0.396$, $0.439) &0.141$ $(-0.273$, $0.555) & -0.086$ $(-0.484$, $0.311) \\ 
  1st-4th grade &  & 16.438^{}$ $(14.044$, $18.832) &  &  \\ 
  5th-6th grade &  & 17.453^{}$ $(15.031$, $19.875) &  &  \\ 
  7th-8th grade & -4.926^{}$ $(-7.867$, $-1.984) & 19.385^{}$ $(17.444$, $21.326) &  &  \\ 
  9th grade & -0.920$ $(-3.610$, $1.771) & 19.260^{}$ $(17.638$, $20.882) &  &  \\ 
  10th grade & -1.688$ $(-4.164$, $0.787) & 19.459^{}$ $(17.063$, $21.854) &  &  \\ 
  11th grade & 11.629^{}$ $(11.629$, $11.629) & 18.819^{}$ $(17.482$, $20.156) &  &  \\ 
  12th grade & -2.848^{}$ $(-4.849$, $-0.848) & 19.667^{}$ $(18.382$, $20.952) &  &  \\ 
  High school graduate & -1.336^{}$ $(-2.892$, $0.221) & 19.456^{}$ $(18.894$, $20.018) &  &  \\ 
  Some college & -1.391^{}$ $(-2.916$, $0.134) & 18.834^{}$ $(18.281$, $19.386) &0.123$ $(-0.438$, $0.684) & -0.241$ $(-0.807$, $0.326) \\ 
  Associate degree & -0.916$ $(-2.493$, $0.661) & 19.268^{}$ $(18.597$, $19.939) &0.322$ $(-0.281$, $0.926) & -0.307$ $(-0.927$, $0.313) \\ 
  Bachelor & -1.481^{}$ $(-2.982$, $0.020) & 19.302^{}$ $(18.700$, $19.904) &0.213$ $(-0.372$, $0.798) & -0.369$ $(-0.974$, $0.235) \\ 
  Master & -1.158$ $(-2.765$, $0.449) & 19.332^{}$ $(18.647$, $20.017) &  &  \\ 
  Professional degree & -0.907$ $(-3.230$, $1.415) & 18.767^{}$ $(17.809$, $19.726) &  &  \\ 
  Doctorate &  & 19.392^{}$ $(18.302$, $20.482) &  &  \\ 
  Master or higher &  &  &0.747^{}$ $(0.001$, $1.492) & -0.084$ $(-0.793$, $0.626) \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{354} & \multicolumn{1}{c}{354} & \multicolumn{1}{c}{367} & \multicolumn{1}{c}{368} \\ 
\hline \\[-1.8ex] 
\end{tabular}
\fillandplacepagenumber
\elandscape

\dsp

\normalsize


```{r Environment-Moral-High-Low-Table, include=FALSE}

stargazer(plr.out.an.high.mor.ev, plr.out.an.low.mor.ev, plr.out.op.high.mor.ev, plr.out.op.low.mor.ev,
          header= FALSE,
          align = TRUE,
          single.row = TRUE,
          title = "Ordinal Logistic Regression Results. High and Low Morality, Environment",
          label = "exp-ev-mor-high-low",
          star.char = c("", "", ""),
          omit.table.layout = "n",
          ci = TRUE,
          column.labels = c("ANES High", "ANES Low", "OP High", "OP Low"),
          column.sep.width = "-60pt",
          dep.var.labels = c("Environment"),
          model.numbers = FALSE)

```


\ssp

\footnotesize

\blandscape
\centering
\captionof{table}{Ordinal Logistic Regression Results. High and Low Morality, Environment\label{exp-ev-mor-high-low}}
\begin{tabular}{@{\extracolsep{-60pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{Environment} \\ 
 & \multicolumn{1}{c}{ANES High} & \multicolumn{1}{c}{ANES Low} & \multicolumn{1}{c}{OP High} & \multicolumn{1}{c}{OP Low} \\ 
\hline \\[-1.8ex] 
 Moral opposing & -1.059^{}$ $(-1.706$, $-0.412) & -0.142$ $(-0.787$, $0.503) & -0.987^{}$ $(-1.613$, $-0.360) &0.200$ $(-0.405$, $0.805) \\ 
  Moral supporting & -0.333$ $(-1.017$, $0.351) &0.409$ $(-0.264$, $1.083) & -0.355$ $(-0.974$, $0.263) &0.285$ $(-0.335$, $0.906) \\ 
  Self-interest opposing & -0.121$ $(-0.780$, $0.538) &0.110$ $(-0.582$, $0.801) & -0.674^{}$ $(-1.283$, $-0.065) &0.163$ $(-0.453$, $0.780) \\ 
  Self-interest supporting &0.237$ $(-0.370$, $0.844) &0.732^{}$ $(0.087$, $1.377) & -0.606^{}$ $(-1.257$, $0.045) &0.649^{}$ $(0.029$, $1.270) \\ 
  Employed full time & -0.194$ $(-0.893$, $0.504) &0.067$ $(-0.647$, $0.781) &0.274$ $(-0.381$, $0.928) & -0.574^{}$ $(-1.219$, $0.071) \\ 
  Employed part time &0.237$ $(-0.597$, $1.071) & -0.019$ $(-0.787$, $0.749) &0.641^{}$ $(-0.104$, $1.387) & -0.797^{}$ $(-1.520$, $-0.075) \\ 
  Homemaker &0.510$ $(-0.450$, $1.470) &0.446$ $(-0.518$, $1.410) &0.496$ $(-0.368$, $1.359) & -0.884^{}$ $(-1.779$, $0.011) \\ 
  Retired & -0.329$ $(-1.027$, $0.370) & -0.059$ $(-0.884$, $0.766) &0.533$ $(-0.137$, $1.203) &0.179$ $(-0.582$, $0.940) \\ 
  Student & -0.507$ $(-2.029$, $1.015) &0.153$ $(-0.903$, $1.209) & -0.202$ $(-1.635$, $1.230) & -1.149^{}$ $(-2.096$, $-0.201) \\ 
  Income & -0.058$ $(-0.193$, $0.078) &0.006$ $(-0.122$, $0.134) &0.064$ $(-0.056$, $0.183) &0.100$ $(-0.026$, $0.225) \\ 
  Democrat &0.674^{}$ $(0.262$, $1.085) &0.295$ $(-0.133$, $0.722) &0.996^{}$ $(0.591$, $1.400) &0.331$ $(-0.070$, $0.732) \\ 
  Male &0.190$ $(-0.235$, $0.616) &0.035$ $(-0.378$, $0.447) &0.108$ $(-0.312$, $0.529) & -0.427^{}$ $(-0.834$, $-0.021) \\ 
  1st-4th grade &  &0.569$ $(-4.455$, $5.594) &  &  \\ 
  5th-6th grade &  & -0.191$ $(-5.112$, $4.730) &  &  \\ 
  7th-8th grade & -0.751$ $(-4.618$, $3.116) & 1.376$ $(-3.476$, $6.228) &  &  \\ 
  9th grade &0.125$ $(-3.216$, $3.465) & -0.529$ $(-5.271$, $4.214) &  &  \\ 
  10th grade &0.231$ $(-2.279$, $2.740) &0.033$ $(-4.892$, $4.958) &  &  \\ 
  11th grade &0.729$ $(-2.431$, $3.888) &0.221$ $(-4.531$, $4.973) &  &  \\ 
  12th grade & -0.041$ $(-2.081$, $1.999) & -0.067$ $(-4.690$, $4.555) &  &  \\ 
  High school graduate &0.492$ $(-1.103$, $2.087) &0.684$ $(-3.757$, $5.125) &  &  \\ 
  Some college &0.919$ $(-0.649$, $2.487) &0.709$ $(-3.731$, $5.150) &0.309$ $(-0.254$, $0.873) & -0.346$ $(-0.915$, $0.223) \\ 
  Associate degree &0.854$ $(-0.766$, $2.474) &0.918$ $(-3.534$, $5.370) &0.152$ $(-0.465$, $0.769) & -0.036$ $(-0.662$, $0.589) \\ 
  Bachelor & 1.190$ $(-0.348$, $2.728) &0.761$ $(-3.667$, $5.189) &0.306$ $(-0.301$, $0.913) & -0.315$ $(-0.906$, $0.276) \\ 
  Master & 1.569^{}$ $(-0.064$, $3.201) &0.544$ $(-3.899$, $4.986) &  &  \\ 
  Professional degree & 1.397$ $(-0.940$, $3.734) &0.815$ $(-3.694$, $5.323) &  &  \\ 
  Doctorate &  &0.790$ $(-3.743$, $5.322) &  &  \\ 
  Master or higher &  &  & -0.144$ $(-0.853$, $0.564) & -0.161$ $(-0.879$, $0.558) \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{354} & \multicolumn{1}{c}{354} & \multicolumn{1}{c}{367} & \multicolumn{1}{c}{368} \\ 
\hline \\[-1.8ex] 
\end{tabular}
\fillandplacepagenumber
\elandscape

\dsp

\normalsize



#### High Self-Interest {#framing-results-experiment-si-high}

```{r Framing-Experiment-ANES-Self-Interest-High-Low, include=FALSE}

df.an <- df.an %>% dplyr::mutate(si.tercile = ntile(`Self-interest`, 3)) # create column that orders self-interest distribution into terciles

# same parameter choices as above for moral conviction
df.an %>% filter(., si.tercile == 3) %>% nrow # 354 people
df.an.high.si <- df.an %>% filter(., si.tercile == 3) %>% dplyr::select(., -one_of(mor.si))
plr.out.an.high.si.hc <- create.formula.plr.out(df.an.high.si, vars.excl.mor.si.an, "hc")
plr.out.an.high.si.ev <- create.formula.plr.out(df.an.high.si, vars.excl.mor.si.an, "ev")
names(plr.out.an.high.si.hc$coefficients) <- repl.names(names(plr.out.an.high.si.hc$coefficients), vars.repl)
names(plr.out.an.high.si.ev$coefficients) <- repl.names(names(plr.out.an.high.si.ev$coefficients), vars.repl)

df.an %>% filter(., si.tercile == 1) %>% nrow # 354 people
df.an.low.si <- df.an %>% filter(., si.tercile == 1) %>% dplyr::select(., -one_of(mor.si))
plr.out.an.low.si.hc <- create.formula.plr.out(df.an.low.si, vars.excl.mor.si.an, "hc")
plr.out.an.low.si.ev <- create.formula.plr.out(df.an.low.si, vars.excl.mor.si.an, "ev")
names(plr.out.an.low.si.hc$coefficients) <- repl.names(names(plr.out.an.low.si.hc$coefficients), vars.repl)
names(plr.out.an.low.si.ev$coefficients) <- repl.names(names(plr.out.an.low.si.ev$coefficients), vars.repl)

```

```{r Framing-Experiment-OP-Self-Interest-High-Low, include=FALSE}

df.op <- df.op %>% dplyr::mutate(si.tercile = ntile(`Self-interest`, 3)) # create column that orders self-interest distribution into terciles

# same parameter choices as above for moral conviction
df.op %>% filter(., si.tercile == 3) %>% nrow # 367 people
df.op.high.si <- df.op %>% filter(., si.tercile == 3) %>% dplyr::select(., -one_of(mor.si))
plr.out.op.high.si.hc <- create.formula.plr.out(df.op.high.si, vars.excl.mor.si.op, "hc")
plr.out.op.high.si.ev <- create.formula.plr.out(df.op.high.si, vars.excl.mor.si.op, "ev")
names(plr.out.op.high.si.hc$coefficients) <- repl.names(names(plr.out.op.high.si.hc$coefficients), vars.repl)
names(plr.out.op.high.si.ev$coefficients) <- repl.names(names(plr.out.op.high.si.ev$coefficients), vars.repl)

df.op %>% filter(., si.tercile == 1) %>% nrow # 368 people
df.op.low.si <- df.op %>% filter(., si.tercile == 1) %>% dplyr::select(., -one_of(mor.si))
plr.out.op.low.si.hc <- create.formula.plr.out(df.op.low.si, vars.excl.mor.si.op, "hc")
plr.out.op.low.si.ev <- create.formula.plr.out(df.op.low.si, vars.excl.mor.si.op, "ev")
names(plr.out.op.low.si.hc$coefficients) <- repl.names(names(plr.out.op.low.si.hc$coefficients), vars.repl)
names(plr.out.op.low.si.ev$coefficients) <- repl.names(names(plr.out.op.low.si.ev$coefficients), vars.repl)

```


The collective measure of self-interest is created in the same way as the above collective measure of morality. I take the average of all self-interest question responses, resulting in one overall self-interest score for each respondent. Respondents with high self-interest scores are defined as respondents with an average self-interest score in the top-third of the self-interest score distribution. Respondents with low self-interest scores are defined as respondents with an average self-interest score in the bottom-third of the self-interest score distribution. This again results in groups of comparable sizes.

<!--
HC
ANES
Self-interest opposing: High is null, Low is negative -> Fail to reject null hypothesis
Self-interest supporting: Both are null -> Fail to reject

OP
Self-interest opposing: High is null, Low is negative -> Fail to reject
Self-interest supporting: Both are null -> Fail to reject

EV
ANES
Self-interest opposing: Both are null -> Fail to reject
Self-interest supporting: Both are null -> Fail to reject

OP
Self-interest opposing: Both are null -> Fail to reject 
Self-interest supporting: High is negative, Low is null -> Fail to reject
-->

As above, I run the same regression models as before after subsetting both data sets. Tables \ref{exp-hc-si-high-low} and \ref{exp-ev-si-high-low} show the results separated by issue. While the evidence for high and low morality (\textbf{H2}) was mixed, with some results indicating the ability to reject the null hypothesis, this is not the case for self-interest. None of the results for any self-interest group for any data set for any issue show evidence that self-interest frames move people with higher self-interest scores more than people with lower self-interest scores, as \textbf{H3} asserts. Only two self-interest results are statistically significant, namely healthcare OP High `Self-Interest Opposing` and OP Low `Self-Interest Opposing`. Both coefficients show the expected negative sign, but the negative effect is larger for OP Low (`r plr.out.op.low.si.hc$coefficients["Self-interest opposing"] %>% round(., digits = 3)` vs. `r plr.out.op.high.si.hc$coefficients["Self-interest opposing"] %>% round(., digits = 3)`). This appears to support the theory that the `Self-Interest Oppsing` frame resonates more strongly with respondents with low self-interest scores. I thus fail to reject the null hypothesis for \textbf{H3}.




```{r Healthcare-Self-Interest-High-Low-Table, include=FALSE}

stargazer(plr.out.an.high.si.hc, plr.out.an.low.si.hc, plr.out.op.high.si.hc, plr.out.op.low.si.hc,
          header= FALSE,
          align = TRUE,
          single.row = TRUE,
          title = "Ordinal Logistic Regression Results. High and Low Self-Interest, Healthcare",
          label = "exp-hc-si-high-low",
          star.char = c("", "", ""),
          omit.table.layout = "n",
          ci = TRUE,
          column.labels = c("ANES High", "ANES Low", "OP High", "OP Low"),
          column.sep.width = "-60pt",
          dep.var.labels = c("Healthcare"),
          model.numbers = FALSE)

```


\ssp

\footnotesize

\blandscape
\centering
\captionof{table}{Ordinal Logistic Regression Results. High and Low Self-Interest, Healthcare\label{exp-hc-si-high-low}}
\begin{tabular}{@{\extracolsep{-60pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{Healthcare} \\ 
 & \multicolumn{1}{c}{ANES High} & \multicolumn{1}{c}{ANES Low} & \multicolumn{1}{c}{OP High} & \multicolumn{1}{c}{OP Low} \\ 
\hline \\[-1.8ex] 
 Moral opposing &0.024$ $(-0.634$, $0.682) & -0.275$ $(-0.950$, $0.401) & -0.877^{}$ $(-1.526$, $-0.229) & -0.752^{}$ $(-1.352$, $-0.153) \\ 
  Moral supporting &0.396$ $(-0.220$, $1.013) &0.478$ $(-0.176$, $1.132) & -0.699^{}$ $(-1.307$, $-0.090) & -0.488$ $(-1.099$, $0.123) \\ 
  Self-interest opposing &0.203$ $(-0.419$, $0.825) & -0.024$ $(-0.594$, $0.547) & -0.666^{}$ $(-1.296$, $-0.036) & -0.706^{}$ $(-1.308$, $-0.104) \\ 
  Self-interest supporting &0.261$ $(-0.374$, $0.895) &0.227$ $(-0.407$, $0.862) & -0.576^{}$ $(-1.166$, $0.015) & -0.014$ $(-0.613$, $0.584) \\ 
  Employed full time & -0.368$ $(-1.059$, $0.323) &0.176$ $(-0.503$, $0.855) & -0.221$ $(-0.837$, $0.395) & -0.290$ $(-0.935$, $0.355) \\ 
  Employed part time & -0.013$ $(-0.747$, $0.721) & -0.318$ $(-1.125$, $0.489) & -0.013$ $(-0.694$, $0.669) & -0.501$ $(-1.229$, $0.227) \\ 
  Homemaker &0.151$ $(-0.918$, $1.220) & -0.081$ $(-0.916$, $0.755) &0.186$ $(-0.969$, $1.341) & -0.063$ $(-0.875$, $0.749) \\ 
  Retired & -0.242$ $(-1.002$, $0.519) & -0.062$ $(-0.771$, $0.648) &0.269$ $(-0.504$, $1.042) & -0.658^{}$ $(-1.314$, $-0.002) \\ 
  Student & -0.310$ $(-1.385$, $0.765) &0.124$ $(-1.160$, $1.409) & -0.351$ $(-1.277$, $0.576) & -0.976^{}$ $(-2.040$, $0.087) \\ 
  Income & -0.001$ $(-0.125$, $0.124) & -0.104$ $(-0.234$, $0.027) &0.082$ $(-0.036$, $0.201) & -0.113^{}$ $(-0.230$, $0.004) \\ 
  Democrat & 1.173^{}$ $(0.748$, $1.599) & 1.002^{}$ $(0.582$, $1.423) & 1.105^{}$ $(0.699$, $1.511) & 1.670^{}$ $(1.244$, $2.096) \\ 
  Male & -0.115$ $(-0.539$, $0.309) &0.050$ $(-0.379$, $0.478) & -0.398^{}$ $(-0.803$, $0.008) &0.002$ $(-0.395$, $0.399) \\ 
  7th-8th grade & -18.900^{}$ $(-18.900$, $-18.900) & 22.434^{}$ $(19.581$, $25.286) &  &  \\ 
  9th grade &0.497$ $(-1.712$, $2.706) & 22.054^{}$ $(19.207$, $24.901) &  &  \\ 
  10th grade & 1.323$ $(-1.727$, $4.373) & 23.931^{}$ $(22.157$, $25.705) &  &  \\ 
  11th grade &0.002$ $(-2.335$, $2.340) & 22.255^{}$ $(20.560$, $23.950) &  &  \\ 
  12th grade & -0.311$ $(-2.375$, $1.753) & 22.217^{}$ $(21.049$, $23.384) &  &  \\ 
  High school graduate &0.213$ $(-1.084$, $1.510) & 22.935^{}$ $(22.364$, $23.506) &  &  \\ 
  Some college &0.221$ $(-1.055$, $1.497) & 22.687^{}$ $(22.117$, $23.256) &0.347$ $(-0.242$, $0.937) &0.149$ $(-0.401$, $0.700) \\ 
  Associate degree & -0.136$ $(-1.468$, $1.195) & 23.078^{}$ $(22.440$, $23.716) & -0.009$ $(-0.645$, $0.626) &0.329$ $(-0.306$, $0.964) \\ 
  Bachelor & -0.044$ $(-1.293$, $1.206) & 22.900^{}$ $(22.269$, $23.531) & -0.215$ $(-0.811$, $0.381) &0.290$ $(-0.281$, $0.861) \\ 
  Master & -0.181$ $(-1.474$, $1.113) & 22.999^{}$ $(22.250$, $23.748) &  &  \\ 
  Professional degree &0.195$ $(-1.376$, $1.766) & 22.055^{}$ $(20.564$, $23.546) &  &  \\ 
  Doctorate &  & 23.211^{}$ $(21.822$, $24.600) &  &  \\ 
  Master or higher &  &  &0.415$ $(-0.254$, $1.083) &0.395$ $(-0.287$, $1.077) \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{354} & \multicolumn{1}{c}{354} & \multicolumn{1}{c}{367} & \multicolumn{1}{c}{368} \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\fillandplacepagenumber
\elandscape

\dsp

\normalsize


```{r Environment-Self-Interest-High-Low-Table, include=FALSE}

stargazer(plr.out.an.high.si.ev, plr.out.an.low.si.ev, plr.out.op.high.si.ev, plr.out.op.low.si.ev,
          header= FALSE,
          align = TRUE,
          single.row = TRUE,
          title = "Ordinal Logistic Regression Results. High and Low Self-Interest, Environment",
          label = "exp-ev-si-high-low",
          star.char = c("", "", ""),
          omit.table.layout = "n",
          ci = TRUE,
          column.labels = c("ANES High", "ANES Low", "OP High", "OP Low"),
          column.sep.width = "-60pt",
          dep.var.labels = c("Environment"),
          model.numbers = FALSE)

```


\ssp

\footnotesize

\blandscape
\centering
\captionof{table}{Ordinal Logistic Regression Results. High and Low Self-Interest, Environment\label{exp-ev-si-high-low}}
\begin{tabular}{@{\extracolsep{-60pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{Environment} \\ 
 & \multicolumn{1}{c}{ANES High} & \multicolumn{1}{c}{ANES Low} & \multicolumn{1}{c}{OP High} & \multicolumn{1}{c}{OP Low} \\ 
\hline \\[-1.8ex] 
 Moral opposing & -0.346$ $(-1.019$, $0.328) & -0.672^{}$ $(-1.322$, $-0.022) & -0.970^{}$ $(-1.579$, $-0.362) & -0.484$ $(-1.074$, $0.106) \\ 
  Moral supporting &0.435$ $(-0.248$, $1.118) & -0.144$ $(-0.785$, $0.498) & -0.999^{}$ $(-1.620$, $-0.378) &0.201$ $(-0.439$, $0.840) \\ 
  Self-interest opposing & -0.118$ $(-0.779$, $0.543) & -0.127$ $(-0.794$, $0.539) & -0.600^{}$ $(-1.239$, $0.040) & -0.246$ $(-0.851$, $0.359) \\ 
  Self-interest supporting &0.564^{}$ $(-0.063$, $1.192) &0.116$ $(-0.495$, $0.726) & -0.476$ $(-1.087$, $0.134) & -0.008$ $(-0.608$, $0.592) \\ 
  Employed full time & -0.513$ $(-1.188$, $0.162) &0.050$ $(-0.638$, $0.737) & -0.248$ $(-0.883$, $0.386) &0.258$ $(-0.372$, $0.888) \\ 
  Employed part time & -0.655^{}$ $(-1.384$, $0.074) & -0.055$ $(-0.853$, $0.743) &0.071$ $(-0.627$, $0.768) &0.155$ $(-0.572$, $0.881) \\ 
  Homemaker &0.404$ $(-0.691$, $1.499) & -0.069$ $(-0.938$, $0.801) & -0.620$ $(-1.745$, $0.506) &0.025$ $(-0.777$, $0.827) \\ 
  Retired & -0.643^{}$ $(-1.375$, $0.089) &0.239$ $(-0.475$, $0.954) &0.067$ $(-0.741$, $0.875) &0.291$ $(-0.353$, $0.934) \\ 
  Student & -0.789$ $(-1.895$, $0.318) & -0.710$ $(-2.000$, $0.581) & -0.535$ $(-1.495$, $0.425) & -0.346$ $(-1.437$, $0.746) \\ 
  Income &0.063$ $(-0.062$, $0.188) & -0.010$ $(-0.140$, $0.119) &0.127^{}$ $(0.009$, $0.244) & -0.034$ $(-0.151$, $0.083) \\ 
  Democrat &0.836^{}$ $(0.417$, $1.254) &0.607^{}$ $(0.188$, $1.026) &0.678^{}$ $(0.282$, $1.073) &0.962^{}$ $(0.552$, $1.372) \\ 
  Male &0.090$ $(-0.332$, $0.513) & -0.004$ $(-0.430$, $0.421) & -0.603^{}$ $(-1.012$, $-0.194) & -0.189$ $(-0.595$, $0.216) \\ 
  7th-8th grade & -17.009^{}$ $(-17.009$, $-17.009) &0.388$ $(-4.562$, $5.339) &  &  \\ 
  9th grade & -1.176$ $(-3.362$, $1.009) & -16.327^{}$ $(-16.327$, $-16.327) &  &  \\ 
  10th grade & -2.583^{}$ $(-5.124$, $-0.041) & 3.193$ $(-1.352$, $7.737) &  &  \\ 
  11th grade &0.146$ $(-2.148$, $2.439) &0.269$ $(-4.027$, $4.565) &  &  \\ 
  12th grade & -0.462$ $(-2.765$, $1.841) &0.506$ $(-3.587$, $4.599) &  &  \\ 
  High school graduate & -0.082$ $(-1.394$, $1.231) &0.728$ $(-3.204$, $4.661) &  &  \\ 
  Some college &0.060$ $(-1.250$, $1.371) & 1.299$ $(-2.629$, $5.227) &0.334$ $(-0.281$, $0.949) &0.043$ $(-0.508$, $0.593) \\ 
  Associate degree & -0.187$ $(-1.558$, $1.185) & 1.508$ $(-2.429$, $5.446) & -0.022$ $(-0.676$, $0.632) &0.405$ $(-0.215$, $1.026) \\ 
  Bachelor & -0.032$ $(-1.309$, $1.244) & 1.643$ $(-2.274$, $5.560) & -0.030$ $(-0.641$, $0.581) & -0.116$ $(-0.706$, $0.474) \\ 
  Master &0.106$ $(-1.219$, $1.431) & 1.225$ $(-2.705$, $5.156) &  &  \\ 
  Professional degree &0.031$ $(-1.541$, $1.602) & -0.058$ $(-4.224$, $4.107) &  &  \\ 
  Doctorate &  &0.676$ $(-3.389$, $4.740) &  &  \\ 
  Master or higher &  &  &0.204$ $(-0.462$, $0.871) & -0.147$ $(-0.823$, $0.530) \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{354} & \multicolumn{1}{c}{354} & \multicolumn{1}{c}{367} & \multicolumn{1}{c}{368} \\ 
\hline \\[-1.8ex] 
\end{tabular}
\fillandplacepagenumber
\elandscape

\dsp

\normalsize

\clearpage


### Imputation {#framing-results-experiment-imputation}

```{r MAR 5 Variables Framing, include=FALSE}

mar.5var.frame.an <- read.csv("data/framing/experiment/mar/results/framing.an.mar.results.5var.1062n.1000it.20perc.csv", stringsAsFactors = TRUE, check.names = FALSE)  %>% .[,-1] %>% addPlus
mar.5var.frame.op <- read.csv("data/framing/experiment/mar/results/framing.op.mar.results.5var.1103n.1000it.20perc.csv", stringsAsFactors = TRUE, check.names = FALSE)  %>% .[,-1] %>% addPlus

mar.5var.frame.an$diff[mar.5var.frame.an$method == "true"] <- mar.5var.frame.an$value[mar.5var.frame.an$method == "true"]
mar.5var.frame.op$diff[mar.5var.frame.op$method == "true"] <- mar.5var.frame.op$value[mar.5var.frame.op$method == "true"]

levels(mar.5var.frame.op$method) <- levels(mar.5var.frame.an$method) <- levs

mar.5var.frame <- cbind(mar.5var.frame.an[, c(1,2,4)], mar.5var.frame.op[,4])
colnames(mar.5var.frame) <- c("Method", "Variable", "ANES", "OP")

# to make the in-text citations shorter
mar.5.frame.an <- mar.5var.frame$ANES
mar.5.frame.op <- mar.5var.frame$OP
mar.5.frame.meth <- mar.5var.frame$Method
mar.5.frame.var <- mar.5var.frame$Variable

tab.mar.5var.frame <- stargazer(mar.5var.frame, 
                                summary = FALSE,
                                align = TRUE,
                                header = FALSE,
                                rownames = FALSE,
                                digits = 4,
                                title = "Accuracy of Multiple Imputation Methods. Framing Data, MAR, 5 Variables with NA",
                                label = "mar.5var.frame")

it <- gsub("\\multicolumn{1}{c}{", "", tab.mar.5var.frame, fixed = TRUE)
cat(it)

```

\vspace{-9cm}

As mentioned above, both methods from the previous chapters are applied in the fielding and analysis of the experiment. The ordered probit blocking method from chapter \ref{ordblock} was used to block respondents into treatment groups based on two differing sets of education categories (ANES and OP). This section now applies the ordinal variable imputation method developed in chapter \ref{ordmiss} to both sets of data from the experiment. As in chapter \ref{ordmiss}, I artificially insert missing values and subsequently impute the missing data with four different imputation methods: `hot.deck` from the `hot.deck` package, `amelia` from the `Amelia` package, `mice` from the `mice` package, and my self-penned method `hd.ord` which is specifically designed for ordinal variables. As before, I also include list-wise deletion with `na.omit`. Since we know the true values for each imputed variable, we can assess which method comes closest to the truth and thus performs best. As in chapter \ref{ordmiss}, the analysis is conducted for two missing data mechanisms, MAR and MNAR. Each data set is imputed 1,000 times with each of the four imputation methods. 20 percent missing values are randomly amputed in each iteration for each data set. As in the preceding sections of this chapter, the analysis is split into ANES and OP sets of education categories. As before, missing data is amputed for five variables. 


With data missing at random in the ANES set (Table \ref{mar.5var.frame}), `hd.ord` performs worse than `amelia` and `mice` but on par with `hot.deck` for the three binary variables `Democrat`, `Male` and `Employed`. `hd.ord` is furthest away from the true value for the interval variables `Income` (`r mar.5var.frame[mar.5var.frame$Method == "hd.ord" & mar.5var.frame$Variable == "Income", "ANES"]`) and `Age` (`r mar.5var.frame[mar.5var.frame$Method == "hd.ord" & mar.5var.frame$Variable == "Age", "ANES"]`), with the exception of `na.omit`.

\ssp

 \begin{table}[H] \centering  
 \caption{Accuracy of Multiple Imputation Methods. Framing Data, MAR, 5 Variables with NA}   
 \label{mar.5var.frame} 
\begin{threeparttable}
 \begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{OP} \\
 \hline \\[-1.8ex]
 true & Democrat & 0.&3879 & 0.&3826 \\ 
 hot.deck & Democrat & +0.&0006 & +0.&0005 \\
 hd.ord & Democrat & +0.&0007 & +0.&0004 \\
 amelia & Democrat & +0.&0001 & --0.&0001 \\
 mice & Democrat & +0.&0001 & --0.&0001 \\
 na.omit & Democrat & --0.&0313 & --0.&0343 \\ 
 true & Male & 0.&4576 & 0.&4714 \\ 
 hot.deck & Male & +0.&0008 & +0.&0013 \\
 hd.ord & Male & +0.&0005 & +0.&0013 \\
 amelia & Male & +0.&0001 & +0.&0000 \\
 mice & Male & +0.&0000 & --0.&0001 \\ 
 na.omit & Male & --0.&0429 & --0.&0421 \\ 
 true & Employed & 0.&5612 & 0.&5684 \\ 
 hot.deck & Employed & +0.&0012 & +0.&0012 \\
 hd.ord & Employed & +0.&0013 & +0.&0013 \\
 amelia & Employed & +0.&0002 & +0.&0001 \\ 
 mice & Employed & --0.&0001 & --0.&0001 \\
 na.omit & Employed & --0.&0353 & --0.&0358 \\
 true & Income & 3.&5537 & 3.&4923 \\
 hot.deck & Income & --0.&0047 & --0.&0026 \\
 hd.ord & Income & --0.&0113 & --0.&0107 \\ 
 amelia & Income & --0.&0004 & --0.&0017 \\ 
 mice & Income & --0.&0014 & --0.&0016 \\ 
 na.omit & Income & --0.&1924 & --0.&1958 \\
 true & Age & 46.&3475 & 44.&9574 \\
 hot.deck & Age & --0.&1621 & --0.&1711 \\ 
 hd.ord & Age & --0.&3107 & --0.&2837 \\
 amelia & Age & --0.&0009 & +0.&0040 \\
 mice & Age & --0.&0073 & +0.&0000 \\ 
 na.omit & Age & --0.&8376 & --0.&8373 \\
 \hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 



```{r MNAR 5 Variables Framing, include=FALSE}

mnar.5var.frame.an <- read.csv("data/framing/experiment/mnar/results/framing.an.mnar.results.5var.1062n.1000it.20perc.csv", stringsAsFactors = TRUE, check.names = FALSE)  %>% .[,-1] %>% addPlus
mnar.5var.frame.op <- read.csv("data/framing/experiment/mnar/results/framing.op.mnar.results.5var.1103n.1000it.20perc.csv", stringsAsFactors = TRUE, check.names = FALSE)  %>% .[,-1] %>% addPlus

mnar.5var.frame.an$diff[mnar.5var.frame.an$method == "true"] <- mnar.5var.frame.an$value[mnar.5var.frame.an$method == "true"]
mnar.5var.frame.op$diff[mnar.5var.frame.op$method == "true"] <- mnar.5var.frame.op$value[mnar.5var.frame.op$method == "true"]

levels(mnar.5var.frame.op$method) <- levels(mnar.5var.frame.an$method) <- levs

mnar.5var.frame <- cbind(mnar.5var.frame.an[, c(1,2,4)], mnar.5var.frame.op[,4])
colnames(mnar.5var.frame) <- c("Method", "Variable", "ANES", "OP")

# to make the in-text citations shorter
mnar.5.frame.an <- mnar.5var.frame$ANES
mnar.5.frame.op <- mnar.5var.frame$OP
mnar.5.frame.meth <- mnar.5var.frame$Method
mnar.5.frame.var <- mnar.5var.frame$Variable

tab.mnar.5var.frame <- stargazer(mnar.5var.frame, 
                                 summary = FALSE,
                                 align = TRUE,
                                 header = FALSE,
                                 rownames = FALSE,
                                 digits = 4,
                                 title = "Accuracy of Multiple Imputation Methods. Framing Data, MNAR, 5 Variables with NA",
                                 label = "mnar.5var.frame")

jt <- gsub("\\multicolumn{1}{c}{", "", tab.mnar.5var.frame, fixed = TRUE)
cat(jt)

```



\begin{table}[H] \centering 
 \caption{Accuracy of Multiple Imputation Methods. Framing Data, MNAR, 5 Variables with NA}  
 \label{mnar.5var.frame}  
 \begin{threeparttable}
\begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex]
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{OP} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3879 & 0.&3826 \\
 hot.deck & Democrat & --0.&0035 & --0.&0031 \\ 
 hd.ord & Democrat & --0.&0039 & --0.&0039 \\
 amelia & Democrat & --0.&0034 & --0.&0032 \\
 mice & Democrat & --0.&0028 & --0.&0028 \\ 
 na.omit & Democrat & --0.&0188 & --0.&0197 \\ 
 true & Male & 0.&4576 & 0.&4714 \\
 hot.deck & Male & --0.&0138 & --0.&0127 \\
 hd.ord & Male & --0.&0134 & --0.&0124 \\
 amelia & Male & --0.&0137 & --0.&0133 \\
 mice & Male & --0.&0138 & --0.&0136 \\
 na.omit & Male & --0.&0231 & --0.&0220 \\
 true & Employed & 0.&5612 & 0.&5684 \\
 hot.deck & Employed & --0.&0027 & --0.&0028 \\
 hd.ord & Employed & --0.&0030 & --0.&0034 \\
 amelia & Employed & --0.&0031 & --0.&0030 \\
 mice & Employed & --0.&0030 & --0.&0030 \\
 na.omit & Employed & --0.&0180 & --0.&0178 \\
 true & Income & 3.&5537 & 3.&4923 \\
 hot.deck & Income & --0.&0498 & --0.&0480 \\
 hd.ord & Income & --0.&0528 & --0.&0541 \\ 
 amelia & Income & --0.&0413 & --0.&0439 \\ 
 mice & Income & --0.&0415 & --0.&0441 \\ 
 na.omit & Income & --0.&1064 & --0.&1043 \\
 true & Age & 46.&3475 & 44.&9574 \\ 
 hot.deck & Age & --0.&4322 & --0.&4795 \\
 hd.ord & Age & --0.&6215 & --0.&5925 \\
 amelia & Age & --0.&2420 & --0.&2725 \\ 
 mice & Age & --0.&2441 & --0.&2768 \\ 
 na.omit & Age & --0.&5301 & --0.&5076 \\
 \hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Each \texttt{true} value shows the true variable mean. All other values show the differences between the imputation means and the true mean, indicated with a + or -- sign.}
\end{tablenotes}
\end{threeparttable}
\end{table} 
 
\dsp 

While `mice` and `amelia` perform equally well for the binary variables, `amelia` shows a markedly better performance for the interval variables. The results for the OP data mirror the ANES results for `hot.deck` and `hd.ord`. They show the same levels of distances for the binary variables and `hd.ord` performs markedly worse for the interval variables. Notably, however, `mice` is not inferior to `amelia` for the interval variables in the OP data. `mice` actually beats `amelia` for `Age`, which is a striking reversal of the ANES results.^[For a repeat of this MAR analysis for 10 amputed variables, see appendix section \ref{app-framing-10var}. The results do not change substantively.]

All imputation results for data MNAR (Table \ref{mnar.5var.frame}) are much further away from the true values than in Table \ref{mar.5var.frame}. This is to be expected given the differing missing data mechanisms. The pattern for MNAR is markedly different: `hot.deck` and `hd.ord` show virtually identical results as `amelia` and `mice` for the binary variables for both data sets. `hd.ord` even performs best among all mechanisms for `Male`. While `amelia` and `mice` still produce significantly closer results for `Income` and `Age`, the distance to the two hot decking methods is notably smaller. Between the two hot decking methods, however, `hd.ord` comes last. `hd.ord` in fact performs worse than `na.omit` for `Age` in both data sets.^[For a repeat of this MNAR analysis for 10 amputed variables, see appendix section \ref{app-framing-10var}. The results do not change substantively.]



## Conclusion {#framing-conclusion}

I set out to test the influence of morality and self-interest in political framing and to investigate how morality and self-interest contribute to persuasive strength in emphasis frames. Morality was defined along the lines of Moral Foundations Theory with a focus on the foundation of Care/Harm, while self-interest was considered to cover the areas of personal autonomy, health/safety, wealth, and status. I designed a survey experiment that measured respondents' morality and self-interest attitudes and applied my self-penned ordered probit method to block each respondent on the basis of their education (ANES, OP) into one of five treatment groups (`Moral opposing`, `Moral supporting`, `Self-interest opposing`, `Self-interest supporting`, and `Control`) for two political issues (healthcare, environment). On the basis of the asserted strength of moral arguments due to their deep connection with emotions and the corresponding lack of strength of non-moral arguments, I hypothesized that moral frames move respondents more than self-interest frames (\textbf{H1}). I further hypothesized that the effect of different frames should be felt differently conditional on respondents' own morality and self-interest priorities. A moral frame should affect respondents' opinion more if they score highly on the morality measures as this type of frame is likely to connect more with their values (\textbf{H2}). Correspondingly, a self-interest frame should register more with respondents' opinion if they score highly on the self-interest measures (\textbf{H3}). Finally, I tested the effectiveness of my developed ordinal variable imputation method by artificially inserting missing data.

The results are mixed. \textbf{H1} is partially confirmed for opposing frames. There is evidence that `Moral Opposing` frames move people more towards opposing the issue policies than `Self-Interest Opposing` frames. The `Moral Opposing` frames for ANES environment, OP environment, and OP healthcare all show statistically significant negative effects that are larger than their self-interest counterparts. The same does not hold for `Moral Supporting` frames. Most of the corresponding regression confidence intervals here include the null of no treatment effect. \textbf{H2} is also partially confirmed for opposing frames. The `Moral Opposing` frames appear to resonate more strongly with respondents with high morality scores for both issues, but the `Moral Supporting` frames do not elicit a treatment effect. \textbf{H3}, however, is not confirmed. None of the results for any self-interest group for any data set for any issue show any evidence that self-interest frames move people with higher self-interest scores more than people with lower self-interest scores. 

The missing data analysis confirms the results from chapter \ref{ordmiss}. `hd.ord` does not perform on the level of `amelia`, particularly for non-binary variables. The gap to `amelia` is narrower for imputations carried out for data MNAR, but this does not change the overall results.

It is unclear why the `Moral Opposing` frames confirm the hypotheses while the `Moral Supporting` ones do not. One possible explanation could be differences in the level of frame wording. It could be that the `Moral Opposing` categories are surreptitiously worded more effectively than their supporting counterparts. This seems unlikely, though, as the results for `Moral Opposing` frames are consistent across both issues. Another potential area of investigation could be the juxtaposition of negativity and positivity. Perhaps it is easier and clearer for people to establish what they are morally against than what they morally support. What I dislike, i.e. negative morality, might be easier to figure out than the things I stand for.

One final aspect is notable: The sheer number of ANES education categories renders ordinal logistic regressions difficult. These finely grated and overly nuanced categories often result in a low number of observations per category. This greatly increases regression coefficients, which makes interpretation cumbersome and sometimes impossible, e.g. when exponentiated coefficients can not be calculated for ANES healthcare. The large number of ANES education categories also leads to frequent collinearity, causing the models to drop variable levels. None of this occurs with the OP education categories, which raises doubt about the necessity for such finely separated categories.




<!--chapter:end:04-framing.Rmd-->

# CONCLUSION {#conclusion}

Education is one of the most important predictors of political behavior in political science. As an ordinal variable, education possesses special characteristics: Its categories are ordered, but unevenly spaced. It is crucial that we try to use this unique information to the greatest extent possible. If we want to know what people think and how they act, we need to make sure our measurements are as good as they can possibly be. The ordered probit approach I outlined and applied in the previous chapters represents an attempt to do so. While there are some encouraging signs, the overall conclusion after these analyses speaks against an endorsement of this approach. Assuming a latent underlying continuous variable underneath education, estimating cutoff thresholds between the education categories, and binning observations according to linear model predictors to obtain a new set of education categories based on data fit overall does not appear to improve upon current research practice. 

Blocking on the two differing education sets (ANES and OP) shows mixed results. While an ANOVA regression of the numerical variables does not result in statistically distinct intercepts, a similar general linear model reveals statistical significance for most of the factor variables. The distributions of the placebo regression treatment variables appear to indicate slightly superior performance by the OP method, but this evidence is tentative because of the sample size. It is thus overall unclear whether the re-estimation of ordinal variable categories with an ordered probit approach matters for the process of blocking.

The verdict is much clearer for multiple hot deck imputation with ordinal variables. Adapting the multiple hot deck imputation function `hot.deck` to treat ordinal variables with an ordered probit model does not improve on current methods. `hd.ord` performs on par with some binary variables but worse than `amelia` and `mice` for interval and ordinal variables. This applies for all mechanisms of missingness, differing numbers of variables with missing values, differing numbers of ordinal variables included in the `polr` treatment, and differing percentages of missingness. A cautious exception might be claimed for data MNAR based on the framing analysis in chapter \ref{framing}, where `hd.ord` performed on par with `amelia` for binary variables. Since the aim was to develop a method that improves upon current approaches rather than merely matching their performance, however, this does not change the overall conclusions. `hd.ord`'s gains in imputation speed do not make up for these shortcomings either. While it is necessary to iterate multiple imputation runs many times over for simulation purposes, users likely will not do so, which greatly diminishes the computing time saved. The result of the quality comparison of major missing data solutions is thus a clear endorsement of `amelia`. 

The methodological results from the online survey experiment on political framing do not show substantive differences. The results obtained from the analysis of the ANES education set do not differ from those for the OP set. Estimating and distinguishing between these two sets of categories does not affect the substantive results regarding morality and self-interest. The missing data analysis confirms the superiority of `amelia`. The substantive results on political framing themselves are mixed. There is evidence that `Moral Opposing` frames move people more towards opposing the issue policies than `Self-Interest Opposing` frames, but the same does not hold true for `Moral Supporting` frames. Similarly, `Moral Opposing` frames do appear to resonate more strongly with respondents with high morality scores, but `Moral Supporting` frames do not. There is no evidence that self-interest frames move people with higher self-interest scores more than people with lower self-interest scores. 

Overall, evidence suggests that the re-estimation and distinction of education categories does not matter. The mixed findings in chapter \ref{ordblock} are outweighed by the clear results from chapters \ref{ordmiss} and \ref{framing}. While the ordered probit re-estimation based on an assumed underlying continuous variable does not appear to be important, however, doubts remain about the need for the overall large number of education categories in the ANES. These finely grated and overly nuanced categories often result in a low number of observations per category, which in turn renders ordinal logistic regression difficult, greatly increases regression coefficients, and makes model estimation cumbersome. The large number also leads to frequent collinearity, causing regression models to drop variable levels. None of this occurs with the OP education categories, of which there are a lot fewer. With a lower number of ANES categories, these problems might not appear.

It is never fun to find out that a developed method does not work, and I naturally hoped for a different outcome. My endeavors have nonetheless not been fruitless. We have learned that a focus on ordinal variables is of minor importance for missing data imputation and that `amelia`'s combination of expectation-maximization with bootstrapping is robust and provides outstanding results for all types of variables in a variety of missing data scenarios. We have also learned that morality at least plays a role in frame strength regarding the things we oppose. These represent important findings on the continuing journey to advance, refine, and conduct public opinion measurement.

<!--
Most importantly, however, we have learned that the current practice of converting ordinal explanatory variables into numerical variables does not seem to be problematic after all, as this convenience method appears to closely reflect the true underlying data structure. 
We need modern statistical methods to fully utilize all this information contained in this variable. So far, this aspect has been largely ignored in the literature. 
Whether my methods are suitable in a specific survey or survey experiment depends on the situation, as no method works for all circumstances. 
For a survey experiment with a very large sample and few treatment groups, there is no need for blocking. Simple randomization does the job here. Similarly, if survey results do not contain important ordinal predictor variables, there is no need for a method specifically geared towards ordinal variables. 
Overall, my dissertation adds two important new tools to the empirical political scientist's toolbox to choose from.
-->




<!--chapter:end:05-conclusion.Rmd-->

\appendix

# BLOCKING {#app-ordblock}

## Online Experiment Environment {#app-ordblock-env}

In order to conduct this experiment, I created an online survey environment based on `R` with `shiny` [@boas_fielding_2013], as there currently is no available tool to block sequentially online. Popular online survey platforms, such as Qualtrics, do not offer this functionality, and none of the attempts to combine `R` code work with Qualtrics concern the 'injection' of `R` code into the Qualtrics randomization engine, which blocking would require [@hainmueller_2014_causal;@barari_2017_package;@testa_2017_qualtricstools;@ginn_2018_package]. The following is a basic outline of the mechanisms behind this survey environment.

The survey questions, i.e. questions that collect demographic information and questions that apply treatment, need to be designed as `.txt` files and incorporated into a local `shiny` environment. This local environment is then hosted in the cloud and publicly accessible. The hosted website sequentially blocks each incoming respondent based on her covariate information and covariate information from all previous respondents through constant interaction with the `R` code. The workflow for any incoming respondent is illustrated in Figure \ref{online-workflow} below.

\vspace{0.8cm}

\begin{figure}[ht]
\centering
\begin{tikzpicture}
    \node [cloud]  (survey) {\scriptsize{P starts survey}};
    \node [cloud, below right=0.8cm and -0.6cm of survey] (dems) {\scriptsize{P answers demographics}};
    \node [cloud, right= 0.2cm of survey] (pulls) {\scriptsize{Pull data from Dropbox}};
    \node [cloud, below right=0.8cm and -0.3cm of pulls] (blocking) {\scriptsize{Blocking}};
    \node [cloud, right= 0.2cm of pulls] (assignment) {\scriptsize{Assignment}};
    \node [cloud, below right=0.8cm and -0.6cm of assignment] (treatment) {\scriptsize{Treatment}};
    \node [cloud, right= 0.2cm of assignment] (results) {\scriptsize{Save data to Dropbox}};
	\path [line] (survey) -- (dems);
	\path [line] (dems) -- (pulls);
	\path [line] (pulls) -- (blocking);
	\path [line] (blocking) -- (assignment);
	\path [line] (assignment) -- (treatment);
	\path [line] (treatment) -- (results);
\end{tikzpicture}
\caption{Online Survey Experiment Workflow} \label{online-workflow}
\end{figure}

A respondent clicks on the survey link and answers the demographic question. After she selects her level of education, `R` code in the background pulls previous respondents' covariate information from a Dropbox server. Based on this information and her chosen education level, the `R` code sequentially blocks and assigns her to a treatment group. The respondent then sees and answers the respective treatment question(s). Her responses are then saved on the same Dropbox server. This process is repeated for all incoming respondents. If the respondent is the first person to take the survey, i.e. if there is no covariate information from previous respondents yet, the code randomly assigns her to one of the treatment groups. All subsequent respondents are then blocked and assigned as just described. To recruit respondents, the website was fed into Lucid's marketplace. 



## Blocking Differences {#app-ordblock-blockdiff}

<!--
One block of code is in ordinal-blocking because I needed it for a table there
-->

```{r Block-ANES-OP-Categories-Table, include=FALSE}

df.all.table <- df.all # I need the columns numeric for the plot, but I want the leading zero dropped for the table
# I might not need the table, but I don't want to throw it away

tab <- stargazer(df.all, 
                 summary = FALSE,
                 header=FALSE,
                 align = TRUE,
                 title = "Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories. Differentiated by Treatment Group",
                 label = "education-blocked")
st <- gsub("{c}", "{l}", tab, fixed = TRUE)
cat(st)

```

```{r Block-ANES-OP-Categories-Figure, include=FALSE}

for(i in 1:ncol(df.all)){
  df.all[,i] <- as.numeric(df.all[,i])
}

df.all <- tibble::rownames_to_column(df.all, "categories") # turn row names into column
df.all$categories <- as.factor(df.all$categories)
df.all <- df.all[,c(1:3, 1, 4, 5, 1, 6, 7)] # repeat the categories column so I can loop over everything
col.n <- c("categories", "ANES", "OP")
iters <- c(1, 1+n.tr, 1+n.tr*2) # it needs to go 1, 2, 3, then 4, 5, 6, then 7, 8, 9. So we need the numbers 1, 4, 7 as i for the loop
blocked.plots <- list()

for(i in iters){
  group <- df.all[,c(i, i+1, i+2)]
  colnames(group) <- col.n
  
  gender <- group[1:3,]
  gender$variable <- rep("Gender", 3) %>% as.factor
  gender.melt <- melt(gender, variable.name = "group")
  
  race <- group[4:8,]
  race$variable <- rep("Race", 5) %>% as.factor
  race.melt <- melt(race, variable.name = "group")
  
  income <- group[9:16,]
  income$categories <- c("<25k", "25-50k", "50-75k", "75-100k", "100-125k", "125-150k", "150-175k", ">175K") %>% factor
  income$variable <- rep("Income", 8) %>% as.factor
  income.melt <- melt(income, variable.name = "group")
  
  empl <- group[17:22,]
  empl$variable <- rep("Employment", 6) %>% as.factor
  empl.melt <- melt(empl, variable.name = "group")
  
  pid <- group[23:26,]
  pid$variable <- rep("Party ID", 4) %>% as.factor
  pid.melt <- melt(pid, variable.name = "group")
  
  pres <- group[27:28,]
  pres$variable <- rep("President", 2) %>% as.factor
  pres.melt <- melt(pres, variable.name = "group")
  
  minw <- group[29:32,]
  minw$variable <- rep("Minimum Wage", 4) %>% as.factor
  minw.melt <- melt(minw, variable.name = "group")
  
  country <- group[33:34,]
  country$variable <- rep("Country", 2) %>% as.factor
  country.melt <- melt(country, variable.name = "group")
  
  age <- group[35,]
  age$variable <- rep("Age", 1) %>% as.factor
  age.melt <- melt(age, variable.name = "group")
  
  feel <- group[36,]
  feel$variable <- rep("Feel Trump", 1) %>% as.factor
  feel.melt <- melt(feel, variable.name = "group")
  
  p.gender <- ggplot(data = gender.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Gender") + 
    theme(axis.title.y=element_blank(), legend.position='none', axis.text.x = element_text(angle = 45))
  p.race <- ggplot(data = race.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Race") + 
    theme(axis.title.y=element_blank(), legend.position = c(0.4, 0.7), axis.text.x = element_text(angle = 45)) + 
    guides(fill=guide_legend(title="Categories"))
  p.empl <- ggplot(data = empl.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Employment") + 
    theme(axis.title.y=element_blank(), legend.position='none', axis.text.x = element_text(angle = 45))
  p.pid <- ggplot(data = pid.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Party ID") + 
    theme(axis.title.y=element_blank(), legend.position='none', axis.text.x = element_text(angle = 45))
  p.age <- ggplot(data = age.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Age") + 
    theme(axis.title.y=element_blank(), legend.position='none',
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
  
  p.income <- ggplot(data = income.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Income") + 
    theme(axis.title.y=element_blank(), legend.position='none', axis.text.x = element_text(angle = 45))
  p.pres <- ggplot(data = pres.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("President") + 
    theme(axis.title.y=element_blank(), legend.position='none', axis.text.x = element_text(angle = 45))
  p.minw <- ggplot(data = minw.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Minimum Wage") + 
    theme(axis.title.y=element_blank(), legend.position='none', axis.text.x = element_text(angle = 45))
  p.country <- ggplot(data = country.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Country") + 
    theme(axis.title.y=element_blank(), legend.position='none', axis.text.x = element_text(angle = 45))
  p.feel <- ggplot(data = feel.melt, aes(x = categories, y = value)) +
    geom_bar(aes(fill = group), stat = "identity", position = "dodge") + xlab("Feel Trump") + 
    theme(axis.title.y=element_blank(), legend.position='none',
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
  
  blocked.plots[[i]] <- list(p.gender, p.race, p.empl, p.pid, p.age, 
                             p.income, p.pres, p.minw, p.country, p.feel)
}

blocked.plots <- blocked.plots[lengths(blocked.plots) != 0] # to remove null elements from list

```


Figures \ref{ANBlock1} to \ref{ANBlock3} show the variable proportions or means, depending on the type of variable in question, after blocking the ANES data on education into three treatment groups. As outlined above, this is done twice, once based on the original ANES education categories, and once based on the newly estimated OP education categories.


```{r ANES-Blocked-Plot-1, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories, Treatment Group 1. `Age` and `Feel Trump` Show Means, All Others Show Proportions.\\label{ANBlock1}"}

grid.arrange(grobs = blocked.plots[[1]], ncol = 5, nrow = 2)

```

```{r ANES-Blocked-Plot-2, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories, Treatment Group 2. `Age` and `Feel Trump` Show Means, All Others Show Proportions.\\label{ANBlock2}"}

grid.arrange(grobs = blocked.plots[[2]], ncol = 5, nrow = 2)

```

```{r ANES-Blocked-Plot-3, echo=FALSE, fig.width=10, fig.height=6, fig.cap="Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories, Treatment Group 3. `Age` and `Feel Trump` Show Means, All Others Show Proportions.\\label{ANBlock3}"}

grid.arrange(grobs = blocked.plots[[3]], ncol = 5, nrow = 2)

```



<!---
\ssp

\footnotesize

\begin{longtable}{cc c @{\hspace{1.3cm}} c c @{\hspace{1.3cm}} c c} \\
 \caption{Variable Proportions/Means After Blocking ANES Data on ANES and OP Education Categories. Differentiated by Treatment Group}  
 \label{education-blocked} 
\\[-1.8ex]\hline  
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\hspace{-1cm}Group 1} & \multicolumn{2}{c}{\hspace{-1.3cm}Group 2} & \multicolumn{2}{c}{\hspace{-1.3cm}Group 3}\\ 
\hline \\[-1.8ex]  
 & ANES & OP & ANES & OP & ANES & OP \\  
\cline{2-3}
\cline{4-5}
\cline{6-7}\\[-1.8ex]
\textbf{Gender} & & & & & & \\  
Male & .477 & .468 & .469 & .470 & .467 & .475 \\
Female & .520 & .530 & .529 & .528 & .531 & .522 \\ 
Other & .003 & .002 & .003 & .003 & .002 & .003 \\
 & & & & & & \\
\textbf{Race} & & & & & & \\  
White & .743 & .750 & .760 & .752 & .770 & .770 \\ 
African-American & .111 & .108 & .088 & .087 & .098 & .103 \\ 
Asian & .035 & .025 & .030 & .039 & .030 & .030 \\ 
Native American & .003 & .003 & .007 & .007 & .007 & .007 \\ 
Hispanic & .108 & .115 & .116 & .115 & .096 & .090 \\
 & & & & & & \\
\textbf{Income} & & & & & & \\  
Under \$25,000 & .209 & .210 & .227 & .220 & .198 & .204 \\
\$25,000-49,999 & .225 & .220 & .208 & .214 & .234 & .232 \\ 
\$50,000-74,9999 & .175 & .183 & .178 & .186 & .195 & .180 \\
\$75,000-99,999 & .123 & .137 & .144 & .133 & .132 & .129 \\
\$100,000-124,999 & .105 & .096 & .089 & .090 & .081 & .089 \\
\$125,000-149,999 & .036 & .038 & .043 & .041 & .049 & .049 \\ 
\$150,000-174,999 & .047 & .041 & .038 & .041 & .038 & .041 \\ 
\$175,000 or more & .081 & .075 & .074 & .075 & .072 & .077 \\ 
 & & & & & & \\
\textbf{Employment} & & & & & & \\  
Employed & .604 & .661 & .638 & .592 & .643 & .631 \\ 
Unemployed & .064 & .050 & .060 & .076 & .057 & .055 \\ 
Retired & .198 & .186 & .187 & .210 & .201 & .190 \\ 
Disabled & .049 & .035 & .035 & .044 & .031 & .036 \\ 
Homemaker & .055 & .047 & .055 & .050 & .050 & .065 \\
Student & .030 & .022 & .025 & .028 & .017 & .023 \\
 & & & & & & \\
\textbf{Party ID} & & & & & & \\  
Democrat & .370 & .342 & .342 & .370 & .352 & .352 \\
Republican & .270 & .294 & .319 & .291 & .298 & .302 \\
Independent & .322 & .325 & .308 & .309 & .318 & .314 \\ 
Something else & .037 & .039 & .031 & .030 & .031 & .031 \\ 
 & & & & & & \\
\textbf{President} & & & & & & \\  
Approve & .552 & .538 & .502 & .535 & .526 & .507 \\ 
Disapprove & .448 & .462 & .498 & .465 & .474 & .493 \\ 
 & & & & & & \\
\textbf{Minimum Wage} & & & & & & \\  
Raised & .649 & .639 & .614 & .639 & .665 & .650 \\ 
Kept the same & .289 & .300 & .321 & .296 & .273 & .287 \\
Lowered & .015 & .017 & .028 & .028 & .023 & .021 \\
Eliminated & .048 & .044 & .037 & .037 & .039 & .043 \\ 
 & & & & & & \\
\textbf{Country} & & & & & & \\  
Right direction & .278 & .279 & .248 & .258 & .270 & .259 \\
Wrong track & .722 & .721 & .752 & .742 & .730 & .741 \\
 & & & & & & \\
\textbf{Age} & \hspace{-0.4cm}48.699 & \hspace{-0.4cm}48.713 & \hspace{-0.4cm}48.873 & \hspace{-0.4cm}49.762 & \hspace{-0.4cm}49.493 & \hspace{-0.4cm}48.590 \\
 & & & & & & \\
\textbf{Feel Trump} & \hspace{-0.4cm}35.064 & \hspace{-0.4cm}36.553 & \hspace{-0.4cm}38.582 & \hspace{-0.4cm}34.916 & \hspace{-0.4cm}36.100 & \hspace{-0.4cm}38.276 \\
 \hline \\[-1.8ex]
 \end{longtable}

\dsp

\normalsize
-->


<!--
\ssp

\footnotesize

 \begin{longtable}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} }
 \caption{ANOVA Chisq Test of GLM Regression of Variable on ANES/OP Indicator. Differentiated by Treatment Group}  
 \label{glm-anov} 
 \\[-1.8ex]\hline
 \hline \\[-1.8ex]  
 \multicolumn{1}{l}{} & \multicolumn{1}{l}{Df} & \multicolumn{1}{l}{Deviance} & \multicolumn{1}{l}{Resid.Df} & \multicolumn{1}{l}{Residual Deviance} & \multicolumn{1}{l}{Pr.Chi} \\ 
 \hline \\[-1.8ex]  
\multicolumn{1}{l}{\textbf{Gender}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.191} & 2,098 & 2,904.616 & \multicolumn{1}{l}{.662} \\
 \multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.002} & 2,098 & 2,903.163 & \multicolumn{1}{l}{.965} \\
 \multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.155} & 2,098 & 2,903.972 & \multicolumn{1}{l}{.694} \\ 
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Race}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.123} & 2,098 & 2,379.100 & \multicolumn{1}{l}{.726} \\
 \multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.165} & 2,098 & 2,332.647 & \multicolumn{1}{l}{.684} \\
 \multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.003} & 2,098 & 2,264.958 & \multicolumn{1}{l}{.959} \\
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Income}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.003} & 2,098 & 2,153.304 & \multicolumn{1}{l}{.957} \\
 \multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.135} & 2,098 & 2,230.463 & \multicolumn{1}{l}{.714} \\
 \multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.107} & 2,098 & 2,107.117 & \multicolumn{1}{l}{.744} \\
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Employment}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{7.380} & 2,098 & 2,754.861 & \multicolumn{1}{l}{.007} \\
 \multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{4.637} & 2,098 & 2,794.022 & \multicolumn{1}{l}{.031} \\
 \multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.297} & 2,098 & 2,750.890 & \multicolumn{1}{l}{.586} \\ 
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Party ID}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{1.869} & 2,098 & 2,733.149 & \multicolumn{1}{l}{.172} \\ 
 \multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{1.869} & 2,098 & 2,733.149 & \multicolumn{1}{l}{.172} \\ 
 \multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.000} & 2,098 & 2,725.414 & \multicolumn{1}{l}{1.000} \\ 
 & & & & & \\  
\multicolumn{1}{l}{\textbf{President}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.432} & 2,098 & 2,893.572 & \multicolumn{1}{l}{.511} \\ 
 \multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{2.337} & 2,098 & 2,905.983 & \multicolumn{1}{l}{.126} \\
 \multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.763} & 2,098 & 2,908.253 & \multicolumn{1}{l}{.382} \\ 
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Minimum Wage}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.208} & 2,098 & 2,734.810 & \multicolumn{1}{l}{.649} \\ 
 \multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{1.376} & 2,098 & 2,773.589 & \multicolumn{1}{l}{.241} \\
 \multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.541} & 2,098 & 2,699.691 & \multicolumn{1}{l}{.462} \\ 
 & & & & & \\  
\multicolumn{1}{l}{\textbf{Country}} & & & & & \\  
 \multicolumn{1}{l}{T1} & 1 & \multicolumn{1}{l}{.002} & 2,098 & 2,484.714 & \multicolumn{1}{l}{.961} \\ 
 \multicolumn{1}{l}{T2} & 1 & \multicolumn{1}{l}{.305} & 2,098 & 2,374.595 & \multicolumn{1}{l}{.581} \\ 
 \multicolumn{1}{l}{T3} & 1 & \multicolumn{1}{l}{.352} & 2,098 & 2,427.160 & \multicolumn{1}{l}{.553} \\  
 \hline \\[-1.8ex]  
 \end{longtable} 
-->





# MISSING DATA {#app-ordmiss}


## Imputing Missing Data for All ANES and CCES Observations {#app-ordmiss-allObs}

While using the full number of observations in the ANES data is possible, doing so for the CCES data provides great computational challenges. All 2,395 ANES observations can be used for 1,000 multiple imputation iterations. With over 42,000 observations, however, the number of iterations that are computationally feasible for the CCES data is reduced to 10. Anything above that maxes out the 120 GB RAM on the cloud container that was at my disposal, which results in termination of the code. Keeping these restrictions in mind, Tables \ref{mar.5var.all} and \ref{mnar.5var.all} show the results for MAR and MNAR for 5 variables with inserted missing data.


```{r MAR 5 Variables All Observations, include=FALSE}

mar.5var.anes.all <- read.csv("data/anes/appendix/anes.mar.results.5var.2395n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mar.5var.cces.all <- read.csv("data/cces/appendix/cces.mar.results.5var.42205n.10it.20perc.csv") %>% .[,-1] %>% addPlus

mar.5var.anes.all[1:6, 2] <- mar.5var.cces.all[1:6, 2] <- rep("Democrat", 6)
mar.5var.anes.all[19:24, 2] <- mar.5var.cces.all[19:24, 2] <- rep("Income", 6)

mar.5var.anes.all$diff[mar.5var.anes.all$method == "true"] <- mar.5var.anes.all$value[mar.5var.anes.all$method == "true"]
mar.5var.cces.all$diff[mar.5var.cces.all$method == "true"] <- mar.5var.cces.all$value[mar.5var.cces.all$method == "true"]

levels(mar.5var.anes.all$method) <- levels(mar.5var.cces.all$method) <-
  levs

mar.5var.all <- cbind(mar.5var.anes.all[, c(1,2,4)], mar.5var.cces.all[,4])

for(i in 1:ncol(mar.5var.all)){
  mar.5var.all[,i] <- mar.5var.all[,i] %>% as.character
}

mar.5var.all <- rbind(mar.5var.all, c("Observations", "", 2395, 42205), c("Iterations", "", 1000, 10))
colnames(mar.5var.all) <- col.names[-5]

tab.mar.5var.all <- stargazer(mar.5var.all, 
                              summary = FALSE,
                              align = TRUE,
                              header = FALSE,
                              rownames = FALSE,
                              title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, MAR, 5 Variables with NA, All Observations",
                              label = "mar.5var.all")

kt <- gsub("\\multicolumn{1}{c}{", "", tab.mar.5var.all, fixed = TRUE)
cat(kt)

```



 \begin{table}[!htbp] \centering    
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, MAR, 5 Variables with NA, All Observations}  
 \label{mar.5var.all}
\begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex]
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3370 & 0.&4032 \\
 hd.ord & Democrat & --0.&0001 & + 0.&0006 \\
 hot.deck & Democrat & --0.&0002 & + 0.&0006 \\ 
 amelia & Democrat & + 0.&0000 & + 0.&0003 \\
 mice & Democrat & + 0.&0000 & + 0.&0003 \\
 na.omit & Democrat & --0.&0302 & --0.&0250 \\
 true & Male & 0.&4868 & 0.&4521 \\
 hd.ord & Male & --0.&0004 & --0.&0001 \\ 
 hot.deck & Male & --0.&0008 & --0.&0002 \\
 amelia & Male & + 0.&0001 & + 0.&0001 \\
 mice & Male & + 0.&0001 & + 0.&0001 \\
 na.omit & Male & --0.&0365 & --0.&0436 \\
 true & Interest & 2.&8806 & 3.&3301 \\
 hd.ord & Interest & --0.&0087 & --0.&0033 \\
 hot.deck & Interest & --0.&0135 & --0.&0046 \\
 amelia & Interest & + 0.&0001 & + 0.&0001 \\
 mice & Interest & + 0.&0000 & + 0.&0000 \\ 
 na.omit & Interest & --0.&0741 & --0.&0763 \\
 true & Income & 16.&6894 & 6.&5830 \\
 hd.ord & Income & --0.&0606 & --0.&0009 \\ 
 hot.deck & Income & --0.&1030 & --0.&0073 \\
 amelia & Income & + 0.&0009 & --0.&0021 \\
 mice & Income & --0.&0007 & --0.&0022 \\
 na.omit & Income & --0.&5574 & --0.&2592 \\
 true & Age & 50.&3745 & 52.&8639 \\
 hd.ord & Age & --0.&2355 & --0.&0221 \\ 
 hot.deck & Age & --0.&3698 & --0.&0790 \\ 
 amelia & Age & + 0.&0056 & --0.&0006 \\
 mice & Age & + 0.&0053 & --0.&0132 \\
 na.omit & Age & --1.&2785 & --1.&2190 \\
 \hline \\[-1.8ex] 
\multicolumn{2}{c}{Observations} & \multicolumn{2}{c}{2395} & \multicolumn{2}{c}{42205} \\ 
\multicolumn{2}{c}{Iterations} & \multicolumn{2}{c}{1000} & \multicolumn{2}{c}{10} \\ 
\hline \\[-1.8ex] 
 \end{tabular} 
 \end{table} 




```{r MNAR 5 Variables All Observations, include=FALSE}

mnar.5var.anes.all <- read.csv("data/anes/appendix/anes.mnar.results.5var.2395n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mnar.5var.cces.all <- read.csv("data/cces/appendix/cces.mnar.results.5var.42205n.10it.20perc.csv") %>% .[,-1] %>% addPlus

mnar.5var.anes.all[1:6, 2] <- mnar.5var.cces.all[1:6, 2] <- rep("Democrat", 6)
mnar.5var.anes.all[19:24, 2] <- mnar.5var.cces.all[19:24, 2] <- rep("Income", 6)

mnar.5var.anes.all$diff[mnar.5var.anes.all$method == "true"] <- mnar.5var.anes.all$value[mnar.5var.anes.all$method == "true"]
mnar.5var.cces.all$diff[mnar.5var.cces.all$method == "true"] <- mnar.5var.cces.all$value[mnar.5var.cces.all$method == "true"]

levels(mnar.5var.anes.all$method) <- levels(mnar.5var.cces.all$method) <-
  levs

mnar.5var.all <- cbind(mnar.5var.anes.all[, c(1,2,4)], mnar.5var.cces.all[,4])

for(i in 1:ncol(mnar.5var.all)){
  mnar.5var.all[,i] <- mnar.5var.all[,i] %>% as.character
}

mnar.5var.all <- rbind(mnar.5var.all, c("Observations", "", 2395, 42205), c("Iterations", "", 1000, 10))
colnames(mnar.5var.all) <- col.names[-5]

tab.mnar.5var.all <- stargazer(mnar.5var.all, 
                               summary = FALSE,
                               align = TRUE,
                               header = FALSE,
                               rownames = FALSE,
                               title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, MNAR, 5 Variables with NA, All Observations",
                               label = "mnar.5var.all")

lt <- gsub("\\multicolumn{1}{c}{", "", tab.mnar.5var.all, fixed = TRUE)
cat(lt)

```


 \begin{table}[!htbp] \centering   
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, MNAR, 5 Variables with NA, All Observations}   
 \label{mnar.5var.all}  
\begin{tabular}{ccr@{}lr@{}l} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex]  
 true & Democrat & 0.&3370 & 0.&4032 \\ 
 hd.ord & Democrat & --0.&0109 & --0.&0105 \\
 hot.deck & Democrat & --0.&0113 & --0.&0105 \\
 amelia & Democrat & --0.&0110 & --0.&0109 \\
 mice & Democrat & --0.&0103 & --0.&0106 \\
 na.omit & Democrat & --0.&0185 & --0.&0145 \\  
 true & Male & 0.&4868 & 0.&4521 \\ 
 hd.ord & Male & --0.&0138 & --0.&0129 \\ 
 hot.deck & Male & --0.&0136 & --0.&0131 \\ 
 amelia & Male & --0.&0134 & --0.&0132 \\
 mice & Male & --0.&0134 & --0.&0131 \\ 
 na.omit & Male & --0.&0196 & --0.&0244 \\ 
 true & Interest & 2.&8806 & 3.&3301 \\
 hd.ord & Interest & --0.&0257 & --0.&0171 \\
 hot.deck & Interest & --0.&0299 & --0.&0179 \\
 amelia & Interest & --0.&0178 & --0.&0147 \\
 mice & Interest & --0.&0179 & --0.&0148 \\
 na.omit & Interest & --0.&0407 & --0.&0418 \\
 true & Income & 16.&6894 & 6.&5830 \\
 hd.ord & Income & --0.&1874 & --0.&0691 \\ 
 hot.deck & Income & --0.&2287 & --0.&0696 \\
 amelia & Income & --0.&1292 & --0.&0637 \\
 mice & Income & --0.&1297 & --0.&0634 \\
 na.omit & Income & --0.&2729 & --0.&1375 \\
 true & Age & 50.&3745 & 52.&8639 \\ 
 hd.ord & Age & --0.&5240 & --0.&2331 \\
 hot.deck & Age & --0.&6609 & --0.&2764 \\
 amelia & Age & --0.&2533 & --0.&2342 \\
 mice & Age & --0.&2474 & --0.&2371 \\
 na.omit & Age & --0.&7188 & --0.&6476 \\
\hline \\[-1.8ex] 
\multicolumn{2}{c}{Observations} & \multicolumn{2}{c}{2395} & \multicolumn{2}{c}{42205} \\ 
\multicolumn{2}{c}{Iterations} & \multicolumn{2}{c}{1000} & \multicolumn{2}{c}{10} \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 
 



<!--

## Speed {#app-ordmiss-speed}

`mice` shows the slowest performance across all combinations of observations and iterations. `amelia`'s relative lack of speed compared to `hd.ord` appears to be due the number of observations, not the number of iterations. `amelia` is roughly 2 times slower than `hd.ord` for 1,000 observations for 1,000 and 10,000 iterations. For 2,395 observations, however, +++ 

```{r Runtimes Increase Observations Increasing Iterations ANES, include=FALSE}

run.all.obs.anes.1000it <- read.csv("data/anes/appendix/anes.mar.runtime.5var.2395n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.all.obs.anes.2500it <- read.csv("data/anes/appendix/anes.5lev.runtime.5var.3145n.2500it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
run.1000.obs.anes.1000it <- read.csv("data/anes/appendix/anes.7lev.runtime.22var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
run.1000.obs.anes.10000it <- read.csv("data/anes/appendix/anes.7lev.runtime.5var.1000n.10000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order

digits.char <- function(df){
  for(i in 1:ncol(df)){
    if(is.numeric(df[,i])){
      df[,i] <- round(df[,i], digits = 3)
    }
    df[,i] <- df[,i] %>% as.character
  }
  return(df)
}

run.all.obs.anes.1000it <- digits.char(run.all.obs.anes.1000it)
run.all.obs.anes.1000it <- rbind(c(NA, "2,395 Observations"), c(NA, "1,000 Iterations"), run.all.obs.anes.1000it)

run.all.obs.anes.2500it <- digits.char(run.all.obs.anes.2500it)
run.all.obs.anes.2500it <- rbind(c(NA, NA), c(NA, "2,500 Iterations"), run.all.obs.anes.2500it)

run.1000.obs.anes.1000it <- digits.char(run.1000.obs.anes.1000it)
run.1000.obs.anes.1000it <- rbind(c(NA, "1,000 Observations"), c(NA, "1000 Iterations"), run.1000.obs.anes.1000it)

run.1000.obs.anes.10000it <- digits.char(run.1000.obs.anes.10000it)
run.1000.obs.anes.10000it <- rbind(c(NA, NA), c(NA, "10,000 Iterations"), run.1000.obs.anes.10000it)

# I am stretching things a bit here. The second one is actually 3,145 observations (not 2,395). The third one actually has 22 variables with NAs (not 5). The first two have 5 levels, the second two have 7 levels. But it makes such a tiny difference here or there that it doesn't change the substantive outcome at all

run.anes.all <- cbind(run.1000.obs.anes.1000it, run.1000.obs.anes.10000it[,2], run.all.obs.anes.1000it[,2], run.all.obs.anes.2500it[,2])

stargazer(run.anes.all,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes). ANES Data, MAR, 5 Variables with NA",
          label = "run.anes.all")

```


\begin{table}[!htbp] \centering 
  \caption{Runtimes of Multiple Imputation Methods (in Minutes). ANES Data, MAR, 5 Variables with NA} 
  \label{run.anes.all} 
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{} & \multicolumn{2}{c}{1,000 Observations} & \multicolumn{2}{c}{2,395 Observations} \\ 
\cline{2-5}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{1000 Iterations} & \multicolumn{1}{c}{10,000 Iterations} & \multicolumn{1}{c}{1,000 Iterations} & \multicolumn{1}{c}{2,500 Iterations} \\ 
\cline{2-3}
\cline{4-5}
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{4.103} & \multicolumn{1}{c}{24.294} & \multicolumn{1}{c}{9.406} & \multicolumn{1}{c}{32.668} \\ 
\multicolumn{1}{c}{hd.norm.orig} & \multicolumn{1}{c}{4.3} & \multicolumn{1}{c}{24.316} & \multicolumn{1}{c}{9.374} & \multicolumn{1}{c}{32.881} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{8.758} & \multicolumn{1}{c}{50.63} & \multicolumn{1}{c}{12.393} & \multicolumn{1}{c}{33.691} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{336.038} & \multicolumn{1}{c}{390.69} & \multicolumn{1}{c}{99.455} & \multicolumn{1}{c}{298.919} \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}



```{r Runtimes All Observations, include=FALSE}

run.all.obs.anes <- read.csv("data/anes/appendix/anes.mar.runtime.5var.2395n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.all.obs.cces <- read.csv("data/cces/appendix/cces.mar.runtime.5var.42205n.10it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.all.obs <- cbind(run.all.obs.anes[, c(1,2)], run.all.obs.cces[,2])

for(i in 1:ncol(run.all.obs)){
  if(is.numeric(run.all.obs[,i])){
     run.all.obs[,i] <- round(run.all.obs[,i], digits = 3)
  }
  run.all.obs[,i] <- run.all.obs[,i] %>% as.character
}

run.all.obs <- rbind(run.all.obs, c("Observations", 2395, 42205), c("Iterations", 1000, 10))
colnames(run.all.obs) <- c("Method", "ANES", "CCES")

stargazer(run.all.obs,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes). ANES and CCES Data, All Available Observations",
          label = "run.all.obs")

```

\begin{table}[!htbp] \centering 
  \caption{Runtimes of Multiple Imputation Methods (in Minutes). ANES and CCES Data, All Available Observations} 
  \label{run.all.obs} 
\begin{threeparttable}
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{9.406} & \multicolumn{1}{c}{21.829} \\ 
\multicolumn{1}{c}{hot.deck} & \multicolumn{1}{c}{9.374} & \multicolumn{1}{c}{21.429} \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{12.393} & \multicolumn{1}{c}{2.602} \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{99.455} & \multicolumn{1}{c}{100.253} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Observations} & \multicolumn{1}{c}{2395} & \multicolumn{1}{c}{42205} \\ 
\multicolumn{1}{c}{Iterations} & \multicolumn{1}{c}{1000} & \multicolumn{1}{c}{10} \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\begin{tablenotes}[para,flushleft]
\footnotesize{\textit{Note:} Due to the very high number of observations, the CCES data can only be run for a low number of iterations. Anything above that maxes out the 120 GB of RAM available to me. The framing data are not included since their total number of observations (1,003) is virtually identical to the sample of 1,000.}
\end{tablenotes}
\end{threeparttable}
\end{table} 


```{r Runtimes MI Methods From Before, include=FALSE}

oldframe.runtime <- read.csv("data/framing/appendix/framing.runtime.5var.1003n.12500it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
anes.runtime <- read.csv("data/anes/appendix/anes.runtime.5var.3223n.2396it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
anes.5lev.runtime <- read.csv("data/anes/appendix/anes.5lev.runtime.5var.3145n.2500it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
oldframe.quadrobs.runtime <- read.csv("data/framing/appendix/quadrupleObs/framing.quadrobs.runtime.5var.4012n.1500it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
oldframe.2000it.runtime <- read.csv("data/framing/appendix/framing.runtime.5var.1003n.2000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order

runtimes <- data.frame(matrix(NA, 7, 6))
colnames(runtimes) <- c("Method", "OldFraming", "ANES2016", "ANES2016_5lev", "OldFramingQuadrObs", "OldFraming_2000it")
runtimes$Method <- c("hd.ord", "hd.norm", "amelia", "mice", "Observations", "Iterations", "Levels")
runtimes$OldFraming <- c(oldframe.runtime[,2], 1003, 12500, 7)
runtimes$ANES2016 <- c(anes.runtime[,2], 3223, 2396, 16)
runtimes$ANES2016_5lev <- c(anes.5lev.runtime[,2], 3145, 2500, 5)
runtimes$OldFramingQuadrObs <- c(oldframe.quadrobs.runtime[,2], 4012, 1500, 7)
runtimes$OldFraming_2000it <- c(oldframe.2000it.runtime[,2], 1003, 2000, 7)

# to make in-text citations shorter
runFullF <- runtimes$OldFraming
runFullA <- runtimes$ANES2016
runFullA5 <- runtimes$ANES2016_5lev
runFullFQ <- runtimes$OldFramingQuadrObs
runFullF2k <- runtimes$OldFraming_2000it
methFull <- runtimes$Method
stargazer(runtimes,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes). ANES and Old Framing Data",
          label = "runtimes")
```


\begin{table}[!htbp] \centering 
  \caption{Runtimes of Multiple Imputation Methods (in Minutes). ANES and Old Framing Data} 
  \label{runtimes} 
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{OldFraming} & \multicolumn{1}{c}{ANES2016} & \multicolumn{1}{c}{ANES2016\_5lev} & \multicolumn{1}{c}{OldFramingQuadrObs} \\ 
\cline{2-2} 
\cline{3-3} 
\cline{4-4}
\cline{5-5}\\[-1.8ex]
\multicolumn{1}{c}{hd.ord} & 34.457 & 32.642 & 32.668 & 31.519 \\ 
\multicolumn{1}{c}{hd.norm} & 34.660 & 32.669 & 32.881 & 31.810 \\ 
\multicolumn{1}{c}{amelia} & 77.956 & 33.051 & 33.691 & 30.035 \\ 
\multicolumn{1}{c}{mice} & 616.023 & 293.722 & 298.919 & 277.021 \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Observations} & 1,003 & 3,223 & 3,145 & 4,012 \\ 
\multicolumn{1}{c}{Iterations} & 12,500 & 2,396 & 2,500 & 1,500 \\ 
\multicolumn{1}{c}{Levels} & 7 & 16 & 5 & 7 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}

As can be seen in Table \ref{runtimes}, `hd.ord` and `hot.deck` show virtually identical imputation times for the old framing data (n = 1,003), with `hd.ord` being `r ((runFullF[methFull == "hd.norm"]-runFullF[methFull == "hd.ord"])*60) %>% round(., digits = 0)` seconds faster. `amelia`, however, is `r (runFullF[methFull == "amelia"] / runFullF[methFull == "hd.ord"]) %>% round(., digits = 1)` times slower than `hd.ord`. `mice` is `r (runFullF[methFull == "mice"] / runFullF[methFull == "hd.ord"]) %>% round(., digits = 1)` times slower than `hd.ord`. This is a dramatic speed gain. The ANES (n = 3,223) data show only a reduced speed gain. `mice` is still drastically slower than `hd.ord` (by a magnitude of `r (runFullA[methFull == "mice"] / runFullA[methFull == "hd.ord"]) %>% round(., digits = 1)`), but the speed gain is cut in half. The previous speed gain over `amelia` is no longer observable. `hd.ord` and `amelia` now show virtually identical runtimes (with a difference of `r round((runFullA[methFull == "amelia"] - runFullA[methFull == "hd.ord"]) * 60, digits = 0)` seconds in favor of `hd.ord`). 

Three factors might explain this sudden and surprising increase in the performance of `amelia`: The levels of the ordinal variable in question (`education`), the number of iterations, and the number of observations in a data set. The ANES (n = 3,223) data contain 17 levels of `education`, whereas the old framing data (n = 1,003) contain only 7. However, when run with 5 levels of `education` and otherwise virtually identical data, the method performances remain unchanged (see column "ANES2016_5lev"). This rules out the levels of the ordinal variable. Another explanation could be the number of observations and the corresponding possible number of iterations. A high number of observations reduces the computationally feasible number of iterations that can be performed before even powerful machines with 120 GB RAM are maxed out. The old framing data (n = 1,003) contain 1,003 observations and can be computationally run for 12,500 iterations. The 2016 ANES data (n = 3,223) contain more than triple the observations than the old framing data (n = 1,003) and can only be run for 2,396 iterations. Indeed, when we quadruple the number of observations in the old framing data (n = 4,012) and are thus forced to reduce the number of possible iterations to 1,500, we observe that `amelia` is now the fastest method (see column "OldFramingQuadrObs"). It thus appears that `amelia` is fast with a low number of iterations and slow with a high number of iterations. 

This would lead us to predict that `amelia` should be the fastest method when the old framing data (n = 1,003) is run for a low number of iterations (2,000). That is not the case: `amelia` is `r (runFullF2k[methFull == "amelia"] / runFullF2k[methFull == "hd.ord"]) %>% round(., digits = 1)` times slower than `hd.ord` (see column "OldFraming_2000it"); on the same level as the 12,500-iteration-run of the old framing data (n = 1,003). This means it's not the number of iterations but the number of observations that affects `amelia`'s relative lack of speed compared to `hd.ord`. `amelia` appears to be much slower than `hd.ord` for around 1,000 observations but on equal footing for 3,000+ observations. 



```{r Runtimes MI Methods From Before, 1000 Observations 10000 Iterations, include=FALSE}

cces.runtime.1000n.10000it <- read.csv("data/cces/appendix/cces.runtime.5var.1000n.10000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
oldframe.runtime.1000n.10000it <- read.csv("data/framing/appendix/framing.runtime.5var.1000n.10000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
anes.7lev.runtime.1000n.10000it <- read.csv("data/anes/appendix/anes.7lev.runtime.5var.1000n.10000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order

cces.runtime.1000n.1000it.more.var <- read.csv("data/cces/appendix/cces.runtime.16var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
oldframe.runtime.1000n.1000it.more.var <- read.csv("data/framing/appendix/framing.runtime.13var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order
anes.7lev.runtime.1000n.1000it.more.var <- read.csv("data/anes/appendix/anes.7lev.runtime.22var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)] # reverse column order

runtimes.1000n <- data.frame(matrix(NA, 8, 7))
colnames(runtimes.1000n) <- c("Method",
                              "CCES10kIt", "OldFraming10kIt", "ANES10kIt",
                              "CCES1kIt", "OldFraming1kIt", "ANES1kIt")
runtimes.1000n$Method <- c("hd.ord", "hd.norm", "amelia", "mice",
                           "Observations", "Iterations", "Ordinal Levels",
                           "Variables with NAs")

runtimes.1000n$CCES10kIt <- c(cces.runtime.1000n.10000it[,2], 1000, 10000, 6, 5)
runtimes.1000n$OldFraming10kIt <- c(oldframe.runtime.1000n.10000it[,2], 1000, 10000, 7, 5)
runtimes.1000n$ANES10kIt <- c(anes.7lev.runtime.1000n.10000it[,2], 1000, 10000, 7, 5)

runtimes.1000n$CCES1kIt <- c(cces.runtime.1000n.1000it.more.var[,2], 1000, 1000, 6, 16)
runtimes.1000n$OldFraming1kIt <- c(oldframe.runtime.1000n.1000it.more.var[,2], 1000, 1000, 7, 13)
runtimes.1000n$ANES1kIt <- c(anes.7lev.runtime.1000n.1000it.more.var[,2], 1000, 1000, 7, 22)


# to make in-text citations shorter
# run1k10kC <- runtimes.1000n$CCES10kIt
# run1k10kF <- runtimes.1000n$OldFraming10kIt
# run1k10kA <- runtimes.1000n$ANES10kIt
#
# run1k1kC <- runtimes.1000n$CCES1kIt
# run1k1kF <- runtimes.1000n$OldFraming1kIt
# run1k1kA <- runtimes.1000n$ANES1kIt

meth <- runtimes.1000n$Method

hd.am.div <- sapply(2:7,
                    function(x)
                      runtimes.1000n[meth == "amelia", x] /
                      runtimes.1000n[meth == "hd.ord", x])

stargazer(runtimes.1000n,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes). ANES, CCES and Old Framing Data, 1000 Observations",
          label = "runtimes1000n")
```


This is indeed confirmed by Table \ref{runtimes1000n}, where the old framing, the ANES, and the CCES data have all been reduced to 1,000 observations. The left half of the table shows the results for 10,000 iterations and low numbers of variables with NAs (Note: `education` in the ANES data was reduced to 7 levels to make this number of iterations computationally feasible). The right half of the table shows the results for 1,000 iterations and high numbers of variables with NAs. `amelia` is consistently between `r hd.am.div %>% min %>% round(., digits = 1)` and `r hd.am.div %>% max %>% round(., digits = 1)` times slower than `hd.ord` for all three data sets, regardless of the number of iterations and the number of variables with NAs.

\begin{table}[!htbp] \centering 
  \caption{Runtimes of Multiple Imputation Methods (in Minutes). ANES, CCES and Old Framing Data, 1000 Observations} 
  \label{runtimes1000n} 
\begin{tabular}{@{\extracolsep{5pt}} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{10,000 Iterations} & \multicolumn{3}{c}{1,000 Iterations}\\
\cline{2-7} \\[-1.8ex]
 & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{OldFraming} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{CCES} & \multicolumn{1}{c}{OldFraming} & \multicolumn{1}{c}{ANES} \\ 
\cline{2-4} 
\cline{5-7} \\[-1.8ex]
\multicolumn{1}{c}{hd.ord} & 24.394 & 26.461 & 24.294 & 2.714 & 2.588 & 4.103 \\ 
\multicolumn{1}{c}{hd.norm} & 24.540 & 26.620 & 24.316 & 2.723 & 2.617 & 4.300 \\ 
\multicolumn{1}{c}{amelia} & 57.403 & 68.862 & 50.630 & 7.787 & 6.926 & 8.758 \\ 
\multicolumn{1}{c}{mice} & 449.027 & 527.532 & 390.690 & 158.583 & 116.609 & 336.038 \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Ordinal Levels} & 6 & 7 & 7 & 6 & 7 & 7 \\ 
\multicolumn{1}{c}{Variables with NAs} & 5 & 5 & 5 & 16 & 13 & 22 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 
-->



<!--
## With Modified `ampute()` {#app-ordmiss-modified.ampute}

Table \ref{amp.oldframe.bycases.acc} shows the results of using `ampute()` with the options `bycases=FALSE` and `cont=FALSE` on the old framing data (n = 1,003) and inserts NAs MAR for 5 variables at 20 percent for 9,644 iterations. `hd.ord` overall performs worse in this scenario. 


```{r results='asis', echo=FALSE}
amp.oldframe.bycases <- read.csv("data/framing/appendix/bycases=FALSE_cont=FALSE/framing.bycases.results.5var.1003n.9644it.20.perc.csv") %>% .[,-1] %>% abs.diff
levels(amp.oldframe.bycases$method) <- c("amelia", "hd.norm", "hd.ord", "mice", "na.omit", "true")
colnames(amp.oldframe.bycases) <- c("Method", "Variable", "Value", "Diff")
stargazer(amp.oldframe.bycases, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. ampute() with bycases, Old Framing Data (n = 1,003)",
          label = "amp.oldframe.bycases.acc")

```

-->



<!--
## With My Own Amputation Methods {#app-ordmiss-own}

Table \ref{own.NA.oldframe.acc} shows the results of using my own function, `own.NA()`, to insert 20 percent NAs MAR into three variables in the old framing data (n = 1,003). In general, something is MAR if you ampute values in column A based on values in column B, e.g. if you ampute the values for `age` where `income = 1` and where `income = 5`. `own.NA()` applies this procedure for any combination of columns. Here, the chosen columns to be amputed are `Dem`, `age`, and `interest`. The chosen columns the amputations depend on are `inc`, `Female`, and `Black`. The function samples 20 percent of observations for each unique value of `inc`. For those observations, the values of `Dem` are amputed. Accordingly, the function samples 20 percent of observations for each unique value of `Female`. For those observations, the values of `age` are amputed. The same occurs for `Black` and `interest`. The resulting data frame is then imputed. Note that the number of amputed and imputed variables is generally lower, as a pair of variables is needed to ampute one variable. As Table \ref{own.NA.oldframe.acc} shows, `hd.ord` does not perform particularly well. It beats `hd.norm` for all three variables but falls considerably short of `amelia` and `mice`. It is notable, however, that my method seems closer to being MCAR than MAR, as `na.omit` performs well and sometimes even outperforms other methods, for instance for `interest`. It is thus questionable how much use `own.NA()` is in its current form.




```{r results='asis', echo=FALSE}
own.NA.oldframe <- read.csv("data/framing/appendix/own.na/framing.own.na.results.3var.1003n.12324it.20perc.csv") %>% .[,-1] %>% abs.diff
levels(own.NA.oldframe$method) <- c("amelia", "hd.norm", "hd.ord", "mice", "na.omit", "true")
colnames(own.NA.oldframe) <- c("Method", "Variable", "Value", "Diff")
stargazer(own.NA.oldframe, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. own.NA(), Old Framing Data (n = 1,003)",
          label = "own.NA.oldframe.acc")

```


`ampute()` seems to spread NAs evenly across columns. This means that observations are mostly complete, with not more than one or two missing values. I wrote another function, `own.NA.rows()`, that changes this. `own.NA.rows()` inserts missingness for a percentage of observations MAR across all columns except `education`. This means that the majority of observations are complete but a percentage of observations misses data on almost all variables. Table \ref{own.NA.rows.oldframe.acc} shows the results, with the missingess percentage set to 20 and NAs inserted into 17 variables.



```{r include=FALSE}
own.NA.rows.oldframe <- read.csv("data/framing/appendix/own.na.rows/framing.own.na.rows.results.17var.1003n.10000it.20perc.csv") %>% .[,-1] %>% abs.diff
levels(own.NA.rows.oldframe$method) <- c("amelia", "hd.norm", "hd.ord", "mice", "na.omit", "true")
colnames(own.NA.rows.oldframe) <- c("Method", "Variable", "Value", "Diff")
# to make the in-text citations shorter
diff <- own.NA.rows.oldframe$Diff
meth <- own.NA.rows.oldframe$Method
varb <- own.NA.rows.oldframe$Variable
stargazer(own.NA.rows.oldframe, 
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          digits = 4,
          title = "Accuracy of Multiple Imputation Methods. own.NA.rows(), Old Framing Data (n = 1,003)",
          label = "own.NA.rows.oldframe.acc")

```


\ssp

\begin{longtable}{@{\extracolsep{5pt}} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} D{.}{.}{-4} } 
  \caption{Accuracy of Multiple Imputation Methods. own.NA.rows(), old framing data (n = 1,003)} 
  \label{own.NA.rows.oldframe.acc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{Value} & \multicolumn{1}{c}{Diff} \\ 
\hline \\[-1.8ex] 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Dem} & .4666 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Dem} & .4666 & 0 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Dem} & .4667 & .0001 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Dem} & .4667 & .0001 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Dem} & .4670 & .0004 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Dem} & .4667 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Ind} & .2802 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Ind} & .2795 & .0007 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Ind} & .2801 & .0001 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Ind} & .2801 & .0001 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Ind} & .2817 & .0015 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Ind} & .2801 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Cons} & .2832 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Cons} & .2830 & .0002 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Cons} & .2831 & .0001 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Cons} & .2831 & .0001 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Cons} & .2823 & .0009 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Cons} & .2831 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Lib} & .5174 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Lib} & .5182 & .0008 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Lib} & .5176 & .0002 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Lib} & .5176 & .0002 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Lib} & .5173 & .0001 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Lib} & .5176 & .0002 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Black} & .0698 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Black} & .0702 & .0004 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Black} & .0698 & 0 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Black} & .0698 & 0 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Black} & .0731 & .0033 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Black} & .0698 & 0 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Hisp} & .0548 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Hisp} & .0546 & .0002 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Hisp} & .0548 & 0 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Hisp} & .0548 & 0 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Hisp} & .0589 & .0041 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Hisp} & .0548 & 0 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{White} & .7717 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{White} & .7712 & .0005 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{White} & .7717 & 0 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{White} & .7717 & 0 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{White} & .7613 & .0104 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{White} & .7717 & 0 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Asian} & .0808 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Asian} & .0812 & .0004 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Asian} & .0808 & 0 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Asian} & .0809 & .0001 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Asian} & .0835 & .0027 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Asian} & .0808 & 0 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Female} & .4666 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Female} & .4665 & .0001 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Female} & .4665 & .0001 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Female} & .4665 & .0001 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Female} & .4673 & .0007 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Female} & .4665 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Unempl} & .1615 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Unempl} & .1610 & .0005 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Unempl} & .1616 & .0001 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Unempl} & .1616 & .0001 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Unempl} & .1624 & .0009 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Unempl} & .1616 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Ret} & .0508 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Ret} & .0506 & .0002 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Ret} & .0508 & 0 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Ret} & .0509 & .0001 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Ret} & .0505 & .0003 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Ret} & .0509 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{Stud} & .0439 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{Stud} & .0446 & .0007 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{Stud} & .0439 & 0 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{Stud} & .0439 & 0 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{Stud} & .0466 & .0027 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{Stud} & .0439 & 0 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{interest} & 3.2164 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{interest} & 3.2161 & .0003 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{interest} & 3.2163 & .0001 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{interest} & 3.2164 & 0 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{interest} & 3.2122 & .0042 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{interest} & 3.2163 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{media} & 1.7268 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{media} & 1.7294 & .0026 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{media} & 1.7270 & .0002 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{media} & 1.7270 & .0002 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{media} & 1.7259 & .0009 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{media} & 1.7269 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{part} & .9561 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{part} & .9575 & .0014 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{part} & .9560 & .0001 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{part} & .9561 & 0 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{part} & .9546 & .0015 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{part} & .9561 & 0 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{inc} & 3.0927 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{inc} & 3.0906 & .0021 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{inc} & 3.0926 & .0001 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{inc} & 3.0926 & .0001 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{inc} & 3.0949 & .0022 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{inc} & 3.0926 & .0001 \\ 
\multicolumn{1}{c}{true} & \multicolumn{1}{c}{age} & 37.9252 & 0 \\ 
\multicolumn{1}{c}{hd.ord} & \multicolumn{1}{c}{age} & 37.9000 & .0252 \\ 
\multicolumn{1}{c}{hd.norm} & \multicolumn{1}{c}{age} & 37.9250 & .0002 \\ 
\multicolumn{1}{c}{amelia} & \multicolumn{1}{c}{age} & 37.9243 & .0009 \\ 
\multicolumn{1}{c}{mice} & \multicolumn{1}{c}{age} & 37.8789 & .0463 \\ 
\multicolumn{1}{c}{na.omit} & \multicolumn{1}{c}{age} & 37.9250 & .0002 \\ 
\hline \\[-1.8ex] 
\end{longtable} 

\dsp

`hd.norm`, `amelia`, and `na.omit` perform very well. No difference to the true value for any variable is greater than `r diff[meth == "amelia" & varb == "age"]` and most are `r diff[meth == "hd.norm" & varb == "media"]`. 

`hd.ord` performs on similar levels except for the nominal variables (`media` (`r diff[meth == "hd.ord" & varb == "media"]`), `part` (`r diff[meth == "hd.ord" & varb == "part"]`), `inc` (`r diff[meth == "hd.ord" & varb == "inc"]`), `age` (`r diff[meth == "hd.ord" & varb == "age"]`)).

Somewhat surprisingly, `mice` performs worst overall for many variables (`Ind` (`r diff[meth == "mice" & varb == "Ind"]`), `Black` (`r diff[meth == "mice" & varb == "Black"]`), `Hisp` (`r diff[meth == "mice" & varb == "Hisp"]`), `White` (`r diff[meth == "mice" & varb == "White"]`), `Asian` (`r diff[meth == "mice" & varb == "Asian"]`), `Stud` (`r diff[meth == "mice" & varb == "Stud"]`), `interest` (`r diff[meth == "mice" & varb == "interest"]`), `part` (`r diff[meth == "mice" & varb == "part"]`), `inc` (`r diff[meth == "mice" & varb == "inc"]`), `age` (`r diff[meth == "mice" & varb == "age"]`)) by some margin.

It is also noteable how often the difference amounts to zero and how well `na.omit` performs overall. Overall, `own.NA` and `own.NA.rows` result in a strong performance of `na.omit`, which should not happen in a missingness mechanism that is supposed to be MAR. The functions thus likely represents a version of MCAR, which puts the usefulness of their imputation results in doubt.

-->



\clearpage 


## Imputing 12 Variables with Missing Data {#app-ordmiss-12var}

```{r MAR 12 Variables, include=FALSE}

mar.12var.anes <- read.csv("data/anes/mar/results/anes.mar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mar.12var.cces <- read.csv("data/cces/mar/results/cces.mar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mar.12var.frame<- read.csv("data/framing/mar/results/framing.mar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mar.12var.anes[1:6, 2] <- mar.12var.cces[1:6, 2] <- rep("Democrat", 6)
mar.12var.anes[19:24, 2] <- mar.12var.cces[19:24, 2] <- rep("Income", 6)
mar.12var.anes[37:42, 2] <- mar.12var.cces[37:42, 2] <- rep("Employed", 6)

mar.12var.anes$diff[mar.12var.anes$method == "true"] <- mar.12var.anes$value[mar.12var.anes$method == "true"]
mar.12var.cces$diff[mar.12var.cces$method == "true"] <- mar.12var.cces$value[mar.12var.cces$method == "true"]
# mar.12var.frame$diff[mar.12var.frame$method == "true"] <- mar.12var.frame$value[mar.12var.frame$method == "true"]

levels(mar.12var.anes$method) <- levels(mar.12var.cces$method) <- levs
# levels(mar.12var.frame$method) <-levs

un.meth <- mar.12var.anes$method %>% unique
un.meth.len <- un.meth %>% length
un.vars <- c(mar.12var.anes$variable %>% as.character,
             mar.12var.cces$variable %>% as.character) %>%
  unique
# un.vars <- c(mar.12var.anes$variable %>% as.character,
#                  mar.12var.cces$variable %>% as.character, 
#                  mar.12var.frame$variable %>% as.character) %>%
#   unique
un.vars.len <- un.vars %>% length
un.anes.len <- mar.12var.anes$variable %>% unique %>% length
un.cces.var <- mar.12var.cces$variable %>% unique %>% as.character
un.cces.len <- un.cces.var %>% length
# un.frame.var <- mar.12var.frame$variable %>% unique %>% as.character
# un.frame.len <- un.frame.var %>% length
Variable <- rep(un.vars, each = un.meth.len)
Method <- rep(un.meth, un.vars.len) %>% as.character
meth.len <- Method %>% length

mar.12var <- cbind(Method, Variable) %>% as.data.frame

mar.12var$mar.anes.col <- c(mar.12var.anes$diff, 
                            rep("---", 
                                un.meth.len * (un.vars.len - un.anes.len)))
mar.12var$mar.cces.col <- rep("---", meth.len) %>% as.character
# mar.12var$mar.frame.col <- rep("---", meth.len) %>% as.character

for(i in 1:(un.cces.len)){
  mar.12var$mar.cces.col[mar.12var$Variable == un.cces.var[i]] <- mar.12var.cces$diff[mar.12var.cces$variable == un.cces.var[i]]
  # mar.12var$mar.frame.col[mar.12var$Variable == un.frame.var[i]] <- mar.12var.frame$diff[mar.12var.frame$variable == un.frame.var[i]]
}

colnames(mar.12var) <- col.names

# to make the in-text citations shorter
mar.12.anes <- mar.12var$ANES
mar.12.cces <- mar.12var$CCES
# mar.12.frame <- mar.12var$Framing
mar.12.meth <- mar.12var$Method
mar.12.var <- mar.12var$Variable

tab.mar.12var <- stargazer(mar.12var, 
                           summary = FALSE,
                           align = TRUE,
                           header = FALSE,
                           rownames = FALSE,
                           digits = 4,
                           title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, MAR, 12 Variables with NA",
                           label = "mar.12var")

mt <- gsub("\\multicolumn{1}{c}{", "", tab.mar.12var, fixed = TRUE)
cat(mt)


```

<!--
MAR 12 Var
Explain vars: First 5 vars the same as for MAR 5 Var. Black, Empl, Religious, Married, OwnHome, Rally, Donate, Gay, StudLoans, Hisp, Official, Stud binary. Media and Participation ordinal (1-5). 
	Binary
		Similar picture to MAR 5 Var, though overall hd.ord arguably closer. Max difference now .0006 (Empl ANES and framing)
		amelia and mice best again, often actually zero difference to true value
	Ordinal
		Same picture as before: hd.ord is worst across all ds for all vars (including Media and Participation framing)
		mice and amelia again by far best
		hd.ord less worse than for MAR 5 Var in terms of performance differences. hd.ord's max diff for Interest is now .0106 (framing) (compared to .0248 (framing) for 5 Var). Possibly explained by thinner spread of NAs, so fewer NAs per variable. Media and Participation framing results confirm those for Interest across all ds
	interval
		Same picture as before: hd.ord is worst across all ds
		Difference to hot.deck still there but less pronounced than for MAR 5 Var
		mice overall better than amelia for Inc, though .0007 vs. .0013 ANES for amelia
		mice also overall better than amelia for Age, though .0015 vs. .0050 CCES for amelia
		As for ordinal, hd.ord less worse than for MAR 5 Var in terms of performance differences but consistent
-->

Table \ref{mar.12var} shows the results of imputing both data sets MAR for 12 amputed variables. The first five listed variables are the same as for the MAR analysis for five amputed variables in Table \ref{mar.5var}. The remaining variables were chosen based on availability in each data set. As much as possible, the same variables were selected across both data sets. `Black`, `Employed`, `Religious`, `Married`, `OwnHome`, `Rally`, `Donate`, `Gay`, `StudLoans`, `Hispanic`, `Official`, and `Student` are binary variables. `Black` indicates whether a respondent is of African-American origin, `Employed` whether she is currently employed, `Religious` whether she follows a religious belief, `Married` whether she is currently married, `OwnHome` whether she owns her home, `Rally` whether she has attended a political rally, `Donate` whether she has donated to a political candidate, `Gay` whether she identifies as homosexual, `StudLoans` whether she currently has student loans, `Hispanic` whether she is of Hispanic origin, `Official` whether she has contacted her political representative, and `Student` whether she currently is a student. `Media` is an ordinal variable and indicates how much she follows public affairs in the media (scaled from 1 to 5). `Participation` is an interval variable and shows the accumulative count of political activities she has participated in (scaled from 0 to 4).

\ssp

\footnotesize

\begin{longtable}{ccr@{}lr@{}l} 
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, MAR, 12 Variables with NA}   
 \label{mar.12var} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex] 
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3420 & 0.&3770 \\ 
 hd.ord & Democrat & --0.&0005 & --0.&0003 \\
 hot.deck & Democrat & --0.&0005 & --0.&0004 \\
 amelia & Democrat & + 0.&0000 & + 0.&0000 \\
 mice & Democrat & + 0.&0000 & + 0.&0001 \\
 na.omit & Democrat & --0.&0191 & --0.&0172 \\
 true & Male & 0.&4890 & 0.&4830 \\ 
 hd.ord & Male & --0.&0004 & --0.&0002 \\
 hot.deck & Male & --0.&0001 & --0.&0003 \\  
 amelia & Male & + 0.&0001 & --0.&0001 \\
 mice & Male & + 0.&0000 & --0.&0002 \\
 na.omit & Male & --0.&0256 & --0.&0364 \\
 true & Interest & 2.&9340 & 3.&3290 \\
 hd.ord & Interest & --0.&0053 & --0.&0041 \\
 hot.deck & Interest & --0.&0077 & --0.&0067 \\
 amelia & Interest & + 0.&0001 & --0.&0001 \\
 mice & Interest & + 0.&0000 & --0.&0001 \\ 
 na.omit & Interest & --0.&0620 & --0.&0515 \\
 true & Income & 16.&6140 & 6.&4810 \\ 
 hd.ord & Income & --0.&0470 & --0.&0130 \\ 
 hot.deck & Income & --0.&0591 & --0.&0212 \\
 amelia & Income & --0.&0007 & --0.&0005 \\
 mice & Income & --0.&0013 & --0.&0003 \\ 
 na.omit & Income & --0.&6303 & --0.&2860 \\
 true & Age & 50.&0410 & 52.&8230 \\
 hd.ord & Age & --0.&1391 & --0.&0883 \\ 
 hot.deck & Age & --0.&1835 & --0.&1435 \\
 amelia & Age & + 0.&0056 & --0.&0015 \\ 
 mice & Age & + 0.&0048 & --0.&0050 \\ 
 na.omit & Age & --0.&8638 & --0.&5974 \\
 true & Black & 0.&0790 & 0.&0950 \\ 
 hd.ord & Black & + 0.&0000 & + 0.&0000 \\
 hot.deck & Black & + 0.&0000 & + 0.&0000 \\ 
 amelia & Black & + 0.&0000 & + 0.&0001 \\
 mice & Black & + 0.&0000 & + 0.&0001 \\ 
 na.omit & Black & --0.&0092 & --0.&0090 \\
 true & Employed & 0.&6610 & 0.&4370 \\ 
 hd.ord & Employed & + 0.&0006 & + 0.&0000 \\ 
 hot.deck & Employed & + 0.&0006 & + 0.&0001 \\
 amelia & Employed & + 0.&0000 & + 0.&0000 \\
 mice & Employed & + 0.&0000 & --0.&0001 \\
 na.omit & Employed & --0.&0087 & --0.&0301 \\
 true & Religious & 0.&6460 & 0.&6420 \\ 
 hd.ord & Religious & --0.&0006 & --0.&0003 \\ 
 hot.deck & Religious & --0.&0005 & --0.&0003 \\
 amelia & Religious & --0.&0001 & --0.&0001 \\
 mice & Religious & --0.&0001 & --0.&0002 \\ 
 na.omit & Religious & --0.&0166 & --0.&0234 \\ 
 true & Married & 0.&5290 & 0.&6310 \\
 hd.ord & Married & + 0.&0002 & --0.&0001 \\
 hot.deck & Married & + 0.&0002 & + 0.&0001 \\
 amelia & Married & --0.&0001 & --0.&0002 \\
 mice & Married & --0.&0001 & --0.&0002 \\ 
 na.omit & Married & --0.&0384 & --0.&0326 \\ 
 true & OwnHome & 0.&6820 & 0.&7010 \\
 hd.ord & OwnHome & --0.&0001 & --0.&0002 \\ 
 hot.deck & OwnHome & + 0.&0000 & + 0.&0000 \\ 
 amelia & OwnHome & + 0.&0001 & + 0.&0000 \\ 
 mice & OwnHome & --0.&0001 & --0.&0001 \\
 na.omit & OwnHome & --0.&0334 & --0.&0304 \\
 true & Rally & 0.&0830 & --- \\
 hd.ord & Rally & --0.&0001 & --- \\ 
 hot.deck & Rally & --0.&0002 & --- \\ 
 amelia & Rally & + 0.&0001 & --- \\ 
 mice & Rally & + 0.&0001 & --- \\ 
 na.omit & Rally & --0.&0191 & --- \\ 
 true & Donate & 0.&1390 & --- \\ 
 hd.ord & Donate & --0.&0002 & --- \\ 
 hot.deck & Donate & --0.&0005 & --- \\
 amelia & Donate & + 0.&0000 & --- \\ 
 mice & Donate & + 0.&0001 & --- \\ 
 na.omit & Donate & --0.&0320 & --- \\ 
 true & Gay & \multicolumn{2}{l}{---} & 0.&0420 \\ 
 hd.ord & Gay & \multicolumn{2}{l}{---}  & + 0.&0001 \\
 hot.deck & Gay & \multicolumn{2}{l}{---} & + 0.&0000 \\ 
 amelia & Gay & \multicolumn{2}{l}{---} & + 0.&0000 \\
 mice & Gay & \multicolumn{2}{l}{---} & + 0.&0000 \\
 na.omit & Gay & \multicolumn{2}{l}{---} &  --0.&0112 \\
 true & StudLoans & \multicolumn{2}{l}{---} & 0.&1910 \\
 hd.ord & StudLoans & \multicolumn{2}{l}{---} & + 0.&0003 \\
 hot.deck & StudLoans & \multicolumn{2}{l}{---} & + 0.&0002 \\
 amelia & StudLoans & \multicolumn{2}{l}{---} & + 0.&0000 \\
 mice & StudLoans & \multicolumn{2}{l}{---} & --0.&0001 \\
 na.omit & StudLoans & \multicolumn{2}{l}{---} & --0.&0117 \\ 
 \hline \\[-1.8ex]
 \end{longtable}

\dsp

\normalsize

The results are consistent with those presented in Table \ref{mar.5var}. For the binary variables, `amelia` and `mice` again perform best with results actually matching the true variable values, though `hd.ord` arguably shows closer results than in Table \ref{mar.5var} with a maximum difference to a true value of `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Employed"]` (ANES `Employed`). For the ordinal variable, `hd.ord` continues to perform worst across both data sets. `mice` and `amelia` again show the best results. Note, however, that `hd.ord` is less worse in terms of performance differences when compared to the MAR analysis of five imputed variables. The maximum difference to the true `Interest` value is `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Interest"]` (ANES) for 12 variables but `r mar.5.anes[mar.12.meth == "hd.ord" & mar.12.var == "Interest"]` (ANES) for five variables. This is possibly explained by a thinner spread of missing values across a higher number of variables, resulting in a lower number of NAs in each amputed variable. 

The results for the interval variables follow the same pattern: `hd.ord` displays the worst results for both data sets. The difference to `hot.deck` is still present though less pronounced than in the MAR analysis of five imputed variables. `mice` overall performs better than `amelia` for `Income` and `Age`, with the exceptions of ANES `Income` (`r mar.12.anes[mar.12.meth == "amelia" & mar.12.var == "Income"]` vs. `r mar.12.anes[mar.12.meth == "mice" & mar.12.var == "Income"]`) and CCES `Age` (`r mar.12.cces[mar.12.meth == "amelia" & mar.12.var == "Age"]` vs. `r mar.12.cces[mar.12.meth == "mice" & mar.12.var == "Age"]`).


```{r MNAR 12 Variables, include=FALSE}

mnar.12var.anes <- read.csv("data/anes/mnar/results/anes.mnar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mnar.12var.cces <- read.csv("data/cces/mnar/results/cces.mnar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mnar.12var.frame <- read.csv("data/framing/mnar/results/framing.mnar.results.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mnar.12var.anes[1:6, 2] <- mnar.12var.cces[1:6, 2] <- rep("Democrat", 6)
mnar.12var.anes[19:24, 2] <- mnar.12var.cces[19:24, 2] <- rep("Income", 6)
mnar.12var.anes[37:42, 2] <- mnar.12var.cces[37:42, 2] <- rep("Employed", 6)

mnar.12var.anes$diff[mnar.12var.anes$method == "true"] <- mnar.12var.anes$value[mnar.12var.anes$method == "true"]
mnar.12var.cces$diff[mnar.12var.cces$method == "true"] <- mnar.12var.cces$value[mnar.12var.cces$method == "true"]
# mnar.12var.frame$diff[mnar.12var.frame$method == "true"] <- mnar.12var.frame$value[mnar.12var.frame$method == "true"]

levels(mnar.12var.anes$method) <- levels(mnar.12var.cces$method) <- levs
# levels(mnar.12var.frame$method) <- levs

# there are several in-between code steps that are identical to MAR 12 Variables
# above, so I didn't include them here

mnar.12var <- cbind(Method, Variable) %>% as.data.frame

mnar.12var$mnar.anes.col <- c(mnar.12var.anes$diff,
                              rep("---",
                                  un.meth.len * (un.vars.len - un.anes.len)))
mnar.12var$mnar.cces.col <- rep("---", meth.len) %>% as.character
# mnar.12var$mnar.frame.col <- rep("---", meth.len) %>% as.character

for(i in 1:(un.cces.len)){
  mnar.12var$mnar.cces.col[mnar.12var$Variable == un.cces.var[i]] <- mnar.12var.cces$diff[mnar.12var.cces$variable == un.cces.var[i]]
  # mnar.12var$mnar.frame.col[mnar.12var$Variable == un.frame.var[i]] <- mnar.12var.frame$diff[mnar.12var.frame$variable == un.frame.var[i]]
}

colnames(mnar.12var) <- col.names

# to make the in-text citations shorter
mnar.12.anes <- mnar.12var$ANES
mnar.12.cces <- mnar.12var$CCES
# mnar.12.frame <- mnar.12var$Framing
mnar.12.meth <- mnar.12var$Method
mnar.12.var <- mnar.12var$Variable

tab.mnar.12var <- stargazer(mnar.12var,
                            summary = FALSE,
                            align = TRUE,
                            header = FALSE,
                            rownames = FALSE,
                            digits = 4,
                            title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, MNAR, 12 Variables with NA",
                            label = "mnar.12var")

nt <- gsub("\\multicolumn{1}{c}{", "", tab.mnar.12var, fixed = TRUE)
cat(nt)


```

<!--
MNAR 12 VAR
Vars the same as for MNAR 5 Var
	Binary
		Similar picture to MNAR 5 Var. amelia and mice perform better, but often not by much. Occasionally, hd.ord eclipses them (.0013 hd.ord vs. .0014 mice Dem framing; 0.0053 hd.ord vs. .0055 mice and amelia Male ANES).
		na.omit again often performs close to the other methods 
	Ordinal
		Exactly the same as for MNAR 5 Var. hd.ord worst across all ds. mice and amelia far better and virtually identical (including Media and Participation framing).
		na.omit further off than for MNAR 5 Var
	interval
		Same method performance as for MNAR 5 Var.
		na.omit better than hot.deck and hd.ord for Age framing (.2239 vs. .2811 and .2989) and Age CCES (.1367 vs. .1732 and .2251)
-->

Table \ref{mnar.12var} shows the results of imputing both data sets MNAR for 12 amputed variables. The results are consistent with those obtained for five amputed variables MNAR. `amelia` and `mice` perform better than `hd.ord` for the binary variables overall, but often not by much. Occasionally, `hd.ord` eclipses them (`r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Male"]` vs. `r mnar.12.anes[mnar.12.meth == "mice" & mnar.12.var == "Male"]` `mice` and `amelia` ANES `Male`). `na.omit` once more performs close to the other methods.

\ssp

\footnotesize

\begin{longtable}{ccr@{}lr@{}l} 
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, MNAR, 12 Variables with NA}  
 \label{mnar.12var} 
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex]
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3420 & 0.&3770 \\
 hd.ord & Democrat & --0.&0049 & --0.&0046 \\
 hot.deck & Democrat & --0.&0049 & --0.&0049 \\
 amelia & Democrat & --0.&0043 & --0.&0045 \\
 mice & Democrat & --0.&0040 & --0.&0044 \\ 
 na.omit & Democrat & --0.&0092 & --0.&0081 \\
 true & Male & 0.&4890 & 0.&4830 \\ 
 hd.ord & Male & --0.&0055 & --0.&0049 \\ 
 hot.deck & Male & --0.&0053 & --0.&0051 \\ 
 amelia & Male & --0.&0055 & --0.&0050 \\
 mice & Male & --0.&0055 & --0.&0049 \\
 na.omit & Male & --0.&0093 & --0.&0119 \\ 
 true & Interest & 2.&9340 & 3.&3290 \\
 hd.ord & Interest & --0.&0113 & --0.&0090 \\ 
 hot.deck & Interest & --0.&0134 & --0.&0113 \\
 amelia & Interest & --0.&0068 & --0.&0061 \\
 mice & Interest & --0.&0068 & --0.&0061 \\
 na.omit & Interest & --0.&0236 & --0.&0161 \\ 
 true & Income & 16.&6140 & 6.&4810 \\ 
 hd.ord & Income & --0.&0899 & --0.&0350 \\ 
 hot.deck & Income & --0.&1046 & --0.&0421 \\
 amelia & Income & --0.&0495 & --0.&0223 \\
 mice & Income & --0.&0503 & --0.&0218 \\ 
 na.omit & Income & --0.&2088 & --0.&0970 \\
 true & Age & 50.&0410 & 52.&8230 \\ 
 hd.ord & Age & --0.&2571 & --0.&1732 \\ 
 hot.deck & Age & --0.&3081 & --0.&2251 \\
 amelia & Age & --0.&1100 & --0.&1014 \\ 
 mice & Age & --0.&1047 & --0.&0986 \\ 
 na.omit & Age & --0.&3397 & --0.&1367 \\
 true & Black & 0.&0790 & 0.&0950 \\
 hd.ord & Black & --0.&0035 & --0.&0038 \\
 hot.deck & Black & --0.&0037 & --0.&0038 \\ 
 amelia & Black & --0.&0037 & --0.&0040 \\ 
 mice & Black & --0.&0034 & --0.&0038 \\
 na.omit & Black & --0.&0045 & --0.&0052 \\
 true & Employed & 0.&6610 & 0.&4370 \\
 hd.ord & Employed & --0.&0034 & --0.&0053 \\
 hot.deck & Employed & --0.&0033 & --0.&0053 \\
 amelia & Employed & --0.&0031 & --0.&0040 \\
 mice & Employed & --0.&0031 & --0.&0040 \\ 
 na.omit & Employed & --0.&0014 & --0.&0111 \\ 
 true & Religious & 0.&6460 & 0.&6420 \\ 
 hd.ord & Religious & --0.&0045 & --0.&0039 \\
 hot.deck & Religious & --0.&0043 & --0.&0038 \\
 amelia & Religious & --0.&0040 & --0.&0040 \\
 mice & Religious & --0.&0040 & --0.&0040 \\ 
 na.omit & Religious & --0.&0049 & --0.&0073 \\ 
 true & Married & 0.&5290 & 0.&6310 \\ 
 hd.ord & Married & --0.&0041 & --0.&0038 \\
 hot.deck & Married & --0.&0040 & --0.&0037 \\ 
 amelia & Married & --0.&0042 & --0.&0037 \\ 
 mice & Married & --0.&0042 & --0.&0037 \\ 
 na.omit & Married & --0.&0122 & --0.&0096 \\ 
 true & OwnHome & 0.&6820 & 0.&7010 \\ 
 hd.ord & OwnHome & --0.&0030 & --0.&0030 \\ 
 hot.deck & OwnHome & --0.&0028 & --0.&0027 \\
 amelia & OwnHome & --0.&0027 & --0.&0030 \\ 
 mice & OwnHome & --0.&0027 & --0.&0030 \\ 
 na.omit & OwnHome & --0.&0111 & --0.&0090 \\
 true & Rally & 0.&0830 & --- \\ 
 hd.ord & Rally & --0.&0043 & --- \\ 
 hot.deck & Rally & --0.&0042 & --- \\ 
 amelia & Rally & --0.&0040 & --- \\ 
 mice & Rally & --0.&0039 & --- \\ 
 na.omit & Rally & --0.&0077 & --- \\  
 true & Donate & 0.&1390 & --- \\ 
 hd.ord & Donate & --0.&0051 & --- \\ 
 hot.deck & Donate & --0.&0054 & --- \\ 
 amelia & Donate & --0.&0049 & --- \\ 
 mice & Donate & --0.&0048 & --- \\ 
 na.omit & Donate & --0.&0122 & --- \\ 
 true & Gay & \multicolumn{2}{l}{---} & 0.&0420 \\ 
 hd.ord & Gay & \multicolumn{2}{l}{---} & --0.&0024 \\ 
 hot.deck & Gay & \multicolumn{2}{l}{---} & --0.&0025 \\ 
 amelia & Gay & \multicolumn{2}{l}{---} & --0.&0026 \\
 mice & Gay & \multicolumn{2}{l}{---} & --0.&0024 \\ 
 na.omit & Gay & \multicolumn{2}{l}{---} & --0.&0035 \\ 
 true & StudLoans & \multicolumn{2}{l}{---} & 0.&1910 \\
 hd.ord & StudLoans & \multicolumn{2}{l}{---} & --0.&0058 \\
 hot.deck & StudLoans & \multicolumn{2}{l}{---} & --0.&0058 \\ 
 amelia & StudLoans & \multicolumn{2}{l}{---} & --0.&0051 \\ 
 mice & StudLoans & \multicolumn{2}{l}{---} & --0.&0050 \\ 
 na.omit & StudLoans & \multicolumn{2}{l}{---} & --0.&0070 \\ 
 \hline \\[-1.8ex]
 \end{longtable} 
 
 \dsp

\normalsize

For the ordinal variable, `hd.ord` shows the worst performance across both data sets. `mice` and `amelia` perform far better and display virtually identical results. `na.omit` does not perform as well as in the corresponding MAR analysis. `mice` and `amelia`'s superior performance is also visible in the results for the ordinal variables. `na.omit` performs better than `hot.deck` and `hd.ord` for CCES `Age` (`r mnar.12.cces[mnar.12.meth == "na.omit" & mnar.12.var == "Age"]` vs. `r mnar.12.cces[mnar.12.meth == "hot.deck" & mnar.12.var == "Age"]` and `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Age"]`). 



## Imputing 11 Variables with Missing Data for Two Ordinal Variables {#app-ordmiss-mult-11var}

```{r MULT MAR 11 Variables, include=FALSE}

mult.mar.11var.anes <- read.csv("data/anes/mar/results/anes.mar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mult.mar.11var.cces <- read.csv("data/cces/mar/results/cces.mar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mult.mar.11var.frame <- read.csv("data/framing/mar/results/framing.mar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mult.mar.11var.anes[1:6, 2] <- mult.mar.11var.cces[1:6, 2] <- rep("Democrat", 6)
mult.mar.11var.anes[13:18, 2] <- mult.mar.11var.cces[13:18, 2] <- rep("Income", 6)
mult.mar.11var.anes[31:36, 2] <- mult.mar.11var.cces[31:36, 2] <- rep("Employed", 6)

mult.mar.11var.anes$diff[mult.mar.11var.anes$method == "true"] <- mult.mar.11var.anes$value[mult.mar.11var.anes$method == "true"]
mult.mar.11var.cces$diff[mult.mar.11var.cces$method == "true"] <- mult.mar.11var.cces$value[mult.mar.11var.cces$method == "true"]
# mult.mar.11var.frame$diff[mult.mar.11var.frame$method == "true"] <- mult.mar.11var.frame$value[mult.mar.11var.frame$method == "true"]

levels(mult.mar.11var.anes$method) <- levels(mult.mar.11var.cces$method) <- levs
# levels(mult.mar.11var.frame$method) <- levs


mult.un.meth <- mult.mar.11var.anes$method %>% unique
mult.un.meth.len <- mult.un.meth %>% length
mult.un.vars <- c(mult.mar.11var.anes$variable %>% as.character,
                  mult.mar.11var.cces$variable %>% as.character) %>%
  unique
# mult.un.vars <- c(mult.mar.11var.anes$variable %>% as.character,
#                   mult.mar.11var.cces$variable %>% as.character,
#                   mult.mar.11var.frame$variable %>% as.character) %>%
#   unique
mult.un.vars.len <- mult.un.vars %>% length
mult.un.anes.len <- mult.mar.11var.anes$variable %>% unique %>% length
mult.un.cces.var <- mult.mar.11var.cces$variable %>% unique %>% as.character
mult.un.cces.len <- mult.un.cces.var %>% length
# mult.un.frame.var <- mult.mar.11var.frame$variable %>% unique %>% as.character
# mult.un.frame.len <- mult.un.frame.var %>% length

mult.variable <- rep(mult.un.vars, each = mult.un.meth.len)
mult.method <- rep(mult.un.meth, mult.un.vars.len) %>% as.character
mult.meth.len <- mult.method %>% length

mult.mar.11var <- cbind(mult.method, mult.variable) %>% as.data.frame

mult.mar.11var$mult.anes.col <- c(mult.mar.11var.anes$diff,
                                  rep("---",
                                      mult.un.meth.len * (mult.un.vars.len - mult.un.anes.len)))
mult.mar.11var$mult.cces.col <- rep("---", mult.meth.len) %>% as.character
# mult.mar.11var$mult.frame.col <- rep("---", mult.meth.len) %>% as.character

for(i in 1:(mult.un.cces.len)){
  mult.mar.11var$mult.cces.col[mult.mar.11var$mult.variable == mult.un.cces.var[i]] <- mult.mar.11var.cces$diff[mult.mar.11var.cces$variable == mult.un.cces.var[i]]
  # mult.mar.11var$mult.frame.col[mult.mar.11var$mult.variable == mult.un.frame.var[i]] <- mult.mar.11var.frame$diff[mult.mar.11var.frame$variable == mult.un.frame.var[i]]
}

colnames(mult.mar.11var) <- col.names

# to make the in-text citations shorter
mult.mar.11.anes <- mult.mar.11var$ANES
mult.mar.11.cces <- mult.mar.11var$CCES
# mult.mar.11.frame <- mult.mar.11var$Framing
mult.mar.11.meth <- mult.mar.11var$Method
mult.mar.11.var <- mult.mar.11var$Variable

tab.mult.mar.11var <- stargazer(mult.mar.11var, 
                                summary = FALSE,
                                align = TRUE,
                                header = FALSE,
                                rownames = FALSE,
                                digits = 4,
                                title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MAR, 11 Variables with NA",
                                label = "mult.mar.11var")

ot <- gsub("\\multicolumn{1}{c}{", "", tab.mult.mar.11var, fixed = TRUE)
cat(ot)

```

<!--
MAR 11 Var
Vars the same as for MAR and MNAR 12 Var, just without Interest (so no ordinal vars here)
	Binary
		Similar picture to MAR 4 Var, though overall hd.ord arguably closer now
		amelia and mice best again, often actually zero difference to true value
		Consistent with MAR 4 Var/5 Var development, hd.ord performs slightly worse with two ordinal variables when compared to MAR 12 Var:
		MAR 12 Var hd.ord: Dem .0005, .0004, .0004. Male .0001, .0003, .0000
		MAR 11 Var hd.ord: Dem .0005, .0005, .0005. Male .0001, .0004, .0002
	Ordinal framing
		hd.ord worst for Media and Participation, confirming previous results
		mice and amelia again by far best
		hd.ord gets slightly better when compared to MAR 12 Var:
		MAR 12 Var hd.ord: Media .0060. Participation .0027
		MAR 11 Var hd.ord: Media .0059, Participation .0026
	interval
		Same picture as before: hd.ord is worst across all ds
		Difference to hot.deck still there but less pronounced than for MAR 4 Var
		amelia is better for age, mice is better for Inc (except Inc ANES, .0027 vs. .0018)
		hd.ord consistently gets slightly worse when compared to MAR 12 Var:		
		MAR 12 Var hd.ord: Inc .0591, .0212, .0089. Age .1835, .1435, .1592 
		MAR 11 Var hd.ord: Inc .0657, .0241, .0095. Age .1951, .1532, .1640
-->

Table \ref{mult.mar.11var} shows the results of imputing both data sets with two `polr`-treated variables MAR for 11 amputed variables. A similar picture for Table \ref{mult.mar.4var} emerges for the binary variables, with `amelia` and `mice` once more displaying the best results, though `hd.ord` appears to perform somewhat closer here. Consistent with the deterioration of `hd.ord` results from Table \ref{mar.5var} to Table \ref{mult.mar.4var}, `hd.ord` again consistently performs slightly worse when compared to the MAR analysis with 12 amputed variables and only `Education` treated by `polr`: `r mult.mar.11.anes[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Democrat"]`, `r mult.mar.11.cces[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Democrat"]` vs. `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Democrat"]`, `r mar.12.cces[mar.12.meth == "hd.ord" & mar.12.var == "Democrat"]` for `Democrat` and `r mult.mar.11.anes[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Male"]`, `r mult.mar.11.cces[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Male"]` vs. `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Male"]`, `r mar.12.cces[mar.12.meth == "hd.ord" & mar.12.var == "Male"]` for `Male`.

\ssp

\footnotesize

\begin{longtable}{ccr@{}lr@{}l} 
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MAR, 11 Variables with NA} 
 \label{mult.mar.11var}  
 \\[-1.8ex]\hline 
 \hline \\[-1.8ex]
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex]
 true & Democrat & 0.&3420 & 0.&3770 \\ 
 hd.ord & Democrat & --0.&0002 & --0.&0004 \\ 
 hot.deck & Democrat & --0.&0005 & --0.&0005 \\ 
 amelia & Democrat & + 0.&0000 & --0.&0001 \\ 
 mice & Democrat & + 0.&0000 & + 0.&0000 \\ 
 na.omit & Democrat & --0.&0200 & --0.&0213 \\
 true & Male & 0.&4890 & 0.&4830 \\ 
 hd.ord & Male & --0.&0002 & --0.&0005 \\
 hot.deck & Male & --0.&0001 & --0.&0004 \\
 amelia & Male & + 0.&0000 & --0.&0001 \\
 mice & Male & + 0.&0000 & --0.&0001 \\ 
 na.omit & Male & --0.&0254 & --0.&0350 \\ 
 true & Income & 16.&6140 & 6.&4810 \\ 
 hd.ord & Income & --0.&0346 & --0.&0114 \\
 hot.deck & Income & --0.&0657 & --0.&0241 \\ 
 amelia & Income & --0.&0018 & --0.&0007 \\
 mice & Income & --0.&0027 & --0.&0005 \\
 na.omit & Income & --0.&6432 & --0.&2888 \\
 true & Age & 50.&0410 & 52.&8230 \\ 
 hd.ord & Age & --0.&0887 & --0.&0704 \\
 hot.deck & Age & --0.&1951 & --0.&1532 \\
 amelia & Age & + 0.&0010 & --0.&0021 \\
 mice & Age & --0.&0016 & --0.&0055 \\ 
 na.omit & Age & --0.&8025 & --0.&4330 \\
 true & Black & 0.&0790 & 0.&0950 \\
 hd.ord & Black & + 0.&0001 & --0.&0001 \\
 hot.deck & Black & + 0.&0000 & --0.&0001 \\
 amelia & Black & + 0.&0000 & + 0.&0001 \\ 
 mice & Black & + 0.&0001 & + 0.&0001 \\ 
 na.omit & Black & --0.&0103 & --0.&0122 \\ 
 true & Employed & 0.&6610 & 0.&4370 \\
 hd.ord & Employed & + 0.&0007 & + 0.&0003 \\
 hot.deck & Employed & + 0.&0008 & + 0.&0001 \\ 
 amelia & Employed & + 0.&0001 & + 0.&0000 \\
 mice & Employed & + 0.&0001 & --0.&0001 \\
 na.omit & Employed & --0.&0109 & --0.&0328 \\
 true & Religious & 0.&6460 & 0.&6420 \\ 
 hd.ord & Religious & --0.&0001 & --0.&0002 \\ 
 hot.deck & Religious & --0.&0005 & --0.&0003 \\ 
 amelia & Religious & + 0.&0000 & --0.&0001 \\
 mice & Religious & + 0.&0000 & --0.&0002 \\
 na.omit & Religious & --0.&0174 & --0.&0241 \\ 
 true & Married & 0.&5290 & 0.&6310 \\ 
 hd.ord & Married & --0.&0001 & --0.&0002 \\ 
 hot.deck & Married & + 0.&0003 & + 0.&0001 \\ 
 amelia & Married & --0.&0001 & --0.&0002 \\ 
 mice & Married & --0.&0001 & --0.&0002 \\ 
 na.omit & Married & --0.&0390 & --0.&0324 \\ 
 true & OwnHome & 0.&6820 & 0.&7010 \\ 
 hd.ord & OwnHome & --0.&0005 & --0.&0001 \\
 hot.deck & OwnHome & --0.&0001 & + 0.&0002 \\ 
 amelia & OwnHome & + 0.&0000 & + 0.&0001 \\
 mice & OwnHome & --0.&0001 & + 0.&0000 \\
 na.omit & OwnHome & --0.&0341 & --0.&0295 \\
 true & Rally & 0.&0830 & --- \\ 
 hd.ord & Rally & + 0.&0000 & --- \\ 
 hot.deck & Rally & --0.&0002 & --- \\
 amelia & Rally & + 0.&0001 & --- \\ 
 mice & Rally & + 0.&0002 & --- \\
 na.omit & Rally & --0.&0186 & --- \\ 
 true & Donate & 0.&1390 & --- \\ 
 hd.ord & Donate & --0.&0003 & --- \\ 
 hot.deck & Donate & --0.&0007 & --- \\
 amelia & Donate & --0.&0001 & --- \\
 mice & Donate & + 0.&0000 & --- \\ 
 na.omit & Donate & --0.&0310 & --- \\ 
 true & Gay & \multicolumn{2}{l}{---} & 0.&0420 \\ 
 hd.ord & Gay & \multicolumn{2}{l}{---} & + 0.&0001 \\ 
 hot.deck & Gay & \multicolumn{2}{l}{---} & + 0.&0000 \\ 
 amelia & Gay & \multicolumn{2}{l}{---} & + 0.&0000 \\ 
 mice & Gay & \multicolumn{2}{l}{---} & + 0.&0001 \\
 na.omit & Gay & \multicolumn{2}{l}{---} & --0.&0113 \\ 
 true & StudLoans & \multicolumn{2}{l}{---} & 0.&1910 \\
 hd.ord & StudLoans & \multicolumn{2}{l}{---} & + 0.&0001 \\ 
 hot.deck & StudLoans & \multicolumn{2}{l}{---} & + 0.&0001 \\
 amelia & StudLoans & \multicolumn{2}{l}{---} & --0.&0001 \\ 
 mice & StudLoans & \multicolumn{2}{l}{---} & --0.&0001 \\ 
 na.omit & StudLoans & \multicolumn{2}{l}{---} & --0.&0146 \\
 \hline \\[-1.8ex] 
 \end{longtable}

\dsp

\normalsize

The same can be observed for the interval variables, with `hd.ord` again claiming last place across both data sets. The difference to `hot.deck` is still there but less pronounced than in Table \ref{mult.mar.4var}. `amelia` does best for `Age` while `mice` performs better for `Income`, with the exception of the ANES (`r mult.mar.11.anes[mult.mar.11.meth == "mice" & mult.mar.11.var == "Income"]` `mice` vs. `r mult.mar.11.anes[mult.mar.11.meth == "amelia" & mult.mar.11.var == "Income"]` `amelia`). As for the binary variables, `hd.ord` also consistently performs slightly worse when compared to the MAR analysis with 12 amputed variables and only `Education` treated by `polr`: `r mult.mar.11.anes[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Income"]`, `r mult.mar.11.cces[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Income"]` vs. `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Income"]`, `r mar.12.cces[mar.12.meth == "hd.ord" & mar.12.var == "Income"]` for `Income` and `r mult.mar.11.anes[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Age"]`, `r mult.mar.11.cces[mult.mar.11.meth == "hd.ord" & mult.mar.11.var == "Age"]` vs. `r mar.12.anes[mar.12.meth == "hd.ord" & mar.12.var == "Age"]`, `r mar.12.cces[mar.12.meth == "hd.ord" & mar.12.var == "Age"]` for `Age`.



```{r MULT MNAR 11 Variables, include=FALSE}

mult.mnar.11var.anes <- read.csv("data/anes/mnar/results/anes.mnar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
mult.mnar.11var.cces <- read.csv("data/cces/mnar/results/cces.mnar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus
# mult.mnar.11var.frame <- read.csv("data/framing/mnar/results/framing.mnar.mult.results.11var.1000n.1000it.20perc.csv") %>% .[,-1] %>% addPlus

mult.mnar.11var.anes[1:6, 2] <- mult.mnar.11var.cces[1:6, 2] <- rep("Democrat", 6)
mult.mnar.11var.anes[13:18, 2] <- mult.mnar.11var.cces[13:18, 2] <- rep("Income", 6)
mult.mnar.11var.anes[31:36, 2] <- mult.mnar.11var.cces[31:36, 2] <- rep("Employed", 6)

mult.mnar.11var.anes$diff[mult.mnar.11var.anes$method == "true"] <- mult.mnar.11var.anes$value[mult.mnar.11var.anes$method == "true"]
mult.mnar.11var.cces$diff[mult.mnar.11var.cces$method == "true"] <- mult.mnar.11var.cces$value[mult.mnar.11var.cces$method == "true"]
# mult.mnar.11var.frame$diff[mult.mnar.11var.frame$method == "true"] <- mult.mnar.11var.frame$value[mult.mnar.11var.frame$method == "true"]

levels(mult.mnar.11var.anes$method) <- levels(mult.mnar.11var.cces$method) <- levs
# levels(mult.mnar.11var.frame$method) <- levs

mult.un.meth <- mult.mnar.11var.anes$method %>% unique
mult.un.meth.len <- mult.un.meth %>% length
mult.un.vars <- c(mult.mnar.11var.anes$variable %>% as.character,
                  mult.mnar.11var.cces$variable %>% as.character) %>%
  unique
# mult.un.vars <- c(mult.mnar.11var.anes$variable %>% as.character,
#                   mult.mnar.11var.cces$variable %>% as.character,
#                   mult.mnar.11var.frame$variable %>% as.character) %>%
#   unique
mult.un.vars.len <- mult.un.vars %>% length
mult.un.anes.len <- mult.mnar.11var.anes$variable %>% unique %>% length
mult.un.cces.var <- mult.mnar.11var.cces$variable %>% unique %>% as.character
mult.un.cces.len <- mult.un.cces.var %>% length
# mult.un.frame.var <- mult.mnar.11var.frame$variable %>% unique %>% as.character
# mult.un.frame.len <- mult.un.frame.var %>% length

mult.variable <- rep(mult.un.vars, each = mult.un.meth.len)
mult.method <- rep(mult.un.meth, mult.un.vars.len) %>% as.character
mult.meth.len <- mult.method %>% length

mult.mnar.11var <- cbind(mult.method, mult.variable) %>% as.data.frame

mult.mnar.11var$mult.anes.col <- c(mult.mnar.11var.anes$diff,
                                  rep("---",
                                      mult.un.meth.len * (mult.un.vars.len - mult.un.anes.len)))
mult.mnar.11var$mult.cces.col <- rep("---", mult.meth.len) %>% as.character
# mult.mnar.11var$mult.frame.col <- rep("---", mult.meth.len) %>% as.character

for(i in 1:(mult.un.cces.len)){
  mult.mnar.11var$mult.cces.col[mult.mnar.11var$mult.variable == mult.un.cces.var[i]] <- mult.mnar.11var.cces$diff[mult.mnar.11var.cces$variable == mult.un.cces.var[i]]
  # mult.mnar.11var$mult.frame.col[mult.mnar.11var$mult.variable == mult.un.frame.var[i]] <- mult.mnar.11var.frame$diff[mult.mnar.11var.frame$variable == mult.un.frame.var[i]]
}

colnames(mult.mnar.11var) <- col.names

# to make the in-text citations shorter
mult.mnar.11.anes <- mult.mnar.11var$ANES
mult.mnar.11.cces <- mult.mnar.11var$CCES
# mult.mnar.11.frame <- mult.mnar.11var$Framing
mult.mnar.11.meth <- mult.mnar.11var$Method
mult.mnar.11.var <- mult.mnar.11var$Variable

tab.mult.mnar.11var <- stargazer(mult.mnar.11var, 
                                 summary = FALSE,
                                 align = TRUE,
                                 header = FALSE,
                                 rownames = FALSE,
                                 digits = 4,
                                 title = "Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MNAR, 11 Variables with NA",
                                 label = "mult.mnar.11var")

pt <- gsub("\\multicolumn{1}{c}{", "", tab.mult.mnar.11var, fixed = TRUE)
cat(pt)


```

<!--
MNAR 11 Var
Vars the same as for MAR 11 Var
	Binary
		amelia and mice perform better, but often not by much. Occasionally, hd.ord eclipses them (.0057 hd.ord vs. .0060 mice Male ANES; .0057 hd.ord vs. .0059 amelia Male framing)
		hd.ord consistently gets slightly worse when compared to MNAR 12 Var:		
		MNAR 12 Var hd.ord: Dem .0049, .0049, .0013. Male .0053, .0051,. 0051
		MNAR 11 Var hd.ord: Dem .0053, .0053, .0024. Male .0057, .0056, .0057
	Ordinal framing
		hd.ord worst for Media but better than hot.deck for Participation
		mice and amelia performs best by a significant margin
		na.omit as far off as for MNAR 12 Var
		hd.ord consistently gets slightly worse when compared to MNAR 12 Var:		
		MNAR 12 Var hd.ord: Media .0152, Participation .0115
		MNAR 11 Var hd.ord: Media .0164. Participation .0130
	interval
		Consistent with previous analyses
		na.omit better than hd.ord for Age for all ds. na.omit also best overall across all methods for Age CCES
		hd.ord consistently gets slightly worse when compared to MNAR 12 Var:		
		MNAR 12 Var hd.ord: Inc .1046, .0421, .0262. Age .3081, .2251, .2989
		MNAR 11 Var hd.ord: Inc .1142, .0473, .0291. Age .3329, .2424, .3271
-->

Table \ref{mult.mnar.11var} shows the results of imputing both data sets with two `polr`-treated variables MNAR for 11 amputed variables. For the binary variables, `amelia` and `mice` perform better, but often not by much. Occasionally, `hd.ord` eclipses them (`r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Male"]` `hd.ord` vs. `r mult.mnar.11.anes[mult.mnar.11.meth == "mice" & mult.mnar.11.var == "Male"]` `mice` ANES `Male`). As was the case in the comparison between the MNAR analysis with four amputed variables and the MNAR analysis with five amputed variables, `hd.ord` consistently demonstrates slightly worse results in the switch from 12 (Table \ref{mnar.12var}) to 11 and one to two `polr`-treated variables: `r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Democrat"]`, `r mult.mnar.11.cces[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Democrat"]` vs. `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Democrat"]`, `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Democrat"]` for `Democrat` and `r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Male"]`, `r mult.mnar.11.cces[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Male"]` vs. `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Male"]`, `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Male"]` for `Male`.

\ssp

\footnotesize

\begin{longtable}{ccr@{}lr@{}l} 
 \caption{Accuracy of Multiple Imputation Methods. ANES and CCES Data, 2 Ordinal Variables (Education, Interest), MNAR, 11 Variables with NA}   
 \label{mult.mnar.11var}
 \\[-1.8ex]\hline  
 \hline \\[-1.8ex] 
 \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{CCES} \\
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3420 & 0.&3770 \\ 
 hd.ord & Democrat & --0.&0050 & --0.&0053 \\ 
 hot.deck & Democrat & --0.&0053 & --0.&0053 \\
 amelia & Democrat & --0.&0047 & --0.&0049 \\
 mice & Democrat & --0.&0044 & --0.&0048 \\ 
 na.omit & Democrat & --0.&0097 & --0.&0097 \\
 true & Male & 0.&4890 & 0.&4830 \\ 
 hd.ord & Male & --0.&0061 & --0.&0058 \\ 
 hot.deck & Male & --0.&0057 & --0.&0056 \\ 
 amelia & Male & --0.&0060 & --0.&0054 \\ 
 mice & Male & --0.&0060 & --0.&0054 \\ 
 na.omit & Male & --0.&0088 & --0.&0116 \\ 
 true & Income & 16.&6140 & 6.&4810 \\
 hd.ord & Income & --0.&0841 & --0.&0349 \\
 hot.deck & Income & --0.&1142 & --0.&0473 \\
 amelia & Income & --0.&0540 & --0.&0250 \\ 
 mice & Income & --0.&0549 & --0.&0249 \\ 
 na.omit & Income & --0.&2112 & --0.&0982 \\
 true & Age & 50.&0410 & 52.&8230 \\ 
 hd.ord & Age & --0.&2124 & --0.&1607 \\ 
 hot.deck & Age & --0.&3329 & --0.&2424 \\ 
 amelia & Age & --0.&1194 & --0.&1135 \\ 
 mice & Age & --0.&1141 & --0.&1102 \\ 
 na.omit & Age & --0.&2978 & --0.&0974 \\ 
 true & Black & 0.&0790 & 0.&0950 \\ 
 hd.ord & Black & --0.&0036 & --0.&0045 \\
 hot.deck & Black & --0.&0040 & --0.&0042 \\
 amelia & Black & --0.&0041 & --0.&0044 \\ 
 mice & Black & --0.&0037 & --0.&0041 \\ 
 na.omit & Black & --0.&0050 & --0.&0066 \\ 
 true & Employed & 0.&6610 & 0.&4370 \\ 
 hd.ord & Employed & --0.&0042 & --0.&0057 \\ 
 hot.deck & Employed & --0.&0036 & --0.&0058 \\ 
 amelia & Employed & --0.&0034 & --0.&0045 \\ 
 mice & Employed & --0.&0033 & --0.&0044 \\ 
 na.omit & Employed & --0.&0022 & --0.&0122 \\ 
 true & Religious & 0.&6460 & 0.&6420 \\
 hd.ord & Religious & --0.&0045 & --0.&0044 \\
 hot.deck & Religious & --0.&0047 & --0.&0042 \\
 amelia & Religious & --0.&0043 & --0.&0045 \\ 
 mice & Religious & --0.&0043 & --0.&0045 \\ 
 na.omit & Religious & --0.&0055 & --0.&0077 \\ 
 true & Married & 0.&5290 & 0.&6310 \\ 
 hd.ord & Married & --0.&0047 & --0.&0043 \\ 
 hot.deck & Married & --0.&0042 & --0.&0040 \\
 amelia & Married & --0.&0046 & --0.&0039 \\ 
 mice & Married & --0.&0045 & --0.&0039 \\ 
 na.omit & Married & --0.&0120 & --0.&0095 \\
 true & OwnHome & 0.&6820 & 0.&7010 \\
 hd.ord & OwnHome & --0.&0037 & --0.&0035 \\ 
 hot.deck & OwnHome & --0.&0030 & --0.&0030 \\
 amelia & OwnHome & --0.&0031 & --0.&0033 \\
 mice & OwnHome & --0.&0030 & --0.&0033 \\ 
 na.omit & OwnHome & --0.&0112 & --0.&0088 \\ 
 true & Rally & 0.&0830 & --- \\ 
 hd.ord & Rally & --0.&0047 & --- \\ 
 hot.deck & Rally & --0.&0047 & --- \\ 
 amelia & Rally & --0.&0045 & --- \\
 mice & Rally & --0.&0044 & --- \\
 na.omit & Rally & --0.&0080 & --- \\
 true & Donate & 0.&1390 & --- \\
 hd.ord & Donate & --0.&0057 & --- \\ 
 hot.deck & Donate & --0.&0060 & --- \\ 
 amelia & Donate & --0.&0054 & --- \\  
 mice & Donate & --0.&0053 & --- \\ 
 na.omit & Donate & --0.&0122 & --- \\ 
 true & Gay & \multicolumn{2}{l}{---} & 0.&0420 \\ 
 hd.ord & Gay & \multicolumn{2}{l}{---} & --0.&0027 \\
 hot.deck & Gay & \multicolumn{2}{l}{---} & --0.&0026 \\ 
 amelia & Gay & \multicolumn{2}{l}{---} & --0.&0028 \\ 
 mice & Gay & \multicolumn{2}{l}{---} & --0.&0027 \\ 
 na.omit & Gay & \multicolumn{2}{l}{---} & --0.&0037 \\
 true & StudLoans & \multicolumn{2}{l}{---} & 0.&1910 \\
 hd.ord & StudLoans & \multicolumn{2}{l}{---} & --0.&0067 \\
 hot.deck & StudLoans & \multicolumn{2}{l}{---} & --0.&0064 \\
 amelia & StudLoans & \multicolumn{2}{l}{---} & --0.&0057 \\
 mice & StudLoans & \multicolumn{2}{l}{---} & --0.&0056 \\ 
 na.omit & StudLoans & \multicolumn{2}{l}{---} & --0.&0082 \\
 \hline \\[-1.8ex] 
 \end{longtable}
 
\dsp

\normalsize

Finally, the results for the interval variables confirm previous results, with `amelia` and `mice` demonstrating the best performance. In addition, note that `na.omit` delivers better results than `hd.ord` for `Age` for both data sets and represents the best method for CCES `Age`. Similar to the MNAR results for four variables, the performance of `hd.ord` in the MNAR analysis of 11 variables deteriorates in the switch from one to two ordinal variables: `r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Income"]`, `r mult.mnar.11.cces[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Income"]` vs. `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Income"]`, `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Income"]` for `Income` and `r mult.mnar.11.anes[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Age"]`, `r mult.mnar.11.cces[mult.mnar.11.meth == "hd.ord" & mult.mnar.11.var == "Age"]` vs. `r mnar.12.anes[mnar.12.meth == "hd.ord" & mnar.12.var == "Age"]`, `r mnar.12.cces[mnar.12.meth == "hd.ord" & mnar.12.var == "Age"]` for `Age`.




## Speed for 12 Imputed Variables {#app-ordmiss-speed-12var}

<!--
The difference between the methods is somewhat less pronounced for 12 variables with missing data when compared to 5 variables. This could possibly be explained by the thinner spread of missing values across a higher number of variables. The substantive conclusions nonetheless remain the same.
-->

```{r Runtimes 12 Variables MAR, include=FALSE}

run.12var.anes <- read.csv("data/anes/mar/runtimes/anes.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
run.12var.cces <- read.csv("data/cces/mar/runtimes/cces.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]
# run.12var.frame <- read.csv("data/framing/mar/runtimes/framing.mar.runtime.12var.1000n.1000it.20perc.csv") %>% .[,-1] %>% .[, order(ncol(.):1)]

run.12var <- cbind(run.12var.anes, run.12var.cces[,2])
colnames(run.12var) <- c("", "ANES", "CCES")
```

```{r Runtimes Increased Missingness MAR 12 Variables Table, results='asis', echo=FALSE}

stargazer(run.12var,
          summary = FALSE,
          align = TRUE,
          header = FALSE,
          rownames = FALSE,
          title = "Runtimes of Multiple Imputation Methods (in Minutes). ANES and CCES Data, MAR, 12 Variables with NA",
          label = "runtimes12var")

```




# FRAMING {#app-framing}

## Framing Questionnaire {#app-framing-questionnaire}

\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire01.jpg}
  \caption{Questionnaire}
  \label{framing-questionnaire}
\end{figure}

\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire02.jpg}
\end{figure}

\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire03.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire04.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire05.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire06.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire07.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire08.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire09.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire10.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire11.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire12.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire13.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire14.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire15.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire16.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire17.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire18.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire19.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire20.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire21.jpg}
\end{figure}
\begin{figure}[hbt]
  \centering
\includegraphics{data/framing/appendix/questionnaire/questionnaire22.jpg}
\end{figure}

\clearpage 



## Survey Experiment Results Differentiated by Party ID {#app-framing-pid}

Tables \ref{exp-anes-op-rep} and \ref{exp-anes-op-dem} show the ordinal regression results differentiated by party ID. The `Moral Opposing` frames are stronger than the `Self-Interest Opposing` frames for Democrat ANES environment, Democrat OP environment, Democrat OP healthcare, and Republican ANES environment. Note that these results change, though, for Republican healthcare. Both the ANES and the OP `Self-Interest Opposing` frames move respondents more than their `Moral Opposing` counterparts. It thus appears that Republicans value self-interest more than morality when it comes to opposition to healthcare. As in the overall analysis above, the `Moral Supporting` frames do not move people more than the `Self-Interest Supporting` frames. All coefficients here include the null, with the exception of Democrat OP environment which actually shows a negative coefficient. Almost none of the control variables show statistically significant results. While the results for opposing frames for Republican healthcare are notable, these results nonetheless overall do not change the substantive conclusions for the hypotheses.


```{r Framing-Experiment-ANES-Republican, include=FALSE}

df.an.rep <- df.an %>% filter(., pid == "Republican")
vars.pid <- vars.op[!names(vars.op) %in% "Democrat"]

plr.out.an.rep.hc <- create.formula.plr.out(df.an.rep, vars.pid, "hc")
plr.out.an.rep.ev <- create.formula.plr.out(df.an.rep, vars.pid, "ev")

# adjust names (has to be done after tidy() because tidy() throws up an error otherwise)
names(plr.out.an.rep.hc$coefficients) <- repl.names(names(plr.out.an.rep.hc$coefficients), vars.repl)
names(plr.out.an.rep.ev$coefficients) <- repl.names(names(plr.out.an.rep.ev$coefficients), vars.repl)

```

```{r Framing-Experiment-OP-Republican, include=FALSE}

df.op.rep <- df.op %>% filter(., pid == "Republican")

plr.out.op.rep.hc <- create.formula.plr.out(df.op.rep, vars.pid, "hc")
plr.out.op.rep.ev <- create.formula.plr.out(df.op.rep, vars.pid, "ev")

# adjust names (has to be done after tidy() because tidy() throws up an error otherwise)
names(plr.out.op.rep.hc$coefficients) <- repl.names(names(plr.out.op.rep.hc$coefficients), vars.repl)
names(plr.out.op.rep.ev$coefficients) <- repl.names(names(plr.out.op.rep.ev$coefficients), vars.repl)

```

```{r ANES-OP-Republican-Table, include=FALSE}

stargazer(plr.out.an.rep.hc, plr.out.op.rep.hc, plr.out.an.rep.ev, plr.out.op.rep.ev,
          header= FALSE,
          align = TRUE,
          single.row = TRUE,
          title = "Ordinal Logistic Regression Results, Republican",
          label = "exp-anes-op-rep",
          star.char = c("", "", ""),
          omit.table.layout = "n",
          ci = TRUE,
          dep.var.labels = c("Healthcare", "Environment"),
          column.labels = c("ANES", "OP", "ANES", "OP"),
          column.sep.width = "-60pt",
          model.numbers = FALSE)

```


\ssp

\footnotesize

\blandscape
\centering
\captionof{table}{Ordinal Logistic Regression Results, Republican\label{exp-anes-op-rep}}
\begin{tabular}{@{\extracolsep{-60pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{2}{c}{Healthcare} & \multicolumn{2}{c}{Environment} \\ 
 & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{OP} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{OP} \\ 
\hline \\[-1.8ex] 
 Moral opposing & -0.261$ $(-1.013$, $0.490) & -0.732^{}$ $(-1.397$, $-0.067) & -1.009^{}$ $(-1.711$, $-0.306) & -0.088$ $(-0.706$, $0.531) \\ 
  Moral supporting & 0.131$ $(-0.586$, $0.847) & -0.471$ $(-1.130$, $0.189) & -0.304$ $(-1.034$, $0.425) & -0.190$ $(-0.832$, $0.452) \\ 
  Self-interest opposing & -0.699^{}$ $(-1.370$, $-0.028) & -0.949^{}$ $(-1.603$, $-0.296) & -0.343$ $(-1.026$, $0.340) & 0.027$ $(-0.567$, $0.622) \\ 
  Self-interest supporting & -0.302$ $(-1.005$, $0.401) & -0.325$ $(-0.948$, $0.298) & 0.063$ $(-0.591$, $0.716) & 0.258$ $(-0.360$, $0.876) \\ 
  Employed full time & 0.222$ $(-0.743$, $1.188) & -0.114$ $(-0.832$, $0.604) & -0.222$ $(-1.150$, $0.707) & 0.137$ $(-0.608$, $0.882) \\ 
  Employed part time & 0.224$ $(-0.828$, $1.276) & -0.337$ $(-1.145$, $0.472) & -0.142$ $(-1.153$, $0.869) & 0.204$ $(-0.624$, $1.032) \\ 
  Homemaker & -0.048$ $(-1.168$, $1.072) & -0.310$ $(-1.285$, $0.664) & -0.006$ $(-1.116$, $1.103) & 0.109$ $(-0.911$, $1.130) \\ 
  Retired & -0.340$ $(-1.323$, $0.643) & -0.427$ $(-1.187$, $0.334) & -0.275$ $(-1.225$, $0.675) & 0.007$ $(-0.782$, $0.796) \\ 
  Student & 1.285$ $(-0.353$, $2.922) & 0.147$ $(-0.931$, $1.225) & -0.103$ $(-1.677$, $1.472) & -0.885$ $(-2.008$, $0.239) \\ 
  Income & -0.006$ $(-0.149$, $0.137) & -0.105^{}$ $(-0.227$, $0.018) & 0.070$ $(-0.070$, $0.209) & 0.054$ $(-0.066$, $0.174) \\ 
  Male & -0.141$ $(-0.578$, $0.295) & 0.072$ $(-0.344$, $0.488) & -0.147$ $(-0.587$, $0.293) & -0.244$ $(-0.671$, $0.183) \\ 
  5th-6th grade & -2.747^{}$ $(-5.811$, $0.317) &  & 0.288$ $(-2.255$, $2.831) &  \\ 
  7th-8th grade & 0.821$ $(-2.644$, $4.286) &  & 2.312$ $(-0.990$, $5.615) &  \\ 
  9th grade & -0.274$ $(-3.679$, $3.132) &  & 1.690$ $(-1.678$, $5.058) &  \\ 
  10th grade & -15.887^{}$ $(-15.887$, $-15.887) &  & -1.034$ $(-3.888$, $1.820) &  \\ 
  11th grade & -0.750$ $(-3.064$, $1.564) &  & -0.872$ $(-3.107$, $1.364) &  \\ 
  12th grade & -0.518$ $(-3.144$, $2.109) &  & 0.513$ $(-1.981$, $3.007) &  \\ 
  High school graduate & 0.036$ $(-1.515$, $1.587) &  & 1.158^{}$ $(-0.205$, $2.521) &  \\ 
  Some college & -0.341$ $(-1.888$, $1.206) & -0.165$ $(-0.744$, $0.413) & 1.160^{}$ $(-0.207$, $2.527) & 0.352$ $(-0.240$, $0.943) \\ 
  Associate degree & -0.626$ $(-2.207$, $0.955) & -0.209$ $(-0.872$, $0.453) & 0.739$ $(-0.666$, $2.145) & -0.152$ $(-0.818$, $0.514) \\ 
  Bachelor & -0.495$ $(-2.018$, $1.029) & 0.122$ $(-0.489$, $0.733) & 1.224^{}$ $(-0.111$, $2.559) & -0.262$ $(-0.880$, $0.356) \\ 
  Master & -1.154$ $(-2.772$, $0.465) &  & 0.868$ $(-0.564$, $2.300) &  \\ 
  Professional degree & -1.146$ $(-3.198$, $0.906) &  & 0.658$ $(-1.457$, $2.772) &  \\ 
  Master or higher &  & -0.117$ $(-0.819$, $0.586) &  & -0.097$ $(-0.814$, $0.619) \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{318} & \multicolumn{1}{c}{331} & \multicolumn{1}{c}{318} & \multicolumn{1}{c}{331} \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\fillandplacepagenumber
\elandscape

\dsp

\normalsize



```{r Framing-Experiment-ANES-Democrat, include=FALSE}

df.an.dem <- df.an %>% filter(., pid == "Democrat")

plr.out.an.dem.hc <- create.formula.plr.out(df.an.dem, vars.pid, "hc")
plr.out.an.dem.ev <- create.formula.plr.out(df.an.dem, vars.pid, "ev")

# adjust names (has to be done after tidy() because tidy() throws up an error otherwise)
names(plr.out.an.dem.hc$coefficients) <- repl.names(names(plr.out.an.dem.hc$coefficients), vars.repl)
names(plr.out.an.dem.ev$coefficients) <- repl.names(names(plr.out.an.dem.ev$coefficients), vars.repl)

```

```{r Framing-Experiment-OP-Democrat, include=FALSE}

df.op.dem <- df.op %>% filter(., pid == "Democrat")

plr.out.op.dem.hc <- create.formula.plr.out(df.op.dem, vars.pid, "hc")
plr.out.op.dem.ev <- create.formula.plr.out(df.op.dem, vars.pid, "ev")

# adjust names (has to be done after tidy() because tidy() throws up an error otherwise)
names(plr.out.op.dem.hc$coefficients) <- repl.names(names(plr.out.op.dem.hc$coefficients), vars.repl)
names(plr.out.op.dem.ev$coefficients) <- repl.names(names(plr.out.op.dem.ev$coefficients), vars.repl)

```

```{r ANES-OP-Democrat-Table, include=FALSE}

stargazer(plr.out.an.dem.hc, plr.out.op.dem.hc, plr.out.an.dem.ev, plr.out.op.dem.ev,
          header= FALSE,
          align = TRUE,
          single.row = TRUE,
          title = "Ordinal Logistic Regression Results, Democrat",
          label = "exp-anes-op-dem",
          star.char = c("", "", ""),
          omit.table.layout = "n",
          ci = TRUE,
          dep.var.labels = c("Healthcare", "Environment"),
          column.labels = c("ANES", "OP", "ANES", "OP"),
          column.sep.width = "-60pt",
          model.numbers = FALSE)

```


\ssp

\footnotesize

\blandscape
\centering
\captionof{table}{Ordinal Logistic Regression Results, Democrat\label{exp-anes-op-dem}}
\begin{tabular}{@{\extracolsep{-60pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{2}{c}{Healthcare} & \multicolumn{2}{c}{Environment} \\ 
 & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{OP} & \multicolumn{1}{c}{ANES} & \multicolumn{1}{c}{OP} \\ 
\hline \\[-1.8ex] 
 Moral opposing & -0.304$ $(-0.890$, $0.283) & -0.781^{}$ $(-1.363$, $-0.199) & -0.676^{}$ $(-1.317$, $-0.036) & -0.671^{}$ $(-1.225$, $-0.116) \\ 
  Moral supporting & 0.415$ $(-0.167$, $0.997) & -0.310$ $(-0.871$, $0.252) & -0.110$ $(-0.733$, $0.514) & -0.591^{}$ $(-1.159$, $-0.022) \\ 
  Self-interest opposing & -0.042$ $(-0.574$, $0.490) & -0.136$ $(-0.712$, $0.440) & -0.145$ $(-0.763$, $0.474) & -0.513^{}$ $(-1.081$, $0.054) \\ 
  Self-interest supporting & -0.073$ $(-0.664$, $0.517) & -0.135$ $(-0.688$, $0.419) & 0.497^{}$ $(-0.085$, $1.079) & -0.559^{}$ $(-1.138$, $0.019) \\ 
  Employed full time & 0.010$ $(-0.634$, $0.653) & -0.027$ $(-0.641$, $0.586) & -0.296$ $(-0.950$, $0.358) & 0.229$ $(-0.378$, $0.835) \\ 
  Employed part time & 0.113$ $(-0.586$, $0.813) & -0.079$ $(-0.748$, $0.591) & -0.403$ $(-1.115$, $0.309) & -0.027$ $(-0.690$, $0.636) \\ 
  Homemaker & 0.469$ $(-0.552$, $1.490) & 0.132$ $(-0.808$, $1.072) & 0.271$ $(-0.772$, $1.315) & -0.199$ $(-1.110$, $0.711) \\ 
  Retired & 0.171$ $(-0.497$, $0.839) & 0.505$ $(-0.158$, $1.169) & 0.085$ $(-0.584$, $0.754) & 0.895^{}$ $0.234$, $1.557) \\ 
  Student & -0.257$ $(-1.303$, $0.789) & -0.288$ $(-1.237$, $0.662) & -0.838$ $(-1.926$, $0.250) & 0.510$ $(-0.458$, $1.479) \\ 
  Income & -0.026$ $(-0.138$, $0.085) & -0.052$ $(-0.166$, $0.061) & -0.024$ $(-0.139$, $0.091) & 0.049$ $(-0.064$, $0.162) \\ 
  Male & 0.006$ $(-0.380$, $0.392) & 0.092$ $(-0.282$, $0.465) & -0.108$ $(-0.491$, $0.275) & -0.175$ $(-0.551$, $0.200) \\ 
  1st-4th grade & 19.675^{}$ $(17.269$, $22.080) &  & 0.217$ $(-4.350$, $4.785) &  \\ 
  7th-8th grade & 0.525^{}$ $0.525$, $0.525) &  & -15.604^{}$ $(-15.604$, $-15.604) &  \\ 
  9th grade & 21.793^{}$ $(19.747$, $23.839) &  & -1.359$ $(-6.089$, $3.371) &  \\ 
  10th grade & 24.851^{}$ $(22.655$, $27.048) &  & 2.036$ $(-2.670$, $6.741) &  \\ 
  11th grade & 23.857^{}$ $(21.309$, $26.404) &  & 0.149$ $(-4.363$, $4.660) &  \\ 
  12th grade & 22.521^{}$ $(20.929$, $24.114) &  & 0.791$ $(-3.562$, $5.144) &  \\ 
  High school graduate & 22.797^{}$ $(22.182$, $23.412) &  & 0.546$ $(-3.447$, $4.538) &  \\ 
  Some college & 23.051^{}$ $(22.499$, $23.603) & 0.566^{}$ $0.004$, $1.129) & 1.217$ $(-2.767$, $5.201) & 0.207$ $(-0.350$, $0.764) \\ 
  Associate degree & 23.238^{}$ $(22.583$, $23.894) & 0.402$ $(-0.231$, $1.034) & 1.483$ $(-2.517$, $5.483) & 0.365$ $(-0.280$, $1.009) \\ 
  Bachelor & 23.384^{}$ $(22.841$, $23.926) & 0.213$ $(-0.334$, $0.760) & 1.745$ $(-2.226$, $5.717) & 0.019$ $(-0.547$, $0.585) \\ 
  Master & 23.704^{}$ $(23.067$, $24.342) &  & 1.784$ $(-2.196$, $5.764) &  \\ 
  Professional degree & 23.191^{}$ $(22.116$, $24.265) &  & 1.935$ $(-2.144$, $6.015) &  \\ 
  Doctorate & 23.703^{}$ $(22.721$, $24.685) &  & 1.418$ $(-2.668$, $5.504) &  \\ 
  Master or higher &  & 0.694^{}$ $0.060$, $1.328) &  & -0.058$ $(-0.677$, $0.562) \\ 
 \hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{412} & \multicolumn{1}{c}{422} & \multicolumn{1}{c}{412} & \multicolumn{1}{c}{422} \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\fillandplacepagenumber
\elandscape

\dsp

\normalsize


<!--
NORMAL:
MO consistent significant ANES ev, OP hc, OP ev
Income sig ANES hc
MS sig ANES hc, OP hc (sign here neg)
Male sig OPev
MS do not move people towards supporting hc and ev
MS ANES hc sig pos and stronger than ANES hc SIS, but all others zero or the wrong sign
MO move people, ANES ev, OP hc, OP ev all sig and stronger than SIO

REP HC:
ANES: SIO stronger (neg) than MO (null)
OP: SIO stronger (neg) than MO (neg but smaller)
MS + SIS all null
All other stuff null

REP EV:
ANES: MO stronger (neg) than SIO (null)
OP: Both null
MS + SIS all null
All other stuff null

DEM HC:
ANES: Both null
OP: MO stronger (neg) than SIO (null)
MS + SIS all null
All other stuff null

DEM EV:
ANES: MO stronger (neg) than SIO (null)
OP: MO stronger (neg) than SIO (null)
MS OP neg, MS anes + all SIS null
OP Retired pos


NORMAL:
MO consistent significant ANES ev, OP hc, OP ev
Income sig ANES hc
MS sig ANES hc, OP hc (sign here neg)
Male sig OPev
MS do not move people towards supporting hc and ev
MS ANES hc sig pos and stronger than ANES hc SIS, but all others zero or the wrong sign
MO move people, ANES ev, OP hc, OP ev all sig and stronger than SIO

No change in controls, almost all are null (one exception is DEM EV OP Retired (positive)
MS do not move people more than SIS. All null except for DEM EV MS OP, but that is negative
MO stronger than SIO for DEM EV ANES (SIO null), DEM EV OP (SIO null), DEM HC OP (SIO null), REP EV ANES (SIO null)
SIO stronger than MO for REP HC ANES (MO null), REP HC OP (MO negative but smaller)

-->





## Imputing 10 Variables with Missing Data {#app-framing-10var}

```{r MAR 10 Variables Framing, include=FALSE}

mar.10var.frame.an <- read.csv("data/framing/experiment/mar/results/framing.an.mar.results.10var.1062n.1000it.20perc.csv", stringsAsFactors = TRUE, check.names = FALSE)  %>% .[,-1] %>% addPlus
mar.10var.frame.op <- read.csv("data/framing/experiment/mar/results/framing.op.mar.results.10var.1103n.1000it.20perc.csv", stringsAsFactors = TRUE, check.names = FALSE)  %>% .[,-1] %>% addPlus

mar.10var.frame.an[49:54, 2] <- mar.10var.frame.op[49:54, 2] <- rep("Democrat", 6)

mar.10var.frame.an$diff[mar.10var.frame.an$method == "true"] <- mar.10var.frame.an$value[mar.10var.frame.an$method == "true"]
mar.10var.frame.op$diff[mar.10var.frame.op$method == "true"] <- mar.10var.frame.op$value[mar.10var.frame.op$method == "true"]

levels(mar.10var.frame.op$method) <- levels(mar.10var.frame.an$method) <- levs

mar.10var.frame <- cbind(mar.10var.frame.an[, c(1,2,4)], mar.10var.frame.op[,4])
colnames(mar.10var.frame) <- c("Method", "Variable", "ANES", "OP")

# to make the in-text citations shorter
mar.10.frame.an <- mar.10var.frame$ANES
mar.10.frame.op <- mar.10var.frame$OP
mar.10.frame.meth <- mar.10var.frame$Method
mar.10.frame.var <- mar.10var.frame$Variable

tab.mar.10var.frame <- stargazer(mar.10var.frame, 
                                 summary = FALSE,
                                 align = TRUE,
                                 header = FALSE,
                                 rownames = FALSE,
                                 digits = 4,
                                 title = "Accuracy of Multiple Imputation Methods. Framing Data, MAR, 10 Variables with NA",
                                 label = "mar.10var.frame")

qt <- gsub("\\multicolumn{1}{c}{", "", tab.mar.10var.frame, fixed = TRUE)
cat(qt)


```


Table \ref{mar.10var.frame} shows the imputation results for missing data inserted at random for 10 variables. In addition to the 5 variables in Table \ref{mar.10var.frame}, missing data is also inserted for `Student`, `Conservative`, `Black`, `Hispanic`, and `Asian`. All of these are binary variables. The results are virtually identical for all methods for `Democrat`, `Male`, `Student`, `Conservative`, `Black`, `Hispanic`, `Asian` for both data sets. The only exception among the binary variables is `Employed`, where `hd.ord` and `hot.deck` perform worse than `mice` and `amelia`. The results for `Income` and `Age` repeat the pattern from Table \ref{mar.5var.frame}: `amelia` overall beats `mice`, the hot decking methods show a much larger distance, and `hd.ord` performs worst out of all methods. Notably, `na.omit` produces better results than the hot decking methods for `Age` for both data sets.

\ssp

\footnotesize

\begin{longtable}{ccr@{}lr@{}l} 
 \caption{Accuracy of Multiple Imputation Methods. Framing Data, MAR, 10 Variables with NA}   
 \label{mar.10var.frame}  
 \\[-1.8ex]\hline
 \hline \\[-1.8ex] 
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{OP} \\ 
 \hline \\[-1.8ex] 
 true & Democrat & 0.&3879 & 0.&3826 \\ 
 hot.deck & Democrat & + 0.&0003 & + 0.&0003 \\ 
 hd.ord & Democrat & + 0.&0002 & + 0.&0003 \\
 amelia & Democrat & + 0.&0002 & + 0.&0000 \\
 mice & Democrat & + 0.&0001 & --0.&0001 \\
 na.omit & Democrat & --0.&0217 & --0.&0249 \\
 true & Male & 0.&4576 & 0.&4714 \\ 
 hot.deck & Male & --0.&0001 & + 0.&0003 \\
 hd.ord & Male & + 0.&0000 & + 0.&0002 \\
 amelia & Male & + 0.&0001 & + 0.&0000 \\ 
 mice & Male & + 0.&0000 & --0.&0001 \\  
 na.omit & Male & --0.&0334 & --0.&0325 \\ 
 true & Employed & 0.&5612 & 0.&5684 \\ 
 hot.deck & Employed & + 0.&0013 & + 0.&0013 \\
 hd.ord & Employed & + 0.&0013 & + 0.&0014 \\ 
 amelia & Employed & + 0.&0002 & + 0.&0001 \\
 mice & Employed & + 0.&0000 & + 0.&0000 \\ 
 na.omit & Employed & --0.&0277 & --0.&0260 \\
 true & Income & 3.&5537 & 3.&4923 \\  
 hot.deck & Income & --0.&0037 & --0.&0035 \\ 
 hd.ord & Income & --0.&0081 & --0.&0074 \\ 
 amelia & Income & + 0.&0000 & --0.&0001 \\
 mice & Income & --0.&0001 & --0.&0002 \\  
 na.omit & Income & --0.&1501 & --0.&1504 \\ 
 true & Age & 46.&3475 & 44.&9574 \\ 
 hot.deck & Age & --0.&1106 & --0.&1115 \\ 
 hd.ord & Age & --0.&1632 & --0.&1593 \\
 amelia & Age & --0.&0019 & + 0.&0008 \\ 
 mice & Age & --0.&0068 & --0.&0009 \\ 
 na.omit & Age & --0.&0890 & --0.&0921 \\ 
 true & Student & 0.&0386 & 0.&0481 \\ 
 hot.deck & Student & --0.&0001 & + 0.&0000 \\
 hd.ord & Student & + 0.&0000 & + 0.&0001 \\
 amelia & Student & + 0.&0001 & + 0.&0000 \\
 mice & Student & + 0.&0000 & --0.&0001 \\
 na.omit & Student & --0.&0069 & --0.&0065 \\ 
 true & Conservative & 0.&3908 & 0.&3744 \\ 
 hot.deck & Conservative & + 0.&0001 & + 0.&0004 \\
 hd.ord & Conservative & --0.&0002 & + 0.&0003 \\ 
 amelia & Conservative & + 0.&0000 & + 0.&0001 \\ 
 mice & Conservative & --0.&0002 & + 0.&0002 \\
 na.omit & Conservative & --0.&0177 & --0.&0193 \\
 true & Black & 0.&1158 & 0.&1242 \\ 
 hot.deck & Black & + 0.&0004 & + 0.&0003 \\ 
 hd.ord & Black & + 0.&0005 & + 0.&0004 \\ 
 amelia & Black & + 0.&0000 & + 0.&0000 \\ 
 mice & Black & + 0.&0001 & + 0.&0000 \\
 na.omit & Black & --0.&0094 & --0.&0086 \\
 true & Democrat & 0.&0895 & 0.&0907 \\ 
 hot.deck & Democrat & + 0.&0004 & + 0.&0003 \\ 
 hd.ord & Democrat & + 0.&0003 & + 0.&0001 \\ 
 amelia & Democrat & + 0.&0000 & + 0.&0000 \\
 mice & Democrat & --0.&0001 & + 0.&0000 \\
 na.omit & Democrat & --0.&0126 & --0.&0152 \\ 
 true & Asian & 0.&0810 & 0.&0662 \\
 hot.deck & Asian & + 0.&0001 & --0.&0002 \\ 
 hd.ord & Asian & + 0.&0000 & --0.&0001 \\
 amelia & Asian & + 0.&0001 & + 0.&0000 \\
 mice & Asian & + 0.&0000 & + 0.&0000 \\ 
 na.omit & Asian & --0.&0165 & --0.&0142 \\ 
 \hline \\[-1.8ex]
 \end{longtable}

\dsp

\normalsize


```{r MNAR 10 Variables Framing, include=FALSE}

mnar.10var.frame.an <- read.csv("data/framing/experiment/mnar/results/framing.an.mnar.results.10var.1062n.1000it.20perc.csv", stringsAsFactors = TRUE, check.names = FALSE)  %>% .[,-1] %>% addPlus
mnar.10var.frame.op <- read.csv("data/framing/experiment/mnar/results/framing.op.mnar.results.10var.1103n.1000it.20perc.csv", stringsAsFactors = TRUE, check.names = FALSE)  %>% .[,-1] %>% addPlus

mnar.10var.frame.an[49:54, 2] <- mnar.10var.frame.op[49:54, 2] <- rep("Democrat", 6)

mnar.10var.frame.an$diff[mnar.10var.frame.an$method == "true"] <- mnar.10var.frame.an$value[mnar.10var.frame.an$method == "true"]
mnar.10var.frame.op$diff[mnar.10var.frame.op$method == "true"] <- mnar.10var.frame.op$value[mnar.10var.frame.op$method == "true"]

levels(mnar.10var.frame.op$method) <- levels(mnar.10var.frame.an$method) <- levs

mnar.10var.frame <- cbind(mnar.10var.frame.an[, c(1,2,4)], mnar.10var.frame.op[,4])
colnames(mnar.10var.frame) <- c("Method", "Variable", "ANES", "OP")

# to make the in-text citations shorter
mnar.10.frame.an <- mnar.10var.frame$ANES
mnar.10.frame.op <- mnar.10var.frame$OP
mnar.10.frame.meth <- mnar.10var.frame$Method
mnar.10.frame.var <- mnar.10var.frame$Variable

tab.mnar.10var.frame <- stargazer(mnar.10var.frame, 
                                  summary = FALSE,
                                  align = TRUE,
                                  header = FALSE,
                                  rownames = FALSE,
                                  digits = 4,
                                  title = "Accuracy of Multiple Imputation Methods. Framing Data, MNAR, 10 Variables with NA",
                                  label = "mnar.10var.frame")

rt <- gsub("\\multicolumn{1}{c}{", "", tab.mnar.10var.frame, fixed = TRUE)
cat(rt)

```


As was the case for Tables \ref{mar.5var.frame} and \ref{mnar.5var.frame}, all imputation results for missing data inserted not at random for 10 variables (Table \ref{mnar.10var.frame}) are much further away from the true values than in Table \ref{mar.10var.frame}. There are still several binary variables where all methods produce virtually identical results (`Democrat`, `Male`, `Employed`, `Conservative`), but distances between `hd.ord` and `mice`/`amelia` are notably larger for `Student`, `Black`, `Hispanic`, and `Asian`. Similarly to before, the distances between `hd.ord` and `mice`/`amelia` decrease for `Income` and `Age` when compared to the results for 10 variables MAR. `hd.ord` once again performs worst for both interval variables for both data sets. Note that `na.omit` is now overall much closer to the imputation methods and the true values. It even produces the best results for `Age` across all methods for both data sets.

\ssp

\footnotesize

\begin{longtable}{ccr@{}lr@{}l} 
 \caption{Accuracy of Multiple Imputation Methods. Framing Data, MNAR, 10 Variables with NA}   
 \label{mnar.10var.frame}
 \\[-1.8ex]\hline
 \hline \\[-1.8ex]
\multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Variable} & \multicolumn{2}{c}{ANES} & \multicolumn{2}{c}{OP} \\ 
 \hline \\[-1.8ex]  
 true & Democrat & 0.&3879 & 0.&3826 \\
 hot.deck & Democrat & --0.&0015 & --0.&0016 \\
 hd.ord & Democrat & --0.&0019 & --0.&0019 \\
 amelia & Democrat & --0.&0015 & --0.&0015 \\
 mice & Democrat & --0.&0013 & --0.&0014 \\ 
 na.omit & Democrat & --0.&0084 & --0.&0094 \\ 
 true & Male & 0.&4576 & 0.&4714 \\
 hot.deck & Male & --0.&0067 & --0.&0063 \\ 
 hd.ord & Male & --0.&0066 & --0.&0062 \\ 
 amelia & Male & --0.&0066 & --0.&0065 \\
 mice & Male & --0.&0067 & --0.&0066 \\
 na.omit & Male & --0.&0102 & --0.&0099 \\ 
 true & Employed & 0.&5612 & 0.&5684 \\ 
 hot.deck & Employed & --0.&0013 & --0.&0015 \\ 
 hd.ord & Employed & --0.&0014 & --0.&0017 \\
 amelia & Employed & --0.&0015 & --0.&0015 \\ 
 mice & Employed & --0.&0015 & --0.&0015 \\
 na.omit & Employed & --0.&0086 & --0.&0077 \\
 true & Income & 3.&5537 & 3.&4923 \\
 hot.deck & Income & --0.&0241 & --0.&0237 \\ 
 hd.ord & Income & --0.&0262 & --0.&0267 \\
 amelia & Income & --0.&0196 & --0.&0212 \\
 mice & Income & --0.&0196 & --0.&0214 \\ 
 na.omit & Income & --0.&0476 & --0.&0490 \\
 true & Age & 46.&3475 & 44.&9574 \\
 hot.deck & Age & --0.&2197 & --0.&2388 \\
 hd.ord & Age & --0.&3100 & --0.&2911 \\ 
 amelia & Age & --0.&1196 & --0.&1307 \\ 
 mice & Age & --0.&1207 & --0.&1341 \\ 
 na.omit & Age & --0.&0274 & --0.&0193 \\ 
 true & Student & 0.&0386 & 0.&0481 \\ 
 hot.deck & Student & --0.&0022 & --0.&0024 \\
 hd.ord & Student & --0.&0023 & --0.&0025 \\
 amelia & Student & --0.&0020 & --0.&0024 \\
 mice & Student & --0.&0015 & --0.&0018 \\
 na.omit & Student & --0.&0026 & --0.&0032 \\
 true & Conservative & 0.&3908 & 0.&3744 \\
 hot.deck & Conservative & --0.&0043 & --0.&0038 \\
 hd.ord & Conservative & --0.&0045 & --0.&0041 \\
 amelia & Conservative & --0.&0042 & --0.&0045 \\ 
 mice & Conservative & --0.&0044 & --0.&0043 \\
 na.omit & Conservative & --0.&0059 & --0.&0065 \\
 true & Black & 0.&1158 & 0.&1242 \\
 hot.deck & Black & --0.&0032 & --0.&0030 \\
 hd.ord & Black & --0.&0033 & --0.&0030 \\
 amelia & Black & --0.&0023 & --0.&0021 \\
 mice & Black & --0.&0023 & --0.&0020 \\ 
 na.omit & Black & --0.&0049 & --0.&0050 \\ 
 true & Democrat & 0.&0895 & 0.&0907 \\ 
 hot.deck & Democrat & --0.&0027 & --0.&0028 \\
 hd.ord & Democrat & --0.&0034 & --0.&0036 \\
 amelia & Democrat & --0.&0023 & --0.&0022 \\
 mice & Democrat & --0.&0023 & --0.&0021 \\ 
 na.omit & Democrat & --0.&0050 & --0.&0057 \\ 
 true & Asian & 0.&0810 & 0.&0662 \\ 
 hot.deck & Asian & --0.&0034 & --0.&0031 \\ 
 hd.ord & Asian & --0.&0038 & --0.&0032 \\ 
 amelia & Asian & --0.&0023 & --0.&0020 \\
 mice & Asian & --0.&0025 & --0.&0017 \\
 na.omit & Asian & --0.&0059 & --0.&0046 \\ 
 \hline \\[-1.8ex] 
 \end{longtable}

\dsp

\normalsize



<!--chapter:end:98-appendix.Rmd-->


# REFERENCES {-}

\noindent

\ssp


<!--chapter:end:99-references.Rmd-->

